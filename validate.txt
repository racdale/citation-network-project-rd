 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 This study sought to address the lack of experimental research examining the influence of contextual factors on African American students' learning. A total of 162 low-income African American and White fourth graders were randomly assigned to ethnically homogeneous, communally structured groups of three to work on a motion acceleration task using either computer simulation or physical tools, or to a control group that did not participate in the learning activities. A 3 (condition) x 2 (ethnicity) MANOVA was computed with initial learning and transfer as dependent variables. Results indicate African American and White students performed equally well on the test of initial learning, with both groups scoring significantly higher than the control group. However, African Americans' transfer outcomes were better than those of their White counterparts. Regarding tools, work with physical apparatus yielded better transfer outcomes than work with computer simulation. Implications for creating optimal learning contexts for African American students are discussed.

 This study tests the hypothesis that cultural differences in group orientation predict an interaction between the student variableethnicityand a learning context variablereward structureon math performance after group learning. One hundred and thirty-two African-American and European-American female and male fourth and fifth grade students studied math estimation in one of three group learning contexts. The learning contexts operationalized were: intergroup competitive, interpersonally competitive, and communal-no reward. ANCOVA confirmed a predicted interaction of ethnicity with learning context on post study session performance. Although there was no difference overall, African-American and European-American students performed best in the aggregate in different contexts. Independent ratings of students' group-positive behaviors mirrored the two-way interaction between learning context and ethnicity. The findings suggest that important student variables interact with the variable elements of group learning and should be studied in greater detail. They also support Boykin's (1994) contention that the cultural context of learning is a critical mediator of children's achievement.

-----------------------
 We show that visio-spatial representations and reasoning can be used as a similarity metric for case-based protein structure prediction. Our system retrieves pairs of alpha-helices based on contact map similarity, then transfers and adapts the structure information to an unknown helix pair. We show that similar protein contact maps predict a similar three-dimensional protein structure. The success of this method provides support for the notion that changing representations can enable similarity metrics in case-based reasoning.

 We show that visuospatial representations and reasoning techniques can be used as a similarity metric for analogical protein structure prediction. Our system retrieves pairs of a-helices based on contact map similarity, then transfers and adapts the structure information to an unknown helix pair, showing that similar protein contact maps predict similar 3D protein structure. The success of this method provides support for the notion that changing representations can enable similarity metrics in analogy.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 Objectives. Errors in eyewitness accounts can occur when a witness comes into contact with post-event 'misinformation'. A common way to encounter misinformation is through face-to-face interaction, in particular, via conversation with other individuals who also witnessed the crime. The current research compares this kind of misinformation with the non-social post-event narrative method typically employed in laboratory studies.

 The forgetting and remembering phenomena that Erdelyi outlines here have little to do with the concept of repression. None of the research that he describes shows that it is possible for people to repress (and then recover) memories for entire, significant, and potentiatly emotion-laden events. In the absence of scientific evidence, we continue to challenge the validity of the concept of repression.

-----------------------
 Under what conditions do perceivers prefer to assign a bound variable interpretation to a pronominal that is ambiguous between a bound variable and a coreferential interpretation? Several experiments were designed to test the hypothesis that language perceivers prefer a bound variable over a coreferential interpretation of a pronoun because the former only requires consultation of a logical form (LF) representation, while the latter requires access to a discourse representation. The hypothesis was disconfirmed in two respects. First, although bound variable interpretations show a processing advantage over coreferential interpretations in VP ellipsis constructions, the preference for bound variable interpretations is not general-it does nor extend to other quantificational contexts. Second, the preference for bound variable interpretations in VP ellipsis constructions is not limited to examples in which the antecedent and the ellipsis site occur in the same sentence. If the bound variable advantage were due to the ready availability of the LF for the current sentence, the advantage should disappear across sentence boundaries. An alternative hypothesis is then considered which could explain the source of the bound variable advantage in VP ellipsis contexts.

 Four naturalness judgment experiments were conducted to test different hypotheses about prosodic phrasing. The hypothesis that syntactic constituents should not be broken into distinct prosodic phrases [as in Truckenbrodt's Wrap constraint (Truckenbrodt, H., 1995. Phonological Phrases: Their Relation to Syntax, Focus, and Prominence. Unpublished PhD Dissertation, MIT)] was less predictive of the results of Experiments I and 2 than the hypothesis that constituents may be freely divided into prosodic phrases, as long as the resulting phrases are semantically coherent [Selkirk, E., 1984. Phonology and Syntax: The Relation Between Sound and Structure. MIT Press, Cambridge, MA]. The results of two further experiments confirmed Watson and Gibson's (Watson, D. G., Gibson, E., 2001. Linguistic structure and intonational phrasing. Paper presented at the Fourteenth Annual CUNY Conference on Human Sentence Processing, Philadelphia, 15-17 March 2001) claim that prosodic breaks are natural before long upcoming constituents, but did not support their hypothesis that the distance between a new item and its integration site is what motivates the presence of a prosodic phrase boundary. The results are interpreted as further evidence that the use of high level breaks in language comprehension is not governed by an invariant local mapping from syntax or processing considerations to prosody/intonation, but is related to the overall pattern of intonational choices made. (C) 2003 Elsevier B.V. All rights reserved.

-----------------------
 Two experimental tasks in psychology, the two-stage gambling game and the Prisoner's Dilemma game, show that people violate the sure thing principle of decision theory. These paradoxical findings have resisted explanation by classical decision theory for over a decade. A quantum probability model, based on a Hilbert space representation and Schrodinger's equation, provides a simple and elegant explanation for this behaviour. The quantum model is compared with an equivalent Markov model and it is shown that the latter is unable to account for violations of the sure thing principle. Accordingly, it is argued that quantum probability provides a better framework for modelling human decision-making.

 Naive observers typically perceive some groupings for a set of stimuli as more intuitive than others. The problem of predicting category intuitiveness has been historically considered the remit of models of unsupervised categorization. In contrast, this article develops a measure of category intuitiveness from one of the most widely supported models of supervised categorization, the generalized context model (GCM). Considering different category assignments for a set of instances, the authors asked how well the GCM can predict the classification of each instance on the basis of all the other instances. The category assignment that results in the smallest prediction error is interpreted as the most intuitive for the GCM-the authors refer to this way of applying the GCM as "unsupervised GCM." The authors systematically compared predictions of category intuitiveness from the unsupervised GCM and two models of unsupervised categorization: the simplicity model and the rational model. The unsupervised GCM compared favorably with the simplicity model and the rational model. This success of the unsupervised GCM illustrates that the distinction between supervised and unsupervised categorization may need to be reconsidered. However, no model emerged as clearly superior, indicating that there is more work to be done in understanding and modeling category intuitiveness.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 This study investigated the usefulness of explicit spatial coordinates from eye movements for the precision of covert shifts of attention within dense arrays of items. Observers shifted their attention covertly from one item to the next in response to a series of beeps and reported the color of the disc on which the series ended, providing an estimate of the accuracy of the "attentional walk". We compared performance in this task when only covert shifts of attention were done to performance when observers first executed an explicit eye movement to the starting point of the attentional walk before beginning the covert attentional walk. The hypothesis was that the eye movement would activate explicit coordinates of the starting point of the attentional walk within brain systems that are involved in controlling both shifts of attention and eye movements. This in turn would provide an anchor for the attentional walk, thereby improving performance. The evidence did not support this hypothesis. Performance was no better with an explicit eye movement prior to the attentional walk than without one. This suggests that covert orienting-shifting attention-and overt orienting-shifting the eyes-access the same coordinate system and therefore activating new coordinates interferes with the old ones, no matter what the system of orienting is.

 The attentional walk task was used to investigate the temporal properties of covert shifts of attention. Observers shifted attention within arrays of identical items in response to a series of auditory commands and reported the color of the final disk. The density of the arrays and the timing of the shift commands varied. Performance decreased as density increased, and the minimal amount of time needed to shift attention depended on the density of the display, varying from 350 to 750 msec. In addition, the observers were able to maintain attentional focus for at least 3,500 msec without a decline in performance, regardless of density. Thus, although the ability to maintain attention at a given position was found to be independent of the precision with which that location was defined, more precise attentional shifts required more time to execute.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 This review provides a cumulative perspective on current human factors research by first briefly acknowledging previous Annual Review articles. We show that several recent conceptual advances are an outgrowth of the information-processing approach adopted by the field and present several areas of current research that are built directly on prior work. Topic areas that provide fundamental tools for human factors analyses are summarized, and several current application areas are reviewed. We end by considering alternatives to the information-processing approach that have been proposed and placing those alternatives in context. We argue that the information-processing language provides the foundation that has enabled much of the growth in human factors. This growth reflects a cumulative development of concepts and methods that continues today.

 In the global economy, design of digital media often involves teams of individuals from a variety of cultures who must function together. Similarly, products must be designed and marketed taking specific cultural characteristics into account. Much is known about decision processes, culture and cognition, design of products and interfaces for human interaction with machines, and organizational processes, but this knowledge is dispersed across several disciplines and research areas. This article reviews current work in these areas and proposes a research agenda for fostering increased understanding of the ways in which cultural differences influence decision making and action in design and use of digital media.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The modifier effect is the reduction in perceived likelihood of a generic property sentence, when the head noun is modified. We investigated the prediction that the modifier effect would be stronger for mutable than for central properties, without finding evidence for this predicted interaction over the course of five experiments. However Experiment 6, which provided a brief context for the modified concepts to lend them greater credibility, did reveal the predicted interaction. It is argued that the modifier effect arises primarily from a general lack of confidence in generic statements about the typical properties of unfamiliar concepts. Neither prototype nor classical models of concept combination receive support from the phenomenon. (C) 2010 Elsevier Inc. All rights reserved.

 Within-category induction is the projection of a generic property from a class (Apples are sweet) to a subtype of that class (Chinese apples are sweet). The modifier effect refers to the discovery reported by Connolly et al., that the subtype statement tends to be judged less likely to be true than the original unmodified sentence. The effect was replicated and shown to be moderated by the typicality of the modifier (Experiment 1). Likelihood judgements were also found to correlate between modified and unmodified versions of sentences. Experiment 2 elicited justifications, which suggested three types of reason for the effect-pragmatics, knowledge-based reasoning, and uncertainty about attribute inheritance. It is argued that the results provide clear evidence for the default inheritance of prototypical attributes in modified concepts, although a full account of the effect remains to be given.

-----------------------
 Two lexical decision experiments investigated priming for a critical item (CI, sleep) and its related yoked associate (YA, blanket) when one had been studied in a related Deese/Roediger-McDermott (DRM) list (Experiments 1 &amp; 2) or a list of totally unrelated words (Experiment 2) and the other had been nonstudied. Semantic priming from the related DRM list occurred for nonstudied CIs (but not YAs) regardless of whether the Cl received within-test priming from its studied related YA during the lexical decision task, though the effect in the absence of within-test priming averaged across experiments was only significant by a one-tailed test. Also averaged across experiments, repetition priming occurred for both studied Os and YAs when they had been studied in related DRM lists whether or not there was also within-test priming from a nonstudied related yoked pairmate, though individual effects within the two experiments were sometimes not significant. Repetition priming boosted semantic priming from related DRM lists less for CIs than for YAs, similar to the finding that memory discriminability is poorer for CIs than for YAs in episodic recognition. This smaller repetition priming boost for CIs than for YAs occurred to the same degree when the CIs or YAs were studied in an unrelated list. When nonstudied CIs and YAs were totally unrelated to all previously studied items and separated by 3-7 items in the lexical decision task, a YA produced a small 16 msec priming effect for its CI, averaged across both experiments. The implications of these results for the activation account of the DRM false-memory effect and for single-prime versus multiple-prime long-term semantic priming effects are discussed.

 Four experiments examined whether studying a single Deese/Roediger-McDermott (DRM) list produces semantic priming for nonstudied critical items (Cls) and semantic + repetition priming for studied associates. After 30 s of mental arithmetic that followed the study of a DRM list, priming was assessed in a lexical decision task when the nonwords were either pronounceable (Experiment 1) or pseudohomophones (Experiments 2 - 4). Priming was measured relative to a baseline containing exactly the same Cls and associates that had not been primed by their related DRM lists. Significant CI semantic priming effects occurred in all four experiments, whether or not there was within-test priming from a related associate preceding the CI by 3 - 7 items. To our knowledge, these are the first experiments using standard DRM study procedures to provide a convincing demonstration of a genuine CI semantic priming effect in a delayed indirect memory test that should be free of intentional retrieval strategies. Discussion focuses on measuring long-term semantic activation effects without the influence of source monitoring in a lexical decision task. (c) 2005 Elsevier Inc. All rights reserved.

-----------------------
 The relations between automatic processing and (the absence of) consciousness are discussed in this paper. It is argued that automatic processing should not be identified with the absence of consciousness. The organism has access to representations resulting from automatic processing, but these representations, in contrast to the representations resulting from nonautomatic processing, are not propositional. Therefore monitoring of the process, the defining feature of nonautomatic processing, is not possible. (C) 1997 Academic Press.

 We apply Dienes &amp; Perner's (D&amp;P's) framework to the automatic/nonautomatic processing contrast. Our analysis leads to the conclusion that automatic and nonautomatic processing result in representations that have explicit results. We propose equating consciousness with explicitness of aspects rather than with full explicitness as defined by D&amp;P.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 Objective: To investigate the long-term effect of tibolone on mammographic density.
 Objective To investigate the effect of 10 years of treatment with tibolone on aortic stiffness and endothelial function.
-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 Two experimental tasks in psychology, the two-stage gambling game and the Prisoner's Dilemma game, show that people violate the sure thing principle of decision theory. These paradoxical findings have resisted explanation by classical decision theory for over a decade. A quantum probability model, based on a Hilbert space representation and Schrodinger's equation, provides a simple and elegant explanation for this behaviour. The quantum model is compared with an equivalent Markov model and it is shown that the latter is unable to account for violations of the sure thing principle. Accordingly, it is argued that quantum probability provides a better framework for modelling human decision-making.

 Naive observers typically perceive some groupings for a set of stimuli as more intuitive than others. The problem of predicting category intuitiveness has been historically considered the remit of models of unsupervised categorization. In contrast, this article develops a measure of category intuitiveness from one of the most widely supported models of supervised categorization, the generalized context model (GCM). Considering different category assignments for a set of instances, the authors asked how well the GCM can predict the classification of each instance on the basis of all the other instances. The category assignment that results in the smallest prediction error is interpreted as the most intuitive for the GCM-the authors refer to this way of applying the GCM as "unsupervised GCM." The authors systematically compared predictions of category intuitiveness from the unsupervised GCM and two models of unsupervised categorization: the simplicity model and the rational model. The unsupervised GCM compared favorably with the simplicity model and the rational model. This success of the unsupervised GCM illustrates that the distinction between supervised and unsupervised categorization may need to be reconsidered. However, no model emerged as clearly superior, indicating that there is more work to be done in understanding and modeling category intuitiveness.

-----------------------
 Because the probability of obtaining an experimental. finding given that the null hypothesis is true [p(F\H,)] is not the same as the probability that the null hypothesis is true given a finding [p(H-0\F)], calculating the former probability does not justify conclusions about the latter one. As the standard null-hypothesis significance-testing procedure does just that, it is logically invalid (J. Cohen, 1994). Theoretically, Bayes's theorem yields p(H-0\F), but in practice, researchers rarely know the correct values for 2 of the variables in the theorem. Nevertheless, by considering a wide range of possible values for the unknown variables, it is possible to calculate a range of theoretical values for p(H-0\F) and to draw conclusions about both hypothesis testing and theory evaluation.

 According to Bayesians, the null hypothesis significance-testing procedure is not deductively valid because it involves the retention or rejection of the null hypothesis under conditions where the posterior probability of that hypothesis is not known. Other criticisms are that this procedure is pointless and encourages imprecise hypotheses. However , according to non-Bayesians, there is no way of assigning a prior probability to the null hypothesis, and so Bayesian statistics do not work either. Consequently, no procedure has been accepted by both groups as providing a compelling reason to accept or reject hypotheses. The author aims to provide such a method. In the process, the author distinguishes between probability and epistemic estimation and argues that, although both are important in a science that is not completely deterministic, epistemic estimation is most relevant for hypothesis testing. Based on this analysis, the author proposes that hypotheses be evaluated via epistemic ratios and explores the implications of this proposal. One implication is that it is possible to encourage precise theorizing by imposing a penalty for imprecise hypotheses.

-----------------------
 The authors present a theory of stochastic interactive parallel processing with special emphasis on channel interactions and their relation to system capacity. The approach is based both on linear systems theory augmented with stochastic elements and decisional operators and on a metatheory of parallel channels' dependencies that incorporates standard independent and coactive parallel models as special cases. The metatheory is applied to OR and AND experimental paradigms, and the authors establish new theorems relating response time performance in these designs to earlier and novel issues. One notable outcome is the remarkable processing efficiency associated with linear parallel-channel systems that include mutually positive interactions. The results may offer insight into perceptual and cognitive configural-holistic processing systems.

 The maximum and minimum of a sample from a probability distribution are extremely important random variables in many areas of psychological theory, methodology, and statistics. For instance, the behavior of the mean of the maximum or minimum processing time, as a function of the number of component random processing times (n), has been studied extensively in an effort to identify the underlying processing architecture (e.g., Townsend &amp; Ashby, 1983; Colonius &amp; Vorberg, 1994). Little is known concerning how measures of variability of the maximum or minimum change with n. Here, a new measure of random variability, the quantile spread, is introduced, which possesses sufficient strength to define distributional orderings and derive a number of results concerning variability of the maximum and the minimum statistics. The quantile spread ordering may be useful in many venues. Several interesting open problems are pointed out.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Previous work showed that older adults' choice performance can be wiser than that of younger adults (Tentori, Osherson, Hasher, &amp; May, 2001). We contrasted two possible interpretations: a general expertise/wisdom view that suggests that older adults are generally more skilled at making decisions than younger adults and a domain-specific expertise view that suggests that older adults are more skilled decision makers only in domains in which they have greater knowledge. These hypotheses were contrasted using attraction effect tasks in two different domains: carning extra credit in a course and grocery shopping, domains presumed to be of different levels of knowledge to younger and older adults. Older adults showed consistent choice for both domains; younger adults showed consistent choice only for the extra credit problem. Several explanations of these findings are considered, including Damasio's somatic marker theory and age differences in reliance on heuristic versus analytic styles.

 A decision-maker is "irregular" if she would choose B from {A, B, C} but not from {A, B} (for example, preferring vanilla ice cream in a choice between vanilla and chocolate, but chocolate in a choice among vanilla, chocolate and strawberry). Similarly to previous studies we observed irregular choices by college students faced with hypothetical discount cards for supermarkets. However, older adults showed no such tendency. The same pattern was observed in three separate studies. We interpret the results in terms of a choice strategy by older adults that protects them from excessive spending. (C) 2001 Elsevier Science B.V. All ri-hts reserved.

-----------------------
 A major problem for developmental science is understanding how the cognitive and emotional networks important in carrying out mental processes can be related to individual differences. The last five years have seen major advances in establishing links between alleles of specific genes and the neural networks underlying aspects of attention. These findings have the potential of illuminating important aspects of normal development and its pathologies. We need to learn how genes and experience combine to influence the structure of neural networks and the efficiency with which they are exercised. Methods for addressing these issues are central to progress in the decade ahead.

 Young children's increasing ability to regulate their thoughts, feelings, and behavior is a hallmark of development that is of critical importance to their socialization. Recent advances in neuroimaging and molecular genetics hold promise for drawing together different levels of analysis of the emergence and growth of self-regulation. In this article, we review research relevant to our approach to understanding self-regulation, beginning with an examination, of the temperament construct of Effortful Control (EC). We trace the development of EC and its links to an anatomically defined attentional network and identify genes that may contribute to individual differences in the efficiency of this network. We also report on how intervention may influence a central component of self-regulation, the executive attention. network. Although much more work remains to be done, we believe that the importance of the questions addressed and the recent progress in understanding self-regulation make this a very exciting area of research.

-----------------------
 Many thousands of students take standardized tests every year. In the current research, we asked whether answering standardized test questions affects students' later test performance. Prior research has shown both positive and negative effects of multiple-choice testing on later tests, with negative effects arising from students selecting incorrect alternatives on multiple-choice tests and then believing they were correct (Roediger &amp; Marsh, 2005). In the current experiments, undergraduates and high school students answered multiple-choice questions retired from SAT II tests (that are no longer in the testing pool) on biology, chemistry, U.S. history, and world history, and later answered cued-recall questions about these subjects. In 3 experiments, we observed positive testing effects: More final cued-recall questions were answered correctly if the items had appeared on the initial multiple-choice test. We also sometimes observed negative testing effects: intrusions of multiple-choice distractors as answers on the final cued-recall test. Students who scored well on the initial test benefited from taking the test, but lower achieving students showed either less benefit (undergraduates) or cost,; from the testing (high school students).

 Multiple-choice testing has both positive and negative consequences for performance on later tests. Prior testing increases the number of questions answered correctly on a later test but also increases the likelihood that questions will be answered with lures from the previous multiple-choice test (Roediger &amp; Marsh, 2005). Prior research has shown that the positive effects of testing persist over a delay, but no one has examined the durability of the negative effects of testing. To address this, subjects took multiple-choice and cued recall tests (on subsets of questions) both immediately and a week after studying. Although delay reduced both the positive and negative testing effects, both still occurred after 1 week, especially if the multiple-choice test had also been delayed. These results are consistent with the argument that recollection underlies both the positive and negative testing effects.

-----------------------
 Younger and older participants were asked to indicate if 240 complex two-digit addition problems were smaller than 100 or not. Half of the problems were small-split problems (i.e., the proposed sums were 2% or 5% away from 100; e. g., 53 + 49) and half were large-split problems (i.e., proposed sums were 10% or 15% away from 100; 46 + 39). Behavioral and event-related potential (ERP) data revealed that ( a) both groups showed a split effect on both reaction times and percent errors, (b) split effects were smaller for older than for younger adults in ERPs, and (c) the hemispheric asymmetry ( left hemisphere advantage) reported for younger adults was reduced in older adults (age-related hemispheric asymmetry reduction). These results suggest that older adults tend to use only one strategy to solve all problems, whereas younger adults flexibly and adaptively use different strategies for small-and large-split problems. Implications of these findings for our and understanding of age-related similarities and differences in arithmetic problem-solving are discussed.

 This study examined performance measures and eye movements associated with complex arithmetic strategies in young and older adults. Participants added pairs of three-digit numbers using two different strategies, under choice and no-choice conditions. Older adults made more errors but were not significantly slower than young adults, and response times and errors showed no interaction between age and the number of carries. Older adults chose strategies less adaptively than young adults. Eye movements were consistent with use of required strategies on no-choice trials and reported strategies on choice trials. Eye movement data also suggested that young adults more successfully distinguished between strategies. Implications of these findings for understanding aging effects in complex arithmetic are discussed. (C) 2006 Elsevier B.V. All rights reserved.

-----------------------
 B. B. Murdock (2006) has interpreted remember-know data within a decision space defined by item and associative information, the fundamental variables in his general recognition memory model TODAM (B. B. Murdock, 1982). He has related parameters of this extended model to stimulus characteristics for several classic remember-know data sets. The authors show that this accomplishment is shared by both one- and two-dimensional signal-detection-based remember-know models (J. C. Dunn, 2004; C. M. Rotello, N. A. Macmillan, &amp; J. A. Reeder, 2004; J. T. Wixted &amp; V: Stretch, 2004), which can be represented in this same decision space and can be related to stimulus characteristics with similar success. Murdock claims that his model, unlike its competitors, is a process model; however, the process aspects of TODAM are not used in his application, and the decision aspects are identical to a previously proposed model. Murdock's claim that one competing model (STREAK; C. M. Rotello et al., 2004) is not fully specified is shown to be false. The new model is not superior to existing ones, but comparisons among the models to date are not definitive. The authors describe several strategies that might be applied to distinguish among them.

 Remember-know judgments provide additional information in recognition memory tests, but the nature of this information and the attendant decision process are in dispute. Competing models have proposed that remember judgments reflect a sum of familiarity and recollective information (the one-dimensional model), are based on a difference between these strengths (STREAK), or are purely recollective (die dual-process model). A choice among these accounts is sometimes made by comparing the precision of their fits to data, but this strategy may be muddied by differences in model complexity: Some models that appear to provide good fits may simply be better able to mimic the data produced by other models. To evaluate this possibility, we simulated data with each of the models in each of three popular remember-know paradigms, then fit those data to each of the models. We found that the one-dimensional model is generally less complex than the others, but despite this handicap, it dominates the others as the best-fitting model. For both reasons, the one-dimensional model should be preferred. In addition, we found that some empirical paradigms are ill-suited for distinguishing among models. For example, data collected by soliciting remember/know/new judgments-that is, the trinary task-provide a particularly weak ground for distinguishing models. Additional tables and figures may be downloaded from the Psychonomic Society's Archive of Norms, Stimuli, and Data, at www.psychonomic.org/archive.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 Objective, To develop a simple score to help assess the presence or absence of infection in critically ill patients using routinely available variables.
 Objective. To determine the incidence of body temperature (BT) alterations in critically ill patients, and their relationship with infection and outcome. Design. Prospective, observational study. Setting. Thirty-one bed, medico-surgical department of intensive care. Patients. Adult patients admitted consecutively to the ICU for at least 24 h, during 6 summer months. Interventions. None. esults. Fever (BTgreater than or equal to38.3degreesC) occurred in 139 (28.2%) patients and hypothermia (BTless than or equal to36degreesC) in 45 (9.1%) patients, at some time during the ICU stay. Fever was present in 52 of 100 (52.0%) infected patients without septic shock, and in 24 of 38 (63.2%) patients with septic shock. Hypothermia occurred in 5 of 100 (5.0%) infected patients without septic shock and in 5 of 38 (13.1%) patients with septic shock. Patients with hypothermia and fever had higher Sequential Organ Failure Assessment (SOFA) scores on admission (6.3+/-3.7 and 6.4+/-3.3 vs 4.6+/-3.2; p&lt;0.01), maximum SOFA scores during ICU stay (7.6+/-5.2 and 8.2+/-4.7 vs 5.4+/-3.8; p&lt;0.01) and mortality rates (33.3 and 35.3% vs 10.3%; p&lt;0.01). The length of stay (LOS) was longer in febrile patients than in hypothermic and normothermic (36degreesC&lt;BT&lt;38.3degreesC) patients [median 6 (1-57) vs 5 (2-28) and 3 (1-33) days, p=0.02 and p=0.01, respectively). Among the septic patients hypothermic patients were older than febrile patients (69+/-9 vs 54+/-7 years, p=0.01). Patients with septic shock had a higher mortality if they were hypothermic than if they were febrile (80 vs 50%, p&lt;0.01). onclusions. Both hypothermia and fever are associated with increased morbidity and mortality rates. Patients with hypothermia have a worse prognosis than those with fever.

-----------------------
 Case-based Reasoning (CBR) began as a theory of human cognition, but has attracted relatively little direct experimental or theoretical investigation in psychology. However, psychologists have developed a range of instance-based theories of cognition and have extensively studied how similarity to past cases can guide categorization of new cases. This paper considers the relation between CBR and psychological research, focussing on similarity in human and artificial case-based reasoning in law. We argue that CBR, psychology and legal theory have complementary contributions to understanding similarity, and describe what each offers. This allows us to establish criteria for assessing existing CBR systems in law and to establish what we consider to be the crucial goals for further research on similarity, both from a psychological and a CBR perspective.

 Normative theories provide essential tools for understanding behaviour, not just for reasoning, judgement, and decision-making, but many other areas of cognition as well; and their utility extends to the development of process theories. Furthermore, the way these tools are used has nothing to do with the is-ought fallacy. There therefore seems no basis for the claim that research would be better off without them.

-----------------------
 Case-based Reasoning (CBR) began as a theory of human cognition, but has attracted relatively little direct experimental or theoretical investigation in psychology. However, psychologists have developed a range of instance-based theories of cognition and have extensively studied how similarity to past cases can guide categorization of new cases. This paper considers the relation between CBR and psychological research, focussing on similarity in human and artificial case-based reasoning in law. We argue that CBR, psychology and legal theory have complementary contributions to understanding similarity, and describe what each offers. This allows us to establish criteria for assessing existing CBR systems in law and to establish what we consider to be the crucial goals for further research on similarity, both from a psychological and a CBR perspective.

 Normative theories provide essential tools for understanding behaviour, not just for reasoning, judgement, and decision-making, but many other areas of cognition as well; and their utility extends to the development of process theories. Furthermore, the way these tools are used has nothing to do with the is-ought fallacy. There therefore seems no basis for the claim that research would be better off without them.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

 Utterances expressing generic kinds ("birds fly") highlight qualities of a category that are stable and enduring, and thus provide insight into conceptual organization. To explore the role that linguistic input plays in children's production of generic nouns, we observed American and Chinese deaf children whose hearing losses prevented them from learning speech and whose hearing parents had not exposed them to sign. These children develop gesture systems that have language-like structure at many different levels. The specific question we addressed in this study was whether the gesture systems, developed without input from a conventional language model, would contain generics. We found that the deaf children used generics in the gestures they invented, and did so at about the same rate as hearing children growing up in the same cultures and learning English or Mandarin. Moreover, the deaf children produced more generics for animals than for artifacts, a bias found previously in adult English- and Mandarin-speakers and also found in both groups of hearing children in our current study. This bias has been hypothesized to reflect the different conceptual organizations underlying animal and artifact categories. Our results suggest that not only is a language model not necessary for young children to produce generic utterances, but the bias to produce more generics for animals than artifacts also does not require linguistic input to develop. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Various theories propose that count nouns are distinguished from mass nouns by their specification of individuation. We present evidence that, while 3-year-old children acquiring language extend words differentially on the basis of mass-count syntax, they quantify over individuals for both novel mass and count nouns. We suggest that children may begin acquisition with an underspecified representation of mass noun semantics, permitting quantification over both individuals and continuous quantities. Also, children may rely on ontologically based biases to guide quantification.

 How do children as young as 2 years of age know that numerals, like one, have exact interpretations, while quantifiers and words like a do not? Previous studies have argued that only numerals have exact lexical meanings. Children Could not use scalar implicature to strengthen numeral meanings, it is argued, since they fail to do so for quantifiers [Papafragou, A., &amp; Musolino,J. (2003). Scalar implicatures: Experiments at the semantics-pragmatics interface. Cognition, 86, 253-282]. Against this view, we present evidence that children's early interpretation Of numerals does rely on scalar implicature, and argue that differences between numerals and quantifiers are due to differences in the availability of the respective scales of which they are members. Evidence from previous studies establishes that (1) children can make scalar inferences when interpreting numerals, (2) children initially assign weak, non-exact interpretations to numerals when first acquiring their meanings, and (3) children can strengthen quantifier interpretations when scalar alternatives are made explicitly available. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 In this paper, a two by three approach to modeling categorization is presented. Similarity representations based upon a geometric, an additive tree and an additive cluster model are combined with an exemplar model and a prototype model in a single approach. The six models are applied to the categorization of pictorial known and unknown fruits and vegetables (Smits et al., 2002). For novel stimuli, the geometric exemplar model and the cluster models gave the best account, indicating a strategy where people compare stimuli with stored members on more general continua or a limited set of features. For well-known stimuli, the tree-based models gave the best account of the data, suggesting more elaborate taxonomic knowledge. More generally, the results show that different categorization models may perform better for different sets of stimuli, and that a systematic empirical comparison of such models is needed.

 This paper concerns the use of similarities based on geometric distance in models of categorization. Two problematic implications of such similarities are outlined. First, in a comparison between two stimuli, geometric distance implies that matching features are not taken into account. Second, missing features are assumed not to exist. Only nonmatching features enter into calculations of similarity. A new model is constructed that is based on the ALCOVE model (Kruschke, 1992), but it uses a feature-matching similarity measure (see, e.g., Tversky, 1977) rather than a geometric one. It is an on-line model in the sense that both dimensions and exemplars are constructed during the categorization process. The model accounts better than ALCOVE does for data with missing features (Experiments 1 and 2) and at least as well as ALCOVE for a data set without missing features (Nosofsky, Kruschke, &amp; McKinley, 1992). This suggests that, at least for some stimulus materials, similarity in categorization is more akin to a feature-matching procedure than to geometric distance calculation.

-----------------------
 Artificial face recognition systems typically do not attempt to handle very variable images. By comparison, human perceivers can recognize familiar faces over much more varied conditions. We describe a prototype face representation based on simple image-averaging. We have argued that this forms a good candidate for understanding human face perception. Here we examine the stability of these representations by asking (i) how quickly they converge; and (ii) how resistant they are to contamination due to previous misidentifications. We conclude that face averages provide promising representations for use in artificial recognition.

 The Bruce and Young (1986) framework makes a number of important distinctions between the types of representation needed to recognize a familiar face. Here, we return to these, focussing particularly on face recognition units. We argue that such representations need to incorporate idiosyncratic within-person variability, asking questions such as 'What counts as a picture of Harrison Ford?'. We describe a mechanism for achieving this, and discuss the relation between image variability and episodic face memories, in the context of behavioural and neurophysiological data.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 To examine the relationship between syntactic processes in language comprehension and language production, we compared structural persistence from sentence primes that speakers heard to persistence from primes that speakers produced. [Bock, J. K., &amp; Griffin, Z. M. (2000). The persistence of structural priming: transient activation or implicit learning? Journal of Experimental Psychology: General, 129, 177-192.] showed that the production of target priming structures increased the probability of spontaneously using the same structures to describe events in subsequent pictures that were semantically unrelated to the primes. These priming effects persisted across as many as ten intervening filler trials. The present studies replicated these results using auditorily presented primes to which participants only listened. The results indicated persistence of priming across all lags, with relative magnitudes of priming as large as those observed by Bock and Griffin. The implication is that structural priming is persistent regardless of the modality in which language structures are experienced, underscoring the power of priming as an implicit learning mechanism. (c) 2006 Elsevier B.V. All rights reserved.

 Structural priming refers to speakers' tendency to produce sentences with previously heard or produced syntactic structures. We review arguments and evidence for three common accounts of the functions of structural priming. One is that structural priming enhances fluency. Only some (reaction time and fluency measure) evidence supports this view. A second account argues that structural priming stems from implicit learning of how features of meaning are linked to syntactic configurations. We describe evidence suggesting that structural priming exhibits effects characteristic of both learning and implicitness. A third account claims that structural priming is an aspect of coordination or alignment among interlocutors. Consistent with this, some evidence shows that structural priming involves a shorter-term component that is broadly sensitive to repeated bindings of wide-ranging types of knowledge. Together, these observations suggest that structural priming is likely a multifaceted force that reflects implicit learning and, possibly independently, alignment among interlocutors.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 To examine the relationship between syntactic processes in language comprehension and language production, we compared structural persistence from sentence primes that speakers heard to persistence from primes that speakers produced. [Bock, J. K., &amp; Griffin, Z. M. (2000). The persistence of structural priming: transient activation or implicit learning? Journal of Experimental Psychology: General, 129, 177-192.] showed that the production of target priming structures increased the probability of spontaneously using the same structures to describe events in subsequent pictures that were semantically unrelated to the primes. These priming effects persisted across as many as ten intervening filler trials. The present studies replicated these results using auditorily presented primes to which participants only listened. The results indicated persistence of priming across all lags, with relative magnitudes of priming as large as those observed by Bock and Griffin. The implication is that structural priming is persistent regardless of the modality in which language structures are experienced, underscoring the power of priming as an implicit learning mechanism. (c) 2006 Elsevier B.V. All rights reserved.

 Structural priming refers to speakers' tendency to produce sentences with previously heard or produced syntactic structures. We review arguments and evidence for three common accounts of the functions of structural priming. One is that structural priming enhances fluency. Only some (reaction time and fluency measure) evidence supports this view. A second account argues that structural priming stems from implicit learning of how features of meaning are linked to syntactic configurations. We describe evidence suggesting that structural priming exhibits effects characteristic of both learning and implicitness. A third account claims that structural priming is an aspect of coordination or alignment among interlocutors. Consistent with this, some evidence shows that structural priming involves a shorter-term component that is broadly sensitive to repeated bindings of wide-ranging types of knowledge. Together, these observations suggest that structural priming is likely a multifaceted force that reflects implicit learning and, possibly independently, alignment among interlocutors.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Four experiments were conducted to investigate whether variations in orientation that profoundly affect the ability to imagine rotations also affect the ability to imagine projective transformations. For a basic rectilinear object and the three simpler Platonic Solids, imagining projective transformations (e.g., the casting of a shadow) was quite successful when the objects were aligned with the direction of projection. For the solids, this alignment occurred when the objects were generalized cylinders about axes aligned with the projection. As the objects were made more oblique to the projection, performance deteriorated markedly. When the objects were moderately aligned with the projection, performance depended on the orientation of the object and the orientation of the projection to the environment. We suggest that the imagination of projection and of rotation is a type of problem solving in which spatial structures are organized in relation to initially given properties of the objects and transformations. When there is alignment among the various structural components, this process of imagination works efficiently. Without such alignment, nonexperts often fail. We suggest that aligned (i.e., parallel and perpendicular) orientations are effective in spatial imagination because they are categorically distinct and singular, and they provide a critical form of redundancy. (C) 1996 Academic Press, Inc.

 The regular polyhedra, commonly known as the ''Platonic solids'', are fundamental three-dimensional structures. It is known that the ease of imagining one of these solids, the cube, varies radically with its orientation to the vertical. We demonstrate the same variation for perception and imagination of all three of the simpler Platonic solids: The cube, octahedron, and tetrahedron. In orientations of the objects that are relatively easy to comprehend, the objects are generalized cylinders about the vertical. In the difficult orientations, the objects are antiprismatic about the vertical. The critical difference between these structures is that generalized cylinders have uniform orientations of edges and surfaces about an object axis while antiprisms have nonuniform orientations. These results support strongly the view that the orientations of object features are important in spatial organization, that humans are highly sensitive to objective forms of regularity in spatial organization, and that the generalized cylinder is a form of spatial regularity that people find simple.

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Three experiments compared the learning of lower-dimensional family resemblance categories (4 dimensions) with the learning of higher-dimensional ones (8 dimensions). Category-learning models incorporating error-driven learning, hypothesis testing, or limited capacity attention predict that additional dimensions should either increase learning difficulty or decrease learning of individual features. Contrary to these predictions, the experiments showed no slower learning of high-dimensional categories; instead, subjects learned more features from high-dimensional categories than from low-dimensional categories. This result obtained both in standard learning with feedback and in noncontingent, observational learning. These results show that rather than interfering with learning, categories with more dimensions cause individuals to learn more. The authors contrast the learning of family resemblance categories with learning in classical conditioning and probability learning paradigms, in which competition among features is well documented.

 Besides traditional supervised classification learning, people can learn categories by inferring the missing features of category members. It has been proposed that feature inference learning promotes learning a category's internal structure (e.g., its typical features and interfeature correlations) whereas classification promotes the learning of diagnostic information. We tracked learners' eye movements and found in Experiment I that inference learners indeed fixated features that were unnecessary for inferring the missing feature, behavior consistent with acquiring the categories' internal structure. However, Experiments 3 and 4 showed that fixations were generally limited to features that needed to be predicted on future trials. We conclude that inference learning induces both supervised and unsupervised learning of category-to-feature associations rather than a general motivation to learn the internal structure of categories. (C) 2008 Elsevier Inc. All rights reserved.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 Three priming experiments investigated the role of attention and view changes when common objects were rotated in depth. Objects were shown in prime-probe trial pairs. Experiment 1 extended findings by Stankiewicz, Hummel, and Cooper (1998) showing that attended objects primed themselves in the same but not in a reflected view, whereas ignored objects only primed themselves in the same view. In Experiment 2, depth-rotations produced changes in the visible part structure between prime and probe view of an object. Priming after depth-rotation was more reduced for attended objects than for ignored objects. Experiment 3 showed that other depth rotations that did not change the perceived part structure revealed a priming pattern similar to that in Experiment 1, with equivalent reduction in priming for attended and ignored objects. These data indicate that recognition of attended objects is mediated by a part-based (analytic) representation together with a view-based (holistic) representation, whereas ignored images are recognized in a strictly view-dependent fashion.

 Three experiments investigated the role of attention in visual priming across rotations in the picture plane. Experiment 1 showed that naming latencies increased with the degree of misorientation for objects commonly seen in an upright view (base objects) but not for objects seen familiarly from many views (no-base objects). In Experiment 2, no-base objects revealed a priming pattern identical to that observed previously for left-right reflections (Stankiewicz, Hummel, &amp; Cooper, 1998): Attended objects primed themselves in the same and rotated views, whereas ignored images primed themselves only in the same view, with additive effects of attention and orientation. In Experiment 3 ignored base objects only primed themselves in a familiar (upright) view, indicating that priming only obtains when that image makes contact with object memory. These data challenge theories of object recognition that rely on any single representation of shape and contribute to evidence suggesting holistic (view-like) representations for ignored and analytic (view-insensitive) representations for attended objects.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 Working memory plays a crucial role in supporting learning, with poor progress in reading and mathematics characterizing children with low memory skills. This study investigated whether these problems can be overcome by a training program designed to boost working memory. Children with low working memory skills were assessed on measures of working memory, IQ and academic attainment before and after training on either adaptive or non-adaptive versions of the program. Adaptive training that taxed working memory to its limits was associated with substantial and sustained gains in working memory, with age-appropriate levels achieved by the majority of children. Mathematical ability also improved significantly 6 months following adaptive training. These findings indicate that common impairments in working memory and associated learning difficulties may be overcome with this behavioral treatment.

 The present study investigates how working memory and fluid intelligence are related in young children and how these links develop over time. The major aim is to determine which aspect of the working memory system-short-term storage or cognitive control drives the relationship with fluid intelligence. A sample of 119 children was followed from kindergarten to second grade and completed multiple assessments of working memory, short-term memory, and fluid intelligence. The data showed that working memory, short-term memory, and fluid intelligence were highly related but separate constructs in young children. The results further showed that when the common variance between working memory and short-term memory was controlled, the residual working memory factor manifested significant links with fluid intelligence whereas the residual short-term memory factor did not. These findings suggest that in young children cognitive control mechanisms rather than the storage component of working memory span tasks are the source of their link with fluid intelligence. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 Four experiments were conducted to investigate whether variations in orientation that profoundly affect the ability to imagine rotations also affect the ability to imagine projective transformations. For a basic rectilinear object and the three simpler Platonic Solids, imagining projective transformations (e.g., the casting of a shadow) was quite successful when the objects were aligned with the direction of projection. For the solids, this alignment occurred when the objects were generalized cylinders about axes aligned with the projection. As the objects were made more oblique to the projection, performance deteriorated markedly. When the objects were moderately aligned with the projection, performance depended on the orientation of the object and the orientation of the projection to the environment. We suggest that the imagination of projection and of rotation is a type of problem solving in which spatial structures are organized in relation to initially given properties of the objects and transformations. When there is alignment among the various structural components, this process of imagination works efficiently. Without such alignment, nonexperts often fail. We suggest that aligned (i.e., parallel and perpendicular) orientations are effective in spatial imagination because they are categorically distinct and singular, and they provide a critical form of redundancy. (C) 1996 Academic Press, Inc.

 The regular polyhedra, commonly known as the ''Platonic solids'', are fundamental three-dimensional structures. It is known that the ease of imagining one of these solids, the cube, varies radically with its orientation to the vertical. We demonstrate the same variation for perception and imagination of all three of the simpler Platonic solids: The cube, octahedron, and tetrahedron. In orientations of the objects that are relatively easy to comprehend, the objects are generalized cylinders about the vertical. In the difficult orientations, the objects are antiprismatic about the vertical. The critical difference between these structures is that generalized cylinders have uniform orientations of edges and surfaces about an object axis while antiprisms have nonuniform orientations. These results support strongly the view that the orientations of object features are important in spatial organization, that humans are highly sensitive to objective forms of regularity in spatial organization, and that the generalized cylinder is a form of spatial regularity that people find simple.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Oaksford &amp; Chater (O&amp;C) subscribe to the view that a conditional expresses a high conditional probability of the consequent, given the antecedent, but they model conditionals as expressing a dependency between antecedent and consequent. Therefore, their model is inconsistent with their theoretical commitment. The model is also inconsistent with some findings on how people interpret conditionals and how they reason from them.

 Parameter recovery of three different implementations of the Ratcliff diffusion model was investigated: the EZ model (Wagenmakers, van der Maas, &amp; Grasman, 2007), fast-dm (Voss &amp; Voss, 2007), and DMAT (Vandekerckhove &amp; Tuerlinckx, 2007). Their capacity to recover both the mean structure and individual differences in parameter values was explored. The three methods were applied to simulated data generated by the diffusion model, by the leaky, competing accumulator (LCA) model (Usher &amp; McClelland, 2001) and by the linear ballistic accumulator (LBA) model (Brown &amp; Heathcote, 2008). Results show that EZ and DMAT are better capable than fast-dm in recovering experimental effects on parameters. EZ was best in recovering individual differences in parameter values. When data were generated by the LCA model, the diffusion model estimates obtained with all three methods correlated well with corresponding LCA model parameters. No such one-on-one correspondence could be established between parameters of the LBA model and the diffusion model. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 Access to visual awareness is often determined by covert, voluntary deployments of visual attention. Voluntary orienting without eye movements requires decoupling attention from the locus of fixation, a shift to the desired location, and maintenance of attention at that location. We used event-related functional magnetic resonance imaging to dissociate these components while observers shifted attention among 3 streams of letters and digits, one located at fixation and 2 in the periphery. Compared with holding attention at the current location, shifting attention between the peripheral locations was associated with transient increases in neural activity in the superior parietal lobule (SPL) and frontal eye fields (FEF), as in previous studies. The supplementary eye fields and separate portions of SPL and FEF were more active for decoupling attention from fixation than for shifting attention to a new location. Large segments of precentral sulcus (PreCS) and posterior parietal cortex (PPC) were more active when attention was maintained in the periphery than when it was maintained at fixation. We conclude that distinct subcomponents of the dorsal frontoparietal network initiate redeployments of covert attention to new locations and disengage attention from fixation, while sustained activity in lateral regions of PPC and PreCS represents sustained states of peripheral attention.

 Visual attention selects task-relevant information from scenes to help achieve behavioral goals. Attention can be deployed within multiple domains to select specific spatial locations, features, or objects. Recent evidence has shown that voluntary shifts of attention in multiple domains are consistently associated with transient increases in cortical activity in medial superior parietal lobule, suggesting that this may be the source of a domain-independent control signal that initiates the reconfiguration of attention. To investigate this hypothesis, we used fMRI to measure changes in cortical activation while human subjects shifted attention between spatial locations or between colors at a location. Univariate multiple regression analysis revealed a common, domain-independent transient signal [in posterior parietal cortex (PPC) and prefrontal cortex] time-locked to shifts of attention in both domains. However, multivariate pattern classification conducted on the cortical surface revealed that the spatiotemporal pattern of activity within PPC differed reliably for spatial and feature-based attention shifts. These results suggest that the posterior parietal cortex is a common hub for the control of attention shifts but contains subpopulations of neurons with domain-specific tuning for cognitive control.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 In order to create adaptive Agent Systems with abilities matching those of their biological counterparts, a natural approach is to incorporate classical conditioning mechanisms into such systems. However, existing models for classical conditioning are usually based on differential equations. Since the design of Agent Systems is traditionally based on qualitative conceptual languages, these differential equations are often not directly appropriate to serve as an input for Agent System design. To deal with this problem, this paper explores a formal description and analysis of a conditioning process based on logical specification and analysis methods of dynamic properties of conditioning. Specific types of dynamic properties are global properties, describing properties of the process as a whole, or local properties, describing properties of basic steps in a conditioning process. If the latter type of properties are specified in an executable format, they provide a temporal declarative specification of a simulation model. Global properties can be checked automatically for simulated or other traces. Using these methods the properties of conditioning processes informally expressed by Los and Heuvel [8] have been formalised and verified against a specification of local properties based on Machado [9]'s mathematical model.

 In the literature classical conditioning is usually described and analysed informally. If formalisation is used, this is often based on mathematical models based on difference or differential equations. This paper explores a formal description and analysis of the process of trace conditioning, based on logical specification and analysis methods of dynamic properties of the process. Specific types of dynamic properties are global dynamic properties, describing properties of the process as a whole, or local dynamic properties, describing properties of basic steps in a conditioning process. If the latter type of properties are specified in an executable format, they provide a temporal declarative specification of a simulation model. By a software environment these local properties can be used to actually perform simulation. Global properties can be checked automatically for simulated or other traces. Using these methods the properties of conditioning processes informally expressed by Los and Van Den Heuvel [ Los, S. A., &amp; Van Den Heuvel, C. E. ( 2001). Intentional and unintentional contributions to non-specific preparation during reaction time foreperiods. Journal of Experimental Psychology: Human Perception and Performance, 27, 370 - 386] have been formalised and verified against a specification of local properties based on Machado's [ Machado, A. ( 1997). Learning the temporal dynamics of behaviour. Psychological Review, 104, 241 - 265] differential equation model. (c) 2006 Elsevier B. V. All rights reserved.

-----------------------
 Previous studies have shown that EEG activity in the gamma range can be modulated by attention. Here, we compared this activity for voluntary and involuntary spatial attention in a spatial-cueing paradigm with faces as targets. The stimuli and trial timing were kept constant across attention conditions with only the predictive value of the cue changing. Gamma-band response was linked to voluntary shifts of attention, but not to the involuntary capture of attention. The presence of increased gamma responses for the voluntary allocation of attention, and its absence in cases of involuntary capture suggests that the neural mechanisms governing these two types of attention are different. Moreover, these data allow a description of the temporal dynamics contributing to the dissociation between voluntary and involuntary attention. The distribution of this correlate of voluntary attention is consistent with a top-down process involving contralateral anterior and posterior regions.

 We propose that voluntary and involuntary attention affect different mechanisms and have different consequences for performance measured in reaction time. Voluntary attention enhances the perceptual representation whereas involuntary attention affects the tendency to respond to stimuli in one location or another. In a spatial-cueing paradigm, we manipulated perceptual difficulty and compared voluntary and involuntary attention. For the voluntary-attention condition, the spatial cue was predictive of the target location, whereas in the involuntary-attention condition it was not. Increasing perceptual difficulty increased the attention effect with Voluntary attention, but decreased it with involuntary attention. Thus voluntary and involuntary attention have different consequences when perceptual difficulty is manipulated and hence are probably caused by different mechanisms.

-----------------------
 The authors evaluated 4 sequential sampling models for 2-choice decisions-the Wiener diffusion, Ornstein-Uhlenbeck (OU) diffusion, accumulator, and Poisson counter models-by fitting them to the response time (RT) distributions and accuracy data from 3 experiments. Each of the models was augmented with assumptions of variability across trials in the rate of accumulation of evidence from stimuli, the values of response criteria, and the value of base RT across trials. Although there was substantial model mimicry, empirical conditions were identified under which the models make discriminably different predictions. The best accounts of the data were provided by the Wiener diffusion model, the OU model with small-to-moderate decay, and the accumulator model with long-tailed (exponential) distributions of criteria, although the last was unable to produce error RTs shorter than correct RTs. The relationship between these models and 3 recent, neurally inspired models was also examined.

 In this letter, we examine the computational mechanisms of reinforcement-based decision making. We bridge the gap across multiple levels of analysis, from neural models of corticostriatal circuits-the basal ganglia (BG) model (Frank, 2005, 2006) to simpler but mathematically tractable diffusion models of two-choice decision making. Specifically, we generated simulated data from the BG model and fit the diffusion model (Ratcliff, 1978) to it. The standard diffusion model fits underestimated response times under conditions of high response and reinforcement conflict. Follow-up fits showed good fits to the data both by increasing nondecision time and by raising decision thresholds as a function of conflict and by allowing this threshold to collapse with time. This profile captures the role and dynamics of the subthalamic nucleus in BG circuitry, and as such, parametric modulations of projection strengths from this nucleus were associated with parametric increases in decision boundary and its modulation by conflict. We then present data from a human reinforcement learning experiment involving decisions with low-and high-reinforcement conflict. Again, the standard model failed to fit the data, but we found that two variants similar to those that fit the BG model data fit the experimental data, thereby providing a convergence of theoretical accounts of complex interactive decision-making mechanisms consistent with available data. This work also demonstrates how to make modest modifications to diffusion models to summarize core computations of the BG model. The result is a better fit and understanding of reinforcement-based choice data than that which would have occurred with either model alone.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 Eye movements were monitored as subjects read sentences containing high- or low-predictable target words. The extent to which target words were predictable from prior context was varied: Half of the target words were predictable, and the other half were unpredictable. In addition, the length of the target word varied: The target words were short (4-6 letters), medium (7-9 letters), or long (10-12 letters). Length and predictability both yielded strong effects on the probability of skipping the target words and on the amount of time readers fixated the target words (when they were not skipped). However, there was no interaction in any of the measures examined for either skipping or fixation time. The results demonstrate that word predictability (due to contextual constraint) and word length have strong and independent influences on word skipping and fixation durations. Furthermore, because the long words extended beyond the word identification span, the data indicate that skipping can occur on the basis of partial information in relation to word identity.

 We examined the initial landing position of the eyes in target words that were either predictable or unpredictable from the preceding sentence context. Although readers skipped over predictable words more than unpredictable words and spent less time on predictable words when they did fixate on them, there was no difference in the launch site of the saccade to the target word. Moreover, there was only a very small difference in the initial landing position on the target word as a function of predictability when the target words were fixated which is most parsimoniously explained by positing that a few programmed skips of the target word fell short of their intended target. These results suggest: that low-level processing is primarily responsible for landing position effects in reading. (C) 2001 Elsevier Science Ltd. All rights reserved.

-----------------------
 We argue that psycholinguistics should be concerned with both the representation and the processing of language. Recent experimental work on syntax in language comprehension has largely concentrated on the way in which language is processed and has assumed that theoretical linguistics serves to determine the representation of language. In contrast, we advocate experimental work on the mental representation of grammatical knowledge, and argue that syntactic priming is a promising way to do this. Syntactic priming is the phenomenon whereby exposure to a sentence with a particular syntactic construction can affect the subsequent processing of an otherwise unrelated sentence with the same (or, perhaps, related) structure, for reasons of that structure. We assess evidence for syntactic priming in corpora, and then consider experimental evidence for priming in production and comprehension and for bidirectional priming between comprehension and production. This in particular strongly suggests that priming is tapping into linguistic knowledge itself; and is not just facilitating particular processes. The final section discusses the importance of priming evidence for any account of language construed as the mental representation of human linguistic capacities.

 Repetition is a central phenomenon of behavior, and researchers have made extensive use of it to illuminate psychological functioning. In the language sciences, a ubiquitous form of such repetition is structural priming, a tendency to repeat or better process a current sentence because of its structural similarity to a previously experienced ("prime") sentence (J. K. Bock, 1986). The recent explosion of research in structural priming has made it the dominant means of investigating the processes involved in the production (and increasingly, comprehension) of complex expressions such as sentences. This review considers its implications for the representation of syntax and the mechanisms of production and comprehension and their relationship. It then addresses the potential functions of structural priming, before turning to its implications for first language acquisition, bilingualism, and aphasia. The authors close with theoretical and empirical recommendations for future investigations.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 Many thousands of students take standardized tests every year. In the current research, we asked whether answering standardized test questions affects students' later test performance. Prior research has shown both positive and negative effects of multiple-choice testing on later tests, with negative effects arising from students selecting incorrect alternatives on multiple-choice tests and then believing they were correct (Roediger &amp; Marsh, 2005). In the current experiments, undergraduates and high school students answered multiple-choice questions retired from SAT II tests (that are no longer in the testing pool) on biology, chemistry, U.S. history, and world history, and later answered cued-recall questions about these subjects. In 3 experiments, we observed positive testing effects: More final cued-recall questions were answered correctly if the items had appeared on the initial multiple-choice test. We also sometimes observed negative testing effects: intrusions of multiple-choice distractors as answers on the final cued-recall test. Students who scored well on the initial test benefited from taking the test, but lower achieving students showed either less benefit (undergraduates) or cost,; from the testing (high school students).

 Multiple-choice testing has both positive and negative consequences for performance on later tests. Prior testing increases the number of questions answered correctly on a later test but also increases the likelihood that questions will be answered with lures from the previous multiple-choice test (Roediger &amp; Marsh, 2005). Prior research has shown that the positive effects of testing persist over a delay, but no one has examined the durability of the negative effects of testing. To address this, subjects took multiple-choice and cued recall tests (on subsets of questions) both immediately and a week after studying. Although delay reduced both the positive and negative testing effects, both still occurred after 1 week, especially if the multiple-choice test had also been delayed. These results are consistent with the argument that recollection underlies both the positive and negative testing effects.

-----------------------
 An experiment with a multiple-cue judgment task tested the hypothesis that humans can only abstract explicit representations of cue-criterion relations when the cues are related to the criterion by an additive function. It is proposed that the sequential and capacity-constrained nature of controlled, explicit thought can only induce and execute linear additive cue integration; non-additive environments require exemplar memory. The results showed that an additive task induced processes of cue abstraction and cue integration, while a multiplicative task induced exemplar processes. The results suggest flexible interplay between distinct representation-levels, a preference to abstract explicit "rules" whenever possible, although this capacity is constrained to additive cue-criterion relations.

 The majority of previous studies on multiple-cue judgment with continuous cues have involved comparisons between judgments and multiple linear regression models that integrated cues into a judgment. The authors present an experiment indicating that in a judgment task with additive combination of multiple continuous cues, people indeed displayed abstract knowledge of the cue criterion relations that was mentally integrated into a judgment, but in a task with multiplicative combination of continuous cues, people instead relied on retrieval of memory traces of similar judgment cases (exemplars). These results suggest that people may adopt qualitatively distinct forms of knowledge, depending on the structure of a multiple-cue judgment task. The authors discuss implications for theories of multiple-cue judgment.

-----------------------
 Learning is typically modelled as a domain-general, data driven, associative process. Even though the potential influence of top-down knowledge is often acknowledged, the typical theoretical approach postulates two separate modules for knowledge and learning. According to this view, knowledge may influence the initial defaults and the output of the learning process but not the structure of the learning mechanism itself. In contrast to this modular approach, this article defends the position that learning and prior knowledge interact. Theoretical analyses and empirical studies are presented that indicate that specific and abstract domain knowledge influence the structure of the learning processes.

 The dominant theoretical. approach to causal learning postulates the acquisition of associative weights between cues and outcomes. This reduction of causal induction to associative learning implies that learners are insensitive to important characteristics of causality, such as the inherent directionality between causes and effects. An ongoing debate centers on the question of whether causal learning is sensitive to causal directionality (as is postulated by causal-model theory) or whether it neglects this important feature of the pkysical world (as implied by associationist theories). Three experiments using different cue competition paradigms are reported that demonstrate the competence of human learners to differentiate between predictive and diagnostic learning. However, the experiments also show that this competence displays itself best in learning situations with few processing demands and with convincingly conveyed causal structures. The study provides evidence for the necessity to distinguish between competence and performance in causal learning.

-----------------------
 Time-accuracy functions for tasks involving single-digit mental addition and subtraction were derived in a sample of 18 younger (mean age = 21.7 years) and 16 older adults (mean age = 68.8 years). Sequential complexity was manipulated by varying the number of operations (5 vs. 10); coordinative complexity was induced by bracketing. Age differences were apparent in the coordinative conditions, even though no age difference was present in the sequential conditions. This indicates that the age difference under conditions of high coordinative demands could not be attributed solely to a decline in basic speed of processing. The Age x Complexity interaction was due to larger onset times and lower asymptotic performance by the older adults in the coordinative conditions but not due to to rate of approach to the asymptote. This implies that coordinative demands do not differentially hurt access from semantic memory in older adults; however, coordinative demands do have disproportionately negative consequences for computation speed and self-monitoring in elder adults.

 This study investigated whether older adults could acquire the ability to perform 2 cognitive operations in parallel in a paradigm in which young adults had been shown to be able to do so (K. Oberauer &amp; R. Kliegl, 2004). Twelve young and 12 older adults practiced a numerical and a visuospatial continuous memory updating task in single-task and dual-task conditions for 16 to 24 sessions. After practice, 9 young adults were able to process the 2 tasks without dual-task costs, but none of the older adults had reached the criterion of parallel processing. The results suggest a qualitative difference between young and older adults in how they approach dual-task situations.

-----------------------
 Recent research shows that similarity comparisons involve an alignment process in which features are placed into correspondence. In 6 studies, the authors showed that alignment is involved in category learning as well. Within a category, aligned matches (feature matches occurring on the same dimension) facilitate learning more than nonaligned matches do (matches on different dimensions), although nonaligned matches still facilitate learning relative to nonmatches. Analogously, feature matches that cross category boundaries hurt learning more if they occur on the same versus a different dimension, and cross-category feature matches on different dimensions hurt learning relative to nonmatching features. Representational assumptions of category learning models must be modified to account for the differences between aligned and nonaligned feature matches.

 Three experiments compared the learning of lower-dimensional family resemblance categories (4 dimensions) with the learning of higher-dimensional ones (8 dimensions). Category-learning models incorporating error-driven learning, hypothesis testing, or limited capacity attention predict that additional dimensions should either increase learning difficulty or decrease learning of individual features. Contrary to these predictions, the experiments showed no slower learning of high-dimensional categories; instead, subjects learned more features from high-dimensional categories than from low-dimensional categories. This result obtained both in standard learning with feedback and in noncontingent, observational learning. These results show that rather than interfering with learning, categories with more dimensions cause individuals to learn more. The authors contrast the learning of family resemblance categories with learning in classical conditioning and probability learning paradigms, in which competition among features is well documented.

-----------------------
 Recent research shows that similarity comparisons involve an alignment process in which features are placed into correspondence. In 6 studies, the authors showed that alignment is involved in category learning as well. Within a category, aligned matches (feature matches occurring on the same dimension) facilitate learning more than nonaligned matches do (matches on different dimensions), although nonaligned matches still facilitate learning relative to nonmatches. Analogously, feature matches that cross category boundaries hurt learning more if they occur on the same versus a different dimension, and cross-category feature matches on different dimensions hurt learning relative to nonmatching features. Representational assumptions of category learning models must be modified to account for the differences between aligned and nonaligned feature matches.

 Three experiments compared the learning of lower-dimensional family resemblance categories (4 dimensions) with the learning of higher-dimensional ones (8 dimensions). Category-learning models incorporating error-driven learning, hypothesis testing, or limited capacity attention predict that additional dimensions should either increase learning difficulty or decrease learning of individual features. Contrary to these predictions, the experiments showed no slower learning of high-dimensional categories; instead, subjects learned more features from high-dimensional categories than from low-dimensional categories. This result obtained both in standard learning with feedback and in noncontingent, observational learning. These results show that rather than interfering with learning, categories with more dimensions cause individuals to learn more. The authors contrast the learning of family resemblance categories with learning in classical conditioning and probability learning paradigms, in which competition among features is well documented.

-----------------------
 Premature sampling is a plausible modification of the random walk that is regarded as sufficient: in allowing the sequential probability ratio test (SPRT) model of choice response time to account for the frequent observation that mean error response latency is smaller than that for correct responses. This paper establishes a mathematical framework for random walks with premature sampling. Four important points are made. First, Wald's Identity and the small steps assumption fail to make the problem tractable in its fullest generality. Second, Laming's (1968) derivation of the important result that premature sampling leads to smaller mean latencies for error responses than correct responses in the SPRT model may be flawed by a tenuous approximation. Third, expressions for response probabilities and mean latencies are derived for the general model on the assumption that premature sampling is not sufficient, by itself, to trigger a response, although it does influence the process. Fourth, the SPRT model with premature sampling does not necessarily imply that when conditioned on a response, mean response time for error responses is smaller than that for correct responses-and counterexamples to this claim are provided. (C) 1996 Academic Press

 A dominant theme in modeling human perceptual judgments is that sensory neural activity is summed or integrated until a critical bound is reached. Such models predict that, in general, the shape of response time distributions change across conditions, although in practice, this shape change may be subtle. An alternative view is that response time distributions are shape invariant across conditions or groups. Shape invariance is predicted by some race models in which the first of several parallel fibers to communicate the signal determines the response. We competitively assess a specific gradual growth model, the one-bound diffusion model, against a natural shape-invariant competitor: shape invariance in an inverse Gaussian distribution. Assessment of subtle shape change versus shape invariance of response time distributions is aided by a Bayesian approach that allows the pooling of information across multiple participants. We find, conditional on reasonable distributional assumptions, subtle shape changes in response time that are highly concordant with a simple diffusion gradual growth model and discordant with shape invariance.

-----------------------
 To examine the relationship between syntactic processes in language comprehension and language production, we compared structural persistence from sentence primes that speakers heard to persistence from primes that speakers produced. [Bock, J. K., &amp; Griffin, Z. M. (2000). The persistence of structural priming: transient activation or implicit learning? Journal of Experimental Psychology: General, 129, 177-192.] showed that the production of target priming structures increased the probability of spontaneously using the same structures to describe events in subsequent pictures that were semantically unrelated to the primes. These priming effects persisted across as many as ten intervening filler trials. The present studies replicated these results using auditorily presented primes to which participants only listened. The results indicated persistence of priming across all lags, with relative magnitudes of priming as large as those observed by Bock and Griffin. The implication is that structural priming is persistent regardless of the modality in which language structures are experienced, underscoring the power of priming as an implicit learning mechanism. (c) 2006 Elsevier B.V. All rights reserved.

 Structural priming refers to speakers' tendency to produce sentences with previously heard or produced syntactic structures. We review arguments and evidence for three common accounts of the functions of structural priming. One is that structural priming enhances fluency. Only some (reaction time and fluency measure) evidence supports this view. A second account argues that structural priming stems from implicit learning of how features of meaning are linked to syntactic configurations. We describe evidence suggesting that structural priming exhibits effects characteristic of both learning and implicitness. A third account claims that structural priming is an aspect of coordination or alignment among interlocutors. Consistent with this, some evidence shows that structural priming involves a shorter-term component that is broadly sensitive to repeated bindings of wide-ranging types of knowledge. Together, these observations suggest that structural priming is likely a multifaceted force that reflects implicit learning and, possibly independently, alignment among interlocutors.

-----------------------
 The verbs cause, enable, and prevent express beliefs about the way the world works. We offer a theory of their meaning in terms of the structure of those beliefs expressed using qualitative properties of causal models, a graphical framework for representing causal structure. We propose that these verbs refer to a causal model relevant to a discourse and that "A causes B" expresses the belief that the causal model includes a link from A to B. "A enables/allows B" entails that the model includes a link from A to B, that A represents a category of events necessary for B, and that an alternative cause of B exists. "A prevents B" entails that the model includes a link from A to B and that A reduces the likelihood of B. This theory is able to account for the results of four experiments as well as a variety of existing data on human reasoning.

 Under the suppositional account of conditionals, when people think about a conditional assertion, "if p then q." they engage in a mental simulation in which they imagine p holds and evaluate the probability that q holds under this supposition. One implication of this account is that belief in a conditional equates to conditional probability [P(q/p)]. In this paper, the authors examine a further implication of this analysis with respect to the wide-scope negation of conditional assertions, "it is not the case that if p then q." Under the suppositional account, nothing categorically follows from the negation of a conditional, other than a second conditional, "if p then not-q." In contrast, according to the mental model theory, a negated conditional is consistent only with the determinate state of affairs, p and not-q. In 4 experiments, the authors compare the contrasting predictions that arise from each of these accounts. The findings are consistent with the suppositional theory but are incongruent with the mental model theory of conditionals.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 This paper presents a cognitive account of the process of evaluating scientific data. Our account assumes that when individuals evaluate data, they construct a mental model of a data-interpretation package, in which the data and theoretical interpretations of the data are integrated. We propose that individuals attempt to discount data by seeking alternative explanations for events within the mental model; data-interpretation packages are accepted when the individual cannot find alternative accounts for these events. Our analysis indicates that there are many levels at which data-interpretation packages can be accepted or denied.

 This article reports the results of a study investigating how undergraduates evaluate realistic scientific data in the domains of geology and paleontology. The results are used to test several predictions of a theory of data evaluation, which we call models-of-data theory. Models-of-data theory assumes that when evaluating data, the individual constructs a particular kind of cognitive model that integrates many features of the data with a theoretical interpretation of the data, The individual evaluates the model by attempting to generate alternative causal explanations for the events in the model. We contrast models-of-data theory with other proposals for how data are cognitively represented and show that models-of-data theory gives a good account of the pattern of written evaluations of data produced by the undergraduates in the study, We discuss theoretical and instructional implications of the theory.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 This paper used self-paced reading to test processing preferences in pronoun interpretation in English two clause sentences. The results demonstrate that people's preferences can be reversed by changing the coherence relation between the clauses. The results are not compatible with the existence of a single all-purpose strategy in pronoun resolution. Rather, the results support Kehler's (2002) hypothesis that the processing patterns observed in pronoun processing are a byproduct of more general cognitive inference processes underlying the establishment of coherence, such that discourse coherence guides pronoun reference, and pronoun reference guides discourse coherence.

 The present study compares the processing of unambiguous restrictive and non-restrictive relative clauses (RCs) within both a null context and a supportive discourse using a self-paced reading methodology. Individuals read restrictive RCs more slowly than non-restrictive RCs in a null context, but processed restrictive RCs faster than non-restrictive RCs in supportive context, resulting in an interaction between context and RC type. These results provide evidence for two theoretical points. First, principles analogous to those in referential theory [Altmann G. T. M., T Steedman, M. (1988). Interaction with context during human sentence processing. Cognition, 30, 191-238; Crain, S., T Steedman, M. (1985). On not being led up the garden path: The use of context by the psychological parser. In D. Dowty, L. Karttunnen, A. Zwicky (Eds.), Natural language parsing. Cambridge, UK: Cambridge University Press] apply not only in resolving ambiguity but also in processing unambiguous sentences. Second, the discourse context can guide and facilitate interpretive processing. This result suggests that intrasentential factors such as syntax are not autonomous from contextual processing, contrary to the modularity hypothesis [Fodor, J. A. (1983). Modularity of mind. Cambridge, MA: MIT Press]. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 Attention is a complex multilevel system subserved by at least three interacting attentional networks in the brain. This paper describes a multilevel computational model of attentional networks, developed in both the symbolic architecture of ACT-R and the connectionist framework of leabra. We evaluated the model using the Attentional Networks Test and the simulation results fitted the empirical data well. We argue that developing multilevel computational models helps to link findings at different levels.

 Recent evidence in cognitive neuroscience has Suggested that attention is a complex organ system subserved by at least three attentional networks in the brain, for alerting, orienting, and executive control functions. However, how these different networks work together to give rise to the seemingly unitary mental faculty of attention remains unclear. We describe a connectionist model of human attentional networks to explore the possible interplays among the networks from a Computational perspective. This model is developed in the framework of leabra (local, error-driven, and associative, biologically realistic algorithm) and simultaneously involves these attentional networks connected in a biologically inspired way. We evaluate the model by simulating the empirical data collected on normal human subjects using the Attentional Network Test (ANT). The simulation results fit the experimental data well. In addition, we show that the same model, with a single parameter change that affects executive control, is able to simulate the empirical data collected from patients with schizophrenia. This model represents a plausible connectionist explanation for the functional structure and interaction of human attentional networks.

-----------------------
 This article reviews the rationale for using accumulative one-step-ahead prediction error (APE) as a data-driven method for model selection. Theoretically, APE is closely related to Bayesian model selection and the method of minimum description length (MDL). The sole requirement for using APE is that the models under consideration are capable of generating a prediction for the next, unseen data point. This means that APE may be readily applied to selection problems involving very complex models. APE automatically takes the functional form of parameters into account, and the 'plug-in' version of APE does not require the specification of priors. APE is particularly easy to compute for data that have a natural ordering, such as time series. Here, we explore the possibility of using APE to discriminate the short-range ARMA(1, 1) model from the long-range ARFIMA(0,d, 0) model. We also illustrate how APE may be used for model meta-selection, allowing one to choose between different model selection methods. (c) 2006 Elsevier Inc. All rights reserved.

 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 At least 3 different types of computational model have been shown to account for various facets of both normal and impaired single word reading: (a) the connectionist triangle model, (b) the dual-route cascaded model, and (c) the connectionist dual process model. Major strengths and weaknesses of these models are identified. In the spirit of nested incremental modeling, a new connectionist dual process model (the CDP+ model) is presented. This model builds on the strengths of 2 of the previous models while eliminating their weaknesses. Contrary to the dual-route cascaded model, CDP+ is able to learn and produce graded consistency effects. Contrary to the triangle and the connectionist dual process models, CDP+ accounts for serial effects and has more accurate nonword reading performance. CDP+ also beats all previous models by an order of magnitude when predicting individual item-level variance on large databases. Thus, the authors show that building on existing theories by combining the best features of previous models-a nested modeling strategy that is commonly used in other areas of science but often neglected in psychology-results in better and more powerful computational models.

 This article describes the Dual Route Cascaded (DRC) model, a computational model of visual word recognition and reading aloud. The DRC is a computational realization of the dual-route theory of reading, and is the only computational model of reading that can perform the 2 tasks most commonly used to study reading: lexical decision and reading aloud. For both tasks, the authors show that a wide variety of variables that influence human latencies influence the DRC model's latencies in exactly the same way. The DRC model simulates a number of such effects that other computational models of reading do not, but there appear to be no effects that any other current computational model of reading can simulate but that the DRC model cannot. The authors conclude that the DRC model is the most successful of the existing computational models of reading.

-----------------------
 Objective: To investigate the long-term effect of tibolone on mammographic density.
 Objective To investigate the effect of 10 years of treatment with tibolone on aortic stiffness and endothelial function.
-----------------------
 Eye movements were monitored as subjects read sentences containing high- or low-predictable target words. The extent to which target words were predictable from prior context was varied: Half of the target words were predictable, and the other half were unpredictable. In addition, the length of the target word varied: The target words were short (4-6 letters), medium (7-9 letters), or long (10-12 letters). Length and predictability both yielded strong effects on the probability of skipping the target words and on the amount of time readers fixated the target words (when they were not skipped). However, there was no interaction in any of the measures examined for either skipping or fixation time. The results demonstrate that word predictability (due to contextual constraint) and word length have strong and independent influences on word skipping and fixation durations. Furthermore, because the long words extended beyond the word identification span, the data indicate that skipping can occur on the basis of partial information in relation to word identity.

 We examined the initial landing position of the eyes in target words that were either predictable or unpredictable from the preceding sentence context. Although readers skipped over predictable words more than unpredictable words and spent less time on predictable words when they did fixate on them, there was no difference in the launch site of the saccade to the target word. Moreover, there was only a very small difference in the initial landing position on the target word as a function of predictability when the target words were fixated which is most parsimoniously explained by positing that a few programmed skips of the target word fell short of their intended target. These results suggest: that low-level processing is primarily responsible for landing position effects in reading. (C) 2001 Elsevier Science Ltd. All rights reserved.

-----------------------
 Background: Most people show a remarkable deficit to report the second of two targets when presented in close temporal succession, reflecting an attentional restriction known as the 'attentional blink' (AB). However, there are large individual differences in the magnitude of the effect, with some people showing no such attentional restrictions.
 Background: Most people show a remarkable deficit in reporting the second of two targets (T2) when presented 200-500 ms after the first (T1), reflecting an 'attentional blink' (AB). However, there are large individual differences in the magnitude of the effect, with some people, referred to as 'non-blinkers', showing no such attentional restrictions.
-----------------------
 In the current literature about human learning, the distinction between implicit and explicit learning is broadly accepted. In the present paper we will argue that the separation into two types of learning has no solid ground - neither theoretically nor empirically. Based on recent definitions of implicit and explicit learning, the distinction between intentional and non-intentional learning is suggested as the scientifically more useful one - a distinction that has been already drawn at the beginning of the history of German psychology. Two research traditions - Wundt' s theory of apperception and Kohler's concept of,,Gestalten" - are exemplary discussed in order to demonstrate that, on a theoretical level, the concept of "intention" is not a major variable for understanding human learning. Also, the empirical distinction between two types of learning, for instance, dissociations between intentional and non-intentional sequence learning, is problematic because of limitations in the interpretation of indicators for both types of learning.

 Sequence teaming tasks rue very simple choice reaction tasks which have become surprisingly popular in cognitive psychology. We analyse possible causes for this popularity by looking at the theoretical status of sequence teaming tasks, and we provide a sketch of the relevant research activities, Two categories of research activities are distinguished. First, sequence learning tasks are used to investigate dissociations between qualitatively different explicit (often referred to as 'conscious') and implicit (often referred to as 'unconscious') learning processes. Second, learning conditions are manipulated directly in order to analyse the mechanisms underlying sequence learning. We conclude that the claimed dissociations have yet to be demonstrated convincingly. Nevertheless, the use of sequence learning tasks can result in valuable contributions to the understanding of sequence learning mechanisms per se.

-----------------------
 To examine the relationship between syntactic processes in language comprehension and language production, we compared structural persistence from sentence primes that speakers heard to persistence from primes that speakers produced. [Bock, J. K., &amp; Griffin, Z. M. (2000). The persistence of structural priming: transient activation or implicit learning? Journal of Experimental Psychology: General, 129, 177-192.] showed that the production of target priming structures increased the probability of spontaneously using the same structures to describe events in subsequent pictures that were semantically unrelated to the primes. These priming effects persisted across as many as ten intervening filler trials. The present studies replicated these results using auditorily presented primes to which participants only listened. The results indicated persistence of priming across all lags, with relative magnitudes of priming as large as those observed by Bock and Griffin. The implication is that structural priming is persistent regardless of the modality in which language structures are experienced, underscoring the power of priming as an implicit learning mechanism. (c) 2006 Elsevier B.V. All rights reserved.

 Structural priming refers to speakers' tendency to produce sentences with previously heard or produced syntactic structures. We review arguments and evidence for three common accounts of the functions of structural priming. One is that structural priming enhances fluency. Only some (reaction time and fluency measure) evidence supports this view. A second account argues that structural priming stems from implicit learning of how features of meaning are linked to syntactic configurations. We describe evidence suggesting that structural priming exhibits effects characteristic of both learning and implicitness. A third account claims that structural priming is an aspect of coordination or alignment among interlocutors. Consistent with this, some evidence shows that structural priming involves a shorter-term component that is broadly sensitive to repeated bindings of wide-ranging types of knowledge. Together, these observations suggest that structural priming is likely a multifaceted force that reflects implicit learning and, possibly independently, alignment among interlocutors.

-----------------------
 This paper reviews the uneven history of the relationship between Anthropology and Cognitive Science over the past 30 years, from its promising beginnings, followed by a period of disaffection, on up to the current context, which may lay the groundwork for reconsidering what Anthropology and (the rest of) Cognitive Science have to offer each other. We think that this history has important lessons to teach and has implications for contemporary efforts to restore Anthropology to its proper place within Cognitive Science. The recent upsurge of interest in the ways that thought may shape and be shaped by action, gesture, cultural experience, and language sets the stage for, but so far has not fully accomplished, the inclusion of Anthropology as an equal partner.

 This overview describes the generation and development of the ideas that led to the Cognitive Neuroscience Treatment Research to Improve Cognition in Schizophrenia (CNTRICS) initiative. It also describes the organization, process, and products of the first meeting. The CNTRICS initiative involves a series of three conferences that will systematically address barriers to translating paradigms developed in the basic animal and human cognitive neuroscience fields for use in translational research aimed at developing novel treatments for cognitive impairments in schizophrenia. The articles in this special section report on the results of the first conference, which used a criterion-based consensus-building process to develop a set of cognitive constructs to be targeted for translation efforts.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 The covariation component of everyday causal inference has been depicted, in both cognitive and social psychology as well as in philosophy, as heterogeneous and prone to biases. The models and biases discussed in these domains are analyzed with respect to focal sets: contextually determined sets of events over which covariation is computed. Moreover, these models are compared to our probabilistic contrast model, which specifies causes as first and higher order contrasts computed over events in a focal set. Contrary to the previous depiction of covariation computation, the present assessment indicates that a single normative mechanism-the computation of probabilistic contrasts-underlies this essential component of natural causal induction both in everyday and in scientific situations.

 Because causal relations are neither observable nor deducible, they must be induced from observable events. The 2 dominant approaches to the psychology of causal induction-the covariation approach and the causal power approach-are each crippled by fundamental problems. This article proposes an integration of these approaches that overcomes these problems. The proposal is that reasoners innately treat the relation between covariation (a function defined in terms of observable events) and causal power(an unobservable entity) as that between scientists' law or model and their theory explaining the model. This solution is formalized in the power PC theory, a causal power theory of the probabilistic contrast model(P. W. Cheng &amp; L. R. Novick, 1990). The article reviews diverse old and new empirical tests discriminating this theory from previous models, none of which is justified by a theory. The results uniquely support the power PC theory.

-----------------------
 The covariation component of everyday causal inference has been depicted, in both cognitive and social psychology as well as in philosophy, as heterogeneous and prone to biases. The models and biases discussed in these domains are analyzed with respect to focal sets: contextually determined sets of events over which covariation is computed. Moreover, these models are compared to our probabilistic contrast model, which specifies causes as first and higher order contrasts computed over events in a focal set. Contrary to the previous depiction of covariation computation, the present assessment indicates that a single normative mechanism-the computation of probabilistic contrasts-underlies this essential component of natural causal induction both in everyday and in scientific situations.

 Because causal relations are neither observable nor deducible, they must be induced from observable events. The 2 dominant approaches to the psychology of causal induction-the covariation approach and the causal power approach-are each crippled by fundamental problems. This article proposes an integration of these approaches that overcomes these problems. The proposal is that reasoners innately treat the relation between covariation (a function defined in terms of observable events) and causal power(an unobservable entity) as that between scientists' law or model and their theory explaining the model. This solution is formalized in the power PC theory, a causal power theory of the probabilistic contrast model(P. W. Cheng &amp; L. R. Novick, 1990). The article reviews diverse old and new empirical tests discriminating this theory from previous models, none of which is justified by a theory. The results uniquely support the power PC theory.

-----------------------
 The covariation component of everyday causal inference has been depicted, in both cognitive and social psychology as well as in philosophy, as heterogeneous and prone to biases. The models and biases discussed in these domains are analyzed with respect to focal sets: contextually determined sets of events over which covariation is computed. Moreover, these models are compared to our probabilistic contrast model, which specifies causes as first and higher order contrasts computed over events in a focal set. Contrary to the previous depiction of covariation computation, the present assessment indicates that a single normative mechanism-the computation of probabilistic contrasts-underlies this essential component of natural causal induction both in everyday and in scientific situations.

 Because causal relations are neither observable nor deducible, they must be induced from observable events. The 2 dominant approaches to the psychology of causal induction-the covariation approach and the causal power approach-are each crippled by fundamental problems. This article proposes an integration of these approaches that overcomes these problems. The proposal is that reasoners innately treat the relation between covariation (a function defined in terms of observable events) and causal power(an unobservable entity) as that between scientists' law or model and their theory explaining the model. This solution is formalized in the power PC theory, a causal power theory of the probabilistic contrast model(P. W. Cheng &amp; L. R. Novick, 1990). The article reviews diverse old and new empirical tests discriminating this theory from previous models, none of which is justified by a theory. The results uniquely support the power PC theory.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 Theories of perceptual transparency have typically been developed within the context of a physical model that generates the percept of transparency (F. Metelli's episcotister model, 1974b). Here 2 fundamental questions are investigated: (a) When does the visual system initiate the percept of one surface seen through another? (b) How does it assign surface properties to a transparent layer? Results reveal systematic deviations from the predictions of Metelli's model, both for initiating image decomposition into multiple surfaces and for assigning surface attributes. Specifically, results demonstrate that the visual system uses Michelson contrast as a critical image variable to initiate percepts of transparency and to assign transmittance to transparent surfaces. Findings are discussed in relation to previous theories of transparency, lightness, brightness, and contrast-contrast.

 A series of experiments was performed to determine how the visual system computes the transmittance of inhomogeneous surfaces and media. Previous work (Anderson, B. L. (1999) Stereoscopic surface perception. Neuron, 26, 919-928; Anderson, B. L. (2003) The role of occlusion in the perception of depth, lightness, and opacity. Psychological Review, 110, 762-784) has suggested that the visual system employs a transmittance anchoring principle in determining when transparency is perceived. This principle states that the visual system interprets the highest contrast region along contours and surfaces as a region in plain view and uses this anchor as a reference point for transparency computations. In particular, recent work has shown that the transmittance of homogeneous transparent surfaces is well described by a ratio of contrasts model (Singh, M., &amp; Anderson, B. L. (2002). Toward a perceptual theory of transparency. Psychological Review, 109, 492-519). In this model, the transmittance of a transparent surface is determined by the contrast of a transparent image region normalized by the contrast of the region in plain view. Here, a series of experiments is reported that assesses this model for inhomogeneous transparent surfaces that vary in both space and time. The results of these experiments reveal that transmittance anchoring has both a spatial and temporal component, and that the perceived transmittance of transparent surfaces is well described by a ratio of perceived contrasts model. (c) 2005 Elsevier Ltd. All rights reserved.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 The role of morphemic processing in reading was investigated in 2 experiments in which participants read sentences as their eye movements were monitored. The target words were 2-morpheme Finnish compound words. In Experiment 1, the length of the component morphemes was varied and word length was held constant, and in Experiment 2, the uniqueness of the initial morpheme was varied and the rated familiarity and length of the word were held constant. The length of the initial morpheme influenced the location of the second fixation on the target word and the pattern of fixation durations (although it had a negligible influence on the gaze duration of the word). The frequency of the initial morpheme influenced the duration of the first fixation on the target word, had a substantial effect on the gaze duration, and also influenced the location of the first and second fixations on the target word. Subsidiary analyses indicated that these effects were unlikely to stem from orthographic factors such as bigram frequency.

 Three experiments examined whether the semantic transparency of a long Finnish compound word has any influence on how the compound word is encoded in reading. The frequency of the first constituent (as a separate word) was manipulated, while matching for the frequencies of the compound word and of the second constituent. The effect of this frequency manipulation on encoding time served as a 'marker' that the compound word was processed, at least in part, componentially. In Experiment 1, each high-frequency transparent compound was paired with a low-frequency transparent compound, and each high-frequency opaque compound was paired with a low-frequency opaque compound. A sentence frame was created for each pair that was identical up to the word following the target word. In Experiments 2 and 3, the matching was done between transparent and opaque word pairs. In addition, Experiment 3 had a display change manipulation in which most of the second constituent was not visible until it was fixated. Readers' eye fixation patterns on and immediately after the target word were examined. Reliable first constituent frequency effects were observed in the fixation duration measures on the target word, but there were no effects of transparency. In addition, a comparison of the display change condition to the standard condition indicated that the constituents of the compound word were processed sequentially. It thus appears that the identification of both transparent and opaque long compound words takes place, at least in part, by accessing the constituent lexemes and does not rely on constructing the meaning from the components.

-----------------------
 The timing of repetitive movements was assessed in a callosotomy patient under unimanual and bimanual conditions. Similar to neurologically healthy individuals, the patient exhibited strong temporal coupling in the bimanual condition. Moreover, for both the left and right hands, within-hand temporal variability was reduced in the bimanual condition compared to the unimanual conditions. This bimanual advantage is hypothesized to reflect the temporal integration of separable timing signals, one associated with the left hand and one associated with the right hand (Helmuth, L. L., &amp; Ivry, R. B. (1996). When two hands are better than one: Reduced timing variability during bimanual movements. Journal of Experimental Psychology. Human Perception and Performance, 2, 278-293). The fact that it persists following callosotomy is inconsistent with models that attribute bimanual coordination in these patients to the control of a single hemisphere. Rather, the results suggest that motor commands from the two hemispheres are integrated subcortically. (C) 1999 Elsevier Science B.V. All rights reserved.

 Rhythmic bimanual movements are highly constrained in the temporal domain, with the gestures of the two hands tightly synchronized. Previous studies have implicated a subcortical locus for temporal coupling based on the observation that these constraints persist in callosotomy patients. We now report that such coupling is restricted to movements entailing a discrete event (such as a movement onset). Three callosotomy patients exhibited a striking lack of temporal coupling during continuous movements, with the two hands oscillating at non-identical frequencies. We propose a subcortical locus of temporal coupling for movements involving discrete events. In contrast, synchronization between the hands during continuous movements depends on interhemispheric transmission across the corpus callosum.

-----------------------
 What are the neural correlates of conscious visual awareness? Tackling this question requires contrasting neural correlates of stimulus processing culminating in visual awareness with neural correlates of stimulus processing unaccompanied by awareness. To produce these two neural states, one must be able to erase an otherwise visible stimulus from awareness. This article describes and assesses visual phenomena involving dissociation of physical stimulation and conscious awareness: degraded stimulation, visual masking, visual crowding, bistable figures, binocular rivalry, motion-induced blindness, inattentional blindness, change blindness and attentional blink. No single approach stands above the others, but those producing changing visual awareness despite invariant physical stimulation are clearly preferable. Such phenomena can help lead us ultimately to a comprehensive account of the neural correlates of conscious awareness.

 We measured visual-adaptation strength under variations in visual awareness by manipulating phenomenal invisibility of adapting stimuli using binocular rivalry and visual crowding. Results showed that the threshold-elevation aftereffect and the translational motion aftereffect were reduced substantially during binocular rivalry and crowding. Importantly, aftereffect reduction was correlated with the proportion of time that the adapting stimulus was removed from visual awareness. These findings indicate that the neural events that underlie both rivalry and crowding are inaugurated at an early stage of visual processing, because both the threshold-elevation aftereffect and translational motion aftereffect arise, at least in part, from adaptation at the earliest stages of cortical processing. Also, our findings make it necessary to reinterpret previous studies whose results were construed as psychophysical evidence against the direct role of neurons in the primary visual cortex in visual awareness.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 We have developed an experimental platform that allows a large number of human participants to interact in real time within a common virtual world. Within this environment, human participants foraged for resources distributed in two spatially separated pools. In addition to varying the relative replenishment rate for the two pools (50-50, 65-35, or 80-20), we manipulated whether the participants could see each other and the entire resource distribution or had their vision restricted to resources at their own location. Two empirical deviations from an optimal distribution of the participants were found. First, the participants were more scattered within a resource pool than the resources were themselves. Second, there was systematic underutilization of the richer pool. For example, the participants distributed themselves 73% and 27% to resource pools that had replenishment rates of 80% and 20%, respectively. In addition, there were oscillations in the harvesting rate of the pools across time, revealed by a Fourier analysis with prominent power near 50 sec per cycle. The suboptimalities and oscillations were more apparent when the locations of the participants and the food were not visible. Individual participant knowledge thus affects the efficiency with which a population of participants harvests resources.

 The allocation of human participants to resources was studied by observing the population dynamics of people interacting in real time within a common virtual world. Resources were distributed in two spatially separated pools with varying relative reinforcement rates (50-50, 65-35, or 80-20). We manipulated whether the participants could see each other and the distribution of the resources. When the participants could see each other but not the resources, the richer pool was underutilized. When the participants could see the resources but not each other, the richer pool was overutilized. In conjunction with prior experiments that correlated the visibility of agents and resources (Goldstone &amp; Ashpole, 2004), these results indicate that participants' foraging decisions are influenced by both forager and resource information. The results suggest that the presence of a crowd at a resource is a deterring, rather than an attractive, factor. Both fast and slow oscillations in the harvesting rates of the pools across time were revealed by Fourier analyses. The slow waves of crowd migration were most prevalent when the resources were invisible, whereas the fast cycles were most prevalent when the resources were visible and the participants were invisible.

-----------------------
 The timing of repetitive movements was assessed in a callosotomy patient under unimanual and bimanual conditions. Similar to neurologically healthy individuals, the patient exhibited strong temporal coupling in the bimanual condition. Moreover, for both the left and right hands, within-hand temporal variability was reduced in the bimanual condition compared to the unimanual conditions. This bimanual advantage is hypothesized to reflect the temporal integration of separable timing signals, one associated with the left hand and one associated with the right hand (Helmuth, L. L., &amp; Ivry, R. B. (1996). When two hands are better than one: Reduced timing variability during bimanual movements. Journal of Experimental Psychology. Human Perception and Performance, 2, 278-293). The fact that it persists following callosotomy is inconsistent with models that attribute bimanual coordination in these patients to the control of a single hemisphere. Rather, the results suggest that motor commands from the two hemispheres are integrated subcortically. (C) 1999 Elsevier Science B.V. All rights reserved.

 Rhythmic bimanual movements are highly constrained in the temporal domain, with the gestures of the two hands tightly synchronized. Previous studies have implicated a subcortical locus for temporal coupling based on the observation that these constraints persist in callosotomy patients. We now report that such coupling is restricted to movements entailing a discrete event (such as a movement onset). Three callosotomy patients exhibited a striking lack of temporal coupling during continuous movements, with the two hands oscillating at non-identical frequencies. We propose a subcortical locus of temporal coupling for movements involving discrete events. In contrast, synchronization between the hands during continuous movements depends on interhemispheric transmission across the corpus callosum.

-----------------------
 This article describes a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in Spanish, including word frequency, syllable frequency, bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, concreteness, familiarity, imageability, valence, arousal, and age-of-acquisition measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words. The program computes measures of orthographic similarity online, with respect to either a default vocabulary of 31,491 Spanish words or a vocabulary specified by the user. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity.

 We describe a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in an agglutinative language (Basque), including measures of word frequency (at the whole-word and lemma levels), bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, and syllable-based measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words and morphology. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity. It is available free of charge from www.uv.es/mperea/E-Hitz.zip.

-----------------------
 A series of experiments assessed priming for single letters and words in a letter-by-letter reader (IH) when primes were displayed briefly (between 100-500msec) and masked. Consistent with previous claims that letter-by-letter readers have difficulties accessing orthographic letter codes, IH failed to show normal cross-case priming for single letters in a naming task (e.g. a/A). Nevertheless, IH showed robust cross-case priming for four-letter words that have few if any perceptual features in common between upper and lower case (e.g. read/READ; the letters r/R, e/E, a/A, and d/D are visually dissimilar in lower/upper case), even at prime durations that failed to support priming for single letters. Furthermore, priming extended to pseudowords (e.g. DEAT), and was highly specific given that no priming was obtained between orthographic neighbours (e.g. face did not prime FACT). Based on this pattern of results, we argue that IH gains relatively normal access to orthographic representations, and that his letter-by-letter reading reflects a partial disconnection between orthographic and phonological representations. Within the context of a disconnection account, we provide an explanation of the paradoxical finding of robust word priming in the absence of single letter priming.

 Five experiments were carried out to test the claim that the modality-specific and modality-nonspecific components of long-term priming are differentially sensitive to word frequency, with the specific component being less affected. In contrast with this claim, specific and nonspecific priming were similarly reduced for high-frequency words in three lexical decision and two perceptual identification experiments. These findings highlight the important role of frequency in modulating priming as well as provide a basic constraint for future theories of priming. In addition, the roles of task and student population in modulating priming are examined.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 Task-induced deactivation (TID) refers to some regional decrease in blood oxygenation level-dependent (BOLD) signal during a cognitive task compared to a baseline. Recent several functional imaging studies have found co-activation in a distributed network cortical regions, including ventral anterior cingulate cortex (vACC) and posterior cingulate cortex (PPC) that characterizes the default mode of human brain. This founding led to a hypothesis that these regions constitute a network supporting a TED mode network of brain function. In this study, two complementary methods, one applying the general linear model and the other a data driven approach called group independent component analysis (ICA), were utilized to analyze the fMRI data obtained two tasks, Chinese verb generation and the English noun word reading. Both methods yielded similar, but not identical, results and detected a network of robustly activated some midline regions including anterior and posterior cingulate cortex and precuneus. However, the ICA method segregated functional elements into two separate maps and identified ventral cingulate component and a fronto-parietal component. The results suggest that these two components in tasks induced deactivation might be linked to different mental function and play a significant role during "resting" or "passive" baseline.

 Default-mode network activity refers to some regional increase in blood oxygenation level-dependent (BOLD) signal during baseline than cognitive tasks. Recent functional imaging studies have found co-activation in a distributed network of cortical regions, including ventral anterior cingulate cortex (vACC) and posterior cingulate cortex (PPC) that characterize the default mode of human brain. In this study, general linear model and group independent component analysis (ICA) were utilized to analyze the fMRI data obtained from two language tasks. Both methods yielded similar, but not identical results and detected a resting deactivation network at some midline regions including anterior and posterior cingulate cortex and precuneus. Particularly, the group ICA method segregated functional elements into two separate maps and identified ventral cingulate component and fronto-parietal component. These results suggest that these two components might be linked to different mental function during "resting" baseline.

-----------------------
 Task-induced deactivation (TID) refers to some regional decrease in blood oxygenation level-dependent (BOLD) signal during a cognitive task compared to a baseline. Recent several functional imaging studies have found co-activation in a distributed network cortical regions, including ventral anterior cingulate cortex (vACC) and posterior cingulate cortex (PPC) that characterizes the default mode of human brain. This founding led to a hypothesis that these regions constitute a network supporting a TED mode network of brain function. In this study, two complementary methods, one applying the general linear model and the other a data driven approach called group independent component analysis (ICA), were utilized to analyze the fMRI data obtained two tasks, Chinese verb generation and the English noun word reading. Both methods yielded similar, but not identical, results and detected a network of robustly activated some midline regions including anterior and posterior cingulate cortex and precuneus. However, the ICA method segregated functional elements into two separate maps and identified ventral cingulate component and a fronto-parietal component. The results suggest that these two components in tasks induced deactivation might be linked to different mental function and play a significant role during "resting" or "passive" baseline.

 Default-mode network activity refers to some regional increase in blood oxygenation level-dependent (BOLD) signal during baseline than cognitive tasks. Recent functional imaging studies have found co-activation in a distributed network of cortical regions, including ventral anterior cingulate cortex (vACC) and posterior cingulate cortex (PPC) that characterize the default mode of human brain. In this study, general linear model and group independent component analysis (ICA) were utilized to analyze the fMRI data obtained from two language tasks. Both methods yielded similar, but not identical results and detected a resting deactivation network at some midline regions including anterior and posterior cingulate cortex and precuneus. Particularly, the group ICA method segregated functional elements into two separate maps and identified ventral cingulate component and fronto-parietal component. These results suggest that these two components might be linked to different mental function during "resting" baseline.

-----------------------
 This article describes a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in Spanish, including word frequency, syllable frequency, bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, concreteness, familiarity, imageability, valence, arousal, and age-of-acquisition measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words. The program computes measures of orthographic similarity online, with respect to either a default vocabulary of 31,491 Spanish words or a vocabulary specified by the user. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity.

 We describe a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in an agglutinative language (Basque), including measures of word frequency (at the whole-word and lemma levels), bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, and syllable-based measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words and morphology. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity. It is available free of charge from www.uv.es/mperea/E-Hitz.zip.

-----------------------
 Three priming experiments investigated the role of attention and view changes when common objects were rotated in depth. Objects were shown in prime-probe trial pairs. Experiment 1 extended findings by Stankiewicz, Hummel, and Cooper (1998) showing that attended objects primed themselves in the same but not in a reflected view, whereas ignored objects only primed themselves in the same view. In Experiment 2, depth-rotations produced changes in the visible part structure between prime and probe view of an object. Priming after depth-rotation was more reduced for attended objects than for ignored objects. Experiment 3 showed that other depth rotations that did not change the perceived part structure revealed a priming pattern similar to that in Experiment 1, with equivalent reduction in priming for attended and ignored objects. These data indicate that recognition of attended objects is mediated by a part-based (analytic) representation together with a view-based (holistic) representation, whereas ignored images are recognized in a strictly view-dependent fashion.

 Three experiments investigated the role of attention in visual priming across rotations in the picture plane. Experiment 1 showed that naming latencies increased with the degree of misorientation for objects commonly seen in an upright view (base objects) but not for objects seen familiarly from many views (no-base objects). In Experiment 2, no-base objects revealed a priming pattern identical to that observed previously for left-right reflections (Stankiewicz, Hummel, &amp; Cooper, 1998): Attended objects primed themselves in the same and rotated views, whereas ignored images primed themselves only in the same view, with additive effects of attention and orientation. In Experiment 3 ignored base objects only primed themselves in a familiar (upright) view, indicating that priming only obtains when that image makes contact with object memory. These data challenge theories of object recognition that rely on any single representation of shape and contribute to evidence suggesting holistic (view-like) representations for ignored and analytic (view-insensitive) representations for attended objects.

-----------------------
 Background This study compares the efficacy of two school-based intervention programmes (Phonology with Reading (P + R) and Oral Language (OL)) for children with poor oral language at school entry. Methods Following screening of 960 children, 152 children (mean age 4;09) were selected from 19 schools on the basis of poor vocabulary and verbal reasoning skills and randomly allocated to either the P + R programme or the OL programme. Both groups of children received 20 weeks of daily intervention alternating between small group and individual sessions, delivered by trained teaching assistants. Children in the P + R group received training in letter-sound knowledge, phonological awareness and book level reading skills. Children in the OL group received instruction in vocabulary, comprehension, inference generation and narrative skills. The children's progress was monitored at four time points: pre-, mid- and post-intervention, and after a 5-month delay, using measures of literacy, language and phonological awareness. Results The data are clustered (children within schools) and robust confidence intervals are reported. At the end of the 20-week intervention programme, children in the P + R group showed an advantage over the OL group on literacy and phonological measures, while children in the OL group showed an advantage over the P + R group on measures of vocabulary and grammatical skills. These gains were maintained over a 5-month period. Conclusions Intervention programmes designed to develop oral language skills can be delivered successfully by trained teaching assistants to children at school entry. Training using P + R fostered decoding ability whereas the OL programme improved vocabulary and grammatical skills that are foundations for reading comprehension. However, at the end of the intervention, more than 50% of at-risk children remain in need of literacy support.

 Interventions combining phonically based reading instruction with phonological training are generally effective for children with reading (decoding) difficulties. However, a minority of children respond poorly to such interventions. This study explored the characteristics of children who showed poor response to reading intervention and aimed to improve their literacy and language skills via a new theoretically motivated intervention. Twelve 8-year-old treatment poor responders with severe and persisting reading difficulties participated. A 9-week reading intervention incorporating reading, phonological and vocabulary training was implemented. Before the intervention began the children showed almost no progress over 6 months of regular classroom education, on measures of oral language and literacy. Over the intervention period improvements were made on measures of reading, phonological awareness and language skills, which were maintained 6 months later. Although the intervention was effective, it should be noted that most children remained poor readers and require ongoing remediation.

-----------------------
 The masked affective priming task was used as an unobtrusive measure of intergroup prejudices in a sample of German adolescents (aged 13-15). Pictures of Turks and Germans were used as masked primes that preceded positive and negative target adjectives conveying either other-relevant valence (e.g., honest, evil) or possessor-relevant valence (e.g., talented, dull). Affective priming indices (denoting relative negativity of Turkish primes) were positively correlated with the open expression of prejudices towards Turks and foreigners in general in questionnaires as well as with discriminative interaction behavior in a virtual ball-tossing game. As expected, these correlations were found only for priming indices based on other-relevant targets, thereby emphasizing the differentiation of automatic prejudice into (imputed) hostility and depreciation.

 In this paper, we yield evidence for the dependence of affective priming on the congruency of the previous trial. Affective priming refers to the finding that valence categorizations of targets are facilitated when the preceding prime is of the same valence. In two experiments, affective priming was diminished after incongruent trials (i.e., prime and target were of different valence), whereas, significant affective priming was observed after congruent trials (i.e., prime and target were of same valence). We compare this pattern to the known sequential dependencies in Stroop- and Eriksen-type tasks. Furthermore, our results can help to improve the statistical power of studies in which the affective priming task is used as a measure for automatic evaluations of attitude-objects. (c) 2008 Elsevier B.V. All rights reserved.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 We report three experiments with language-impaired and unimpaired speakers of Italian, assessing: (1) whether nonsyntatic (both conceptual and morphophonological) information is used in encoding the syntactic structure of a sentence; and (2) whether the integration of syntactic and non-syntactic information can be differentially impaired in Broca's aphasics. In all the experiments, gender agreement errors between a noun, subject of the sentence, and a predicative adjective were induced by presenting participants with sentence fragments to complete. The first experiment assessed the role of conceptual information. The second experiment investigated whether agreement is disrupted by the presence of another noun with different gender in the subject noun phrase. In the last experiment we assessed whether morphophonological cues are used. We found that both populations used nonsyntactic information (both conceptual and morphophonological). However, patients were disrupted to a greater extent than normals by the presence of a gender mismatching noun in the subject noun phrase. The results are discussed in terms of how information integration during production is achieved and how it can be disrupted in aphasia.

 This article addresses the question of whether accuracy in developing a syntactic frame for a to-be-uttered sentence is influenced by conceptual information beyond the initial assignment of grammatical functions on the basis of the speakers' intentions, as predicted by the maximalist hypothesis we put forward in previous work (Vigliocco &amp; Franck, 1999). In a series of four parallel experiments (in Italian and French) investigating gender agreement between a sentential subject and a predicative adjective, we found that conceptual information helps syntactic accuracy when congruent with syntactic information and hinders accuracy when incongruent, In a final experiment in Italian, potentially confounded linguistic factors were assessed and dismissed. (C) 2001 Academic Press.

-----------------------
 The commentators expressed concerns regarding the relevance and value of non-computational non-symbolic explanations of cognitive performance. But what counts as an explanation depends on the pre-theoretical assumptions behind the scenes of empirical science regarding the kinds of variables and relationships that are sought out in the first place, and some of the present disagreements stem from incommensurate assumptions. Traditional cognitive science presumes cognition to be a decomposable system of components interacting according to computational rules to generate cognitive performances (i.e., component-dominant dynamics). We assign primacy to interaction-dominant dynamics among components. Though either choice can be a good guess before the fact, the primacy of interactions is now supported by much recent empirical work in cognitive science. Consequently, in the main, the commentators have failed so far to address the growing evidence corroborating the theory-driven predictions of complexity science.

 Readers of TopiCS are invited to join a debate about the utility of ideas and methods of complexity science. The topics of debate include empirical instances of qualitative change in cognitive activity and whether this empirical work demonstrates sufficiently the empirical flags of complexity. In addition, new phenomena discovered by complexity scientists, and motivated by complexity theory, call into question some basic assumptions of conventional cognitive science such as stable equilibria and homogeneous variance. The articles and commentaries that appear in this issue also illustrate a new debate style format for topiCS.

-----------------------
 We argue that psycholinguistics should be concerned with both the representation and the processing of language. Recent experimental work on syntax in language comprehension has largely concentrated on the way in which language is processed and has assumed that theoretical linguistics serves to determine the representation of language. In contrast, we advocate experimental work on the mental representation of grammatical knowledge, and argue that syntactic priming is a promising way to do this. Syntactic priming is the phenomenon whereby exposure to a sentence with a particular syntactic construction can affect the subsequent processing of an otherwise unrelated sentence with the same (or, perhaps, related) structure, for reasons of that structure. We assess evidence for syntactic priming in corpora, and then consider experimental evidence for priming in production and comprehension and for bidirectional priming between comprehension and production. This in particular strongly suggests that priming is tapping into linguistic knowledge itself; and is not just facilitating particular processes. The final section discusses the importance of priming evidence for any account of language construed as the mental representation of human linguistic capacities.

 Repetition is a central phenomenon of behavior, and researchers have made extensive use of it to illuminate psychological functioning. In the language sciences, a ubiquitous form of such repetition is structural priming, a tendency to repeat or better process a current sentence because of its structural similarity to a previously experienced ("prime") sentence (J. K. Bock, 1986). The recent explosion of research in structural priming has made it the dominant means of investigating the processes involved in the production (and increasingly, comprehension) of complex expressions such as sentences. This review considers its implications for the representation of syntax and the mechanisms of production and comprehension and their relationship. It then addresses the potential functions of structural priming, before turning to its implications for first language acquisition, bilingualism, and aphasia. The authors close with theoretical and empirical recommendations for future investigations.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 We argue that psycholinguistics should be concerned with both the representation and the processing of language. Recent experimental work on syntax in language comprehension has largely concentrated on the way in which language is processed and has assumed that theoretical linguistics serves to determine the representation of language. In contrast, we advocate experimental work on the mental representation of grammatical knowledge, and argue that syntactic priming is a promising way to do this. Syntactic priming is the phenomenon whereby exposure to a sentence with a particular syntactic construction can affect the subsequent processing of an otherwise unrelated sentence with the same (or, perhaps, related) structure, for reasons of that structure. We assess evidence for syntactic priming in corpora, and then consider experimental evidence for priming in production and comprehension and for bidirectional priming between comprehension and production. This in particular strongly suggests that priming is tapping into linguistic knowledge itself; and is not just facilitating particular processes. The final section discusses the importance of priming evidence for any account of language construed as the mental representation of human linguistic capacities.

 People have a tendency to repeat the types of sentences they use during language production. Recent experimental work has shown that this phenomenon is at least partly due to 'syntactic priming', whereby the act of processing an utterance with a particular form facilitates processing a subsequent utterance with the same or a related form. In this review, we first provide an overview of the evidence for syntactic priming. The review will then explore the implications of this research for three different areas of language theory: the possible functional significance of syntactic priming in coordinating speakers during dialogue, the mechanisms underlying sentence production, and the nature of linguistic representation.

-----------------------
 Previous research has suggested that the initial portion of a word activates similar sounding words that compete for recognition. Other research has shown that the number of similar sounding words that are activated influences the speed and accuracy of recognition. Words with few neighbors are processed more quickly and accurately than words with many neighbors. The influences of the number of lexical competitors in the initial part of the word were examined in a shadowing and a lexical-decision task. Target words with few neighbors that share the initial phoneme were responded to more quickly than target words with many neighbors that share the initial phoneme. The implications of onset-density effects for models of spoken-word recognition are discussed.

 Clustering coefficient-a measure derived from the new science of networks-refers to the proportion of phonological neighbors of a target word that are also neighbors of each other. Consider the words bat, hat, and can, all of which are neighbors of the word cat; the words bat and hat are also neighbors of each other. In a perceptual identification task, words with a low clustering coefficient (i.e., few neighbors are neighbors of each other) were more accurately identified than words with a high clustering coefficient (i.e., many neighbors are neighbors of each other). In a lexical decision task, words with a low clustering coefficient were responded to more quickly than words with a high clustering coefficient. These findings suggest that the structure of the lexicon (i.e., the similarity relationships among neighbors of the target word measured by clustering coefficient) influences lexical access in spoken word recognition. Simulations of the TRACE and Shortlist models of spoken word recognition failed to account for the present findings. A framework for a new model of spoken word recognition is proposed.

-----------------------
 The shape of a response time (RT) distribution can be described by a 3-parameter model consisting of the convolution of the normal and exponential distributions, the ex-Gaussian.  Analyses based on mean  RT do not take the distribution's shape into account and, for that reason, may obscure aspects of performance.  To illustrate the point, the ex-Gaussian model was applied to data obtained from a Stroop task.  Mean RT revealed strong interference but no facilitation, whereas the analysis based on the ex-Gaussian model showed both interference and facilitation.  In short, analyses that do not take the shape of RT distributions into account can mislead and, therefore, should be avoided.

 Cohen, Dunbar, and McClelland's (1990) model was tested for Strooplike interference tasks by studying the shape of the distribution of response latencies produced by Ss and by the model. The model correctly anticipates changes in mean response latency (M(RT)) across congruent and incongruent conditions. It does not, however, correctly anticipate changes in the shape of the distributions, even though changes in the shape of the distributions underlie the changes in M(RT). Thus, the model predicts M(RT) successfully but for the wrong reason. It is concluded that the model is not an adequate account of Ss' performance in the Stroop task.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 The aim of this study was to enhance our insight into the underlying deficit in developmental apraxia of speech (DAS). In particular, the involvement of planning and/or programming of speech movements in context was tested by analysing coarticulatory cohesion. For this purpose, second formant frequency measurements were conducted in repetitions of nonsense utterances ([(sic)CV] C=/s,x,b,d/; V = /i,a,u/), and compared across nine children with DAS, six normally speaking (NS) children and six adult women. The results showed both intra- and intersyllabic anticipatory coarticulation in NS children and adult women, in which the intersyllabic coarticulation was stronger in NS children than in adult women. The children with DAS showed more variability as compared to NS children, made, on average, less distinction between the vowels, and showed individually idiosyncratic coarticulation patterns. These results are discussed in the light of a delay as well as a deviance of speech development in children with DAS.

 The aim of the present study was to investigate whether children with developmental apraxia of speech (DAS) show a deficit in planning syllables in speech production. Six children with DAS and six normally speaking (NS) children produced high- and low-frequency of occurrence syllable utterances, in which the syllable structure was systematically manipulated in an otherwise unchanging phoneme sequence. Anticipatory coarticulation, using second formant trajectories, and durational structure were analysed. The results showed stronger coarticulation in the children with DAS when compared to the normally speaking children, but in contrast to our expectations, in neither group was a systematic effect of syllable structure on the second format trajectory found. Effects of syllable structure did emerge for durational structure in that durational adjustments were found in the segments of the second syllable. These adjustments were less systematic in children with DAS when compared to normally speaking children. Furthermore, at the prosodic level, normally speaking children showed metrical contrasts that were not realized by the children with DAS. The latter results are interpreted as evidence for a problem in the planning of syllables in speech production of children with DAS, in particular concerning prosodic aspects, which is discussed in relation to the automation of speech production.

-----------------------
 This paper considers the nature of second language dialogues, involving at least one non-native (L2) speaker. We assume that dialogue is characterised by a process in which interlocutors develop similar mental states to each other (Pickering Garrod, 2004). We first consider various means in which interlocutors align their mental states, and suggest why such alignment may be different in second language dialogues from dialogues involving native (L1) speakers. Specifically, we consider alignment in L2 speakers conversing with L1 speakers, L1 speakers conversing with L2 speakers, and L2 speakers conversing with each other, and sketch a range of experimental predictions.

 We present an overview of recent research conducted in the field of language production based on papers presented at the first edition of the International Workshop on Language Production (Marseille, France, September 2004). This article comprises two main parts. In the first part, consisting of three sections, we review the articles that are included in this Special Issue. These three sections deal with three different topics of general interest for models of language production: (A) the general organisational principles of the language production system, (B) several aspects of the lexical selection process and (C) the representations and processes used during syntactic encoding. In the second part, we discuss future directions for research in the field of language production, given the considerable developments that have occurred in recent years.

-----------------------
 This paper considers the nature of second language dialogues, involving at least one non-native (L2) speaker. We assume that dialogue is characterised by a process in which interlocutors develop similar mental states to each other (Pickering Garrod, 2004). We first consider various means in which interlocutors align their mental states, and suggest why such alignment may be different in second language dialogues from dialogues involving native (L1) speakers. Specifically, we consider alignment in L2 speakers conversing with L1 speakers, L1 speakers conversing with L2 speakers, and L2 speakers conversing with each other, and sketch a range of experimental predictions.

 We present an overview of recent research conducted in the field of language production based on papers presented at the first edition of the International Workshop on Language Production (Marseille, France, September 2004). This article comprises two main parts. In the first part, consisting of three sections, we review the articles that are included in this Special Issue. These three sections deal with three different topics of general interest for models of language production: (A) the general organisational principles of the language production system, (B) several aspects of the lexical selection process and (C) the representations and processes used during syntactic encoding. In the second part, we discuss future directions for research in the field of language production, given the considerable developments that have occurred in recent years.

-----------------------
 In this paper, a two by three approach to modeling categorization is presented. Similarity representations based upon a geometric, an additive tree and an additive cluster model are combined with an exemplar model and a prototype model in a single approach. The six models are applied to the categorization of pictorial known and unknown fruits and vegetables (Smits et al., 2002). For novel stimuli, the geometric exemplar model and the cluster models gave the best account, indicating a strategy where people compare stimuli with stored members on more general continua or a limited set of features. For well-known stimuli, the tree-based models gave the best account of the data, suggesting more elaborate taxonomic knowledge. More generally, the results show that different categorization models may perform better for different sets of stimuli, and that a systematic empirical comparison of such models is needed.

 This paper concerns the use of similarities based on geometric distance in models of categorization. Two problematic implications of such similarities are outlined. First, in a comparison between two stimuli, geometric distance implies that matching features are not taken into account. Second, missing features are assumed not to exist. Only nonmatching features enter into calculations of similarity. A new model is constructed that is based on the ALCOVE model (Kruschke, 1992), but it uses a feature-matching similarity measure (see, e.g., Tversky, 1977) rather than a geometric one. It is an on-line model in the sense that both dimensions and exemplars are constructed during the categorization process. The model accounts better than ALCOVE does for data with missing features (Experiments 1 and 2) and at least as well as ALCOVE for a data set without missing features (Nosofsky, Kruschke, &amp; McKinley, 1992). This suggests that, at least for some stimulus materials, similarity in categorization is more akin to a feature-matching procedure than to geometric distance calculation.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 Background: Neuroimaging offers unique opportunities for understanding the acquisition of reading by children and for unravelling the mystery of developmental dyslexia. Here, I provide a selective overview of recent neuroimaging studies, drawing out implications for education and the teaching of reading.
 Purpose: To explore the phonological awareness skills of deaf children with cochlear implants (CIs) and relationships with vocabulary and reading development.
-----------------------
 Face-to-face conversations in every day life are conducted over a range of distances. However, previous research provides only limited indications of the effects of distance on visual and audiovisual speech recognition. We report an experiment which investigated effects of distance on perception of unimodal visual speech and congruent and incongruent audiovisual speech using a talking face presented at distances of 1, 5, 10, 20, and 30m and auditory, visual, congruent, and incongruent forms of the syllables /ba/, /bi/, /ga/, and /gi/. Identification of unimodal visual speech was unaffected by increasing distance to 10m, but was impaired at 20 and 30m. However, despite these drops in unimodal visual speech identification, visual speech improved performance with congruent auditory speech at all distances and impaired performance with incongruent auditory speech at distances up to 20m, indicating that auditory speech recognition is influenced by visual speech even when encoded from distant faces. Implications of these findings for understanding visual and audiovisual speech recognition are discussed.

 Seeing a talker's face influences auditory speech recognition, but the visible input essential for this influence has yet to be established. Using a new seamless editing technique, the authors examined effects of restricting visible movement to oral or extraoral areas of a talking face. In Experiment 1, visual speech identification and visual influences on identifying auditory speech were compared across displays in which the whole face moved, the oral area moved, or the extraoral area moved. Visual speech influences on auditory speech recognition were substantial and unchanging across whole-face and oral-movement displays. However, extraoral movement also influenced identification of visual and audiovisual speech. Experiments 2 and 3 demonstrated that these results are dependent on intact and upright facial contexts, but only with extraoral movement displays:

-----------------------
 This article describes a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in Spanish, including word frequency, syllable frequency, bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, concreteness, familiarity, imageability, valence, arousal, and age-of-acquisition measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words. The program computes measures of orthographic similarity online, with respect to either a default vocabulary of 31,491 Spanish words or a vocabulary specified by the user. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity.

 We describe a Windows program that enables users to obtain a broad range of statistics concerning the properties of word and nonword stimuli in an agglutinative language (Basque), including measures of word frequency (at the whole-word and lemma levels), bigram and biphone frequency, orthographic similarity, orthographic and phonological structure, and syllable-based measures. It is designed for use by researchers in psycholinguistics, particularly those concerned with recognition of isolated words and morphology. In addition to providing standard orthographic and phonological neighborhood measures, the program can be used to obtain information about other forms of orthographic similarity, such as transposed-letter similarity and embedded-word similarity. It is available free of charge from www.uv.es/mperea/E-Hitz.zip.

-----------------------
 In development, children often use gesture to communicate before they use words. The question is whether these gestures merely precede language development or are fundamentally tied to it. We examined 10 children making the transition from single words to two-word combinations and found that gesture had a tight relation to the children's lexical and syntactic development. First, a great many of the lexical items that each child produced initially in gesture later moved to that child's verbal lexicon. Second, children who were first to produce gesture-plus-word combinations conveying two elements in a proposition (point at bird and say "nap") were also first to produce two-word combinations ("bird nap"). Changes in gesture thus not only predate but also predict changes in language, suggesting that early gesture may be paving the way for future developments in language.

 Children produce their first gestures before their first words, and their first gesture+word sentences before their first word+word sentences. These gestural accomplishments have been found not only to predate linguistic milestones, but also to predict them. Findings of this sort suggest that gesture itself might be playing a role in the language-learning process. But what role does it play? Children's gestures could elicit from their mothers the kinds of words and sentences that the children need to hear in order to take their next linguistic step. We examined maternal responses to the gestures and speech that 10 children produced during the one-word period. We found that all 10 mothers 'translated' their children's gestures into words, providing timely models for how one- and two-word ideas can be expressed in English. Gesture thus offers a mechanism by which children can point out their thoughts to mothers, who then calibrate their speech to those thoughts, and potentially facilitate language-learning.

-----------------------
 The role of morphemic processing in reading was investigated in 2 experiments in which participants read sentences as their eye movements were monitored. The target words were 2-morpheme Finnish compound words. In Experiment 1, the length of the component morphemes was varied and word length was held constant, and in Experiment 2, the uniqueness of the initial morpheme was varied and the rated familiarity and length of the word were held constant. The length of the initial morpheme influenced the location of the second fixation on the target word and the pattern of fixation durations (although it had a negligible influence on the gaze duration of the word). The frequency of the initial morpheme influenced the duration of the first fixation on the target word, had a substantial effect on the gaze duration, and also influenced the location of the first and second fixations on the target word. Subsidiary analyses indicated that these effects were unlikely to stem from orthographic factors such as bigram frequency.

 Three experiments examined whether the semantic transparency of a long Finnish compound word has any influence on how the compound word is encoded in reading. The frequency of the first constituent (as a separate word) was manipulated, while matching for the frequencies of the compound word and of the second constituent. The effect of this frequency manipulation on encoding time served as a 'marker' that the compound word was processed, at least in part, componentially. In Experiment 1, each high-frequency transparent compound was paired with a low-frequency transparent compound, and each high-frequency opaque compound was paired with a low-frequency opaque compound. A sentence frame was created for each pair that was identical up to the word following the target word. In Experiments 2 and 3, the matching was done between transparent and opaque word pairs. In addition, Experiment 3 had a display change manipulation in which most of the second constituent was not visible until it was fixated. Readers' eye fixation patterns on and immediately after the target word were examined. Reliable first constituent frequency effects were observed in the fixation duration measures on the target word, but there were no effects of transparency. In addition, a comparison of the display change condition to the standard condition indicated that the constituents of the compound word were processed sequentially. It thus appears that the identification of both transparent and opaque long compound words takes place, at least in part, by accessing the constituent lexemes and does not rely on constructing the meaning from the components.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 Two experiments with highly fluent Spanish-English bilinguals examined repetition priming of picture identification and word retrieval in picture naming. In Experiment 1, between-language priming of picture naming was symmetric, but within-language priming was stronger in the nondominant language. In Experiment 2, priming between picture naming and translation was symmetric within both the dominant language and the nondominant language, but priming was stronger in the nondominant language. A mathematical model required only 3 process parameters to explain the pattern of priming across 8 conditions. These results indicate that shared processes are the basis of priming, that difficulty influences priming only at the process level, and that translation in both directions is concept mediated in fluent bilinguals.

 One measure of conceptual implicit memory is repetition priming in the generation of exemplars from a semantic category, but does such priming transfer across languages? That is, do the overlapping conceptual representations for translation equivalents provide a sufficient basis for such priming? In Experiment 1 (N=96) participants carried out a deep encoding task, and priming between languages was statistically reliable, but attenuated, relative to within-language priming. Experiment 2 (N=96) replicated the findings of Experiment 1 and assessed the contributions of conceptual and non-conceptual processes using a levels-of-processing manipulation. Words that underwent shallow encoding exhibited within-language, but not between-language, priming. Priming in shallow conditions cannot therefore be explained by incidental activation of the concept. Instead, part of the within-language priming effect, even under deep-encoding conditions, is due to increased availability of language-specific lemmas or phonological word forms.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 Aim Structural and functional imaging techniques were combined to investigate sensory system function in amyotrophic lateral sclerosis (ALS).

 Background: In the graph theoretical analysis of anatomical brain connectivity, the white matter connections between regions of the brain are identified and serve as basis for the assessment of regional connectivity profiles, for example, to locate the hubs of the brain. But regions of the brain can be characterised further with respect to their gray matter volume or resting state perfusion. Local anatomical connectivity, gray matter volume and perfusion are traits of each brain region that are likely to be interdependent, however, particular patterns of systematic covariation have not yet been identified.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 Hand allograft is a method in the stage of clinical experimentation, which is reserved in France for the treatment of bilateral traumatic amputees. This study reports the Lyon team experience, which is pioneer in this domain. Four patients Q mates and 1 female) underwent seven (one unilateral and three bilateral) hand transplantations from September 1998 to February 2007. The Level of amputation was at the wrist or at the mid-forearm. Delay since hand loss ranged from 2.5 to 9 years. The surgical protocol was elaborated and planned case by case. All. recipients received the same immunosuppressive treatment. Episodes of acute rejection were observed in the first 3 months after transplantation, which were easily managed after a few days increasing oral prednisone doses and applying topical immunosuppressants. Currently the patients receive the doses of immunosuppressants comparable to those in kidney-grafted patients. We have not registered any severe complication of immunosuppressive treatment up tilt now (7 years follow-up for the earliest graft). We performed analytical and functional clinical, as well as questionnaire evaluation of patients. The first case (unilateral graft) resulted in graft failure at 2 years due to non-compliance of the patient. The three bilateral graftees demonstrate a favorable evolution despite. some immunological (hyperglycemia, serum sickness) and surgical (thrombosis, osteomyelitis, skin toss) complications, which could be managed. The middle and long-term follow-up evaluation revealed good to excellent sensorimotor recovery of 4 hands in both mate recipients (4 and 7 years) with satisfactory social adaptation, higher or equal to those expected after post-traumatic replantations at the equivalent level and higher to those obtained with currently available myoelectric prosthesis. The last patient, a young female who has been grafted in February 2007, receives ongoing reeducation course and shows normal progress of functional restoration of both hands. The encouraging results of this clinical experimentation make us currently consider hand allografting as reasonable and useful both for the patients and for evolution of research in composite tissues allotransplantation (CTA). Further long-term careful research and worldwide monitoring of all patients with hand allografts is required to, on the one part, state on the authorization of this surgery, and, on the other part, to better elucidate the mechanisms of successful CTA. (c) 2007 Publie par Elsevier Masson SAS.

 Neuropsychological studies have revealed that schizophrenic (SZ) patients have severe impairments in the cognitive integration of static and moving perceptual stimuli. Research on knowledge structures has shown that sequences of continuous actions are represented in memory as clusters of goal-directed events in a hierarchical manner. In the present study, we investigated the ability to segment familiar sequences of dynamic goal-directed actions into small and large meaningful units in a group of patients with schizophrenia (N = 16) and a group of healthy control subjects (N = 17). While viewing two videotaped movies , participants were requested to detect the transitions between component events at both low and high levels of the action categorical structure. Both groups detected significantly more events under the small-oriented condition as compared to the large-oriented condition. Differently from normal controls, patients recalled the event scenes in a detailed and fragmentary manner and showed considerable difficulties in detecting large action units. Moreover, low performance on action boundary detection significantly correlated with higher levels of disorganisation symptoms in patients with schizophrenia. A defective conceptual organisation of perceptive action knowledge would help to explain the severe everyday difficulties of these patients both in monitoring their own actions and in understanding others intentions. (C) 2004 Elsevier Ireland Ltd. All rights reserved.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Inhibition of return refers to the finding that response latencies are longer for targets appearing at previously attended (cued) locations than at novel (uncued) locations. The present research was designed to examine the pattern of detection latencies that occurred for targets appearing at various uncued locations. The first 2 experiments showed that responses were fastest when the target occurred at a location directly opposite the cue. Experiment 3 showed that latencies were related to the angle between the target and the direction in which attention was being oriented. Experiments 4 and 5 showed that manipulating the direction of attentional orientation affected inhibition of return. Overall, the results suggest that inhibition of return may be due to the difference between orienting attention to locations along the path of attention versus orienting attention to those off the path of attention.

 The role of covert orienting of attention in response channel activation was examined using the flanker interference and precueing paradigms. Four experiments assessed the influence of distractors on the discrimination of a target colour patch under cueing conditions (three with noninformative, exogenous cues and one with informative, endogenous cues) that modulated attention at the flanker or target locations. Across all of the experiments, the amount of interference generated by the distractors was not modulated by the facilitation and inhibition of return induced by spatial attention precues. These results are consistent with previous reports of patients with neglect, which demonstrated that flanker interference proceeds at unattended locations (Audet, Bub, Lecours, 1991; Cohen, Ivry, Rafal, &amp; Kohn, 1995), and they suggest that response channel activation can occur independently from spatial attention.

-----------------------
 Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0 degrees due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation.

 Cognitive performance exhibits patterns of trial-to-trial variation that can be described as 1/f or pink noise, as do repeated measures of locomotor performance. Although cognitive and locomotor performances are known to interact when performed concurrently, it is not known whether concurrent performance affects the tasks' pink noise dynamical structure. In this study, participants performed a cognitive task (repeatedly producing a temporal interval) and a motor task (walking on a treadmill) in single- and dual-task conditions. In single-task conditions both tasks exhibited pink noise structure. For concurrent performance the dynamical structure of the cognitive task changed reliably in the direction of white (random) noise. The dynamical structure of locomotion remained pink noise. The change in cognitive dynamics occurred despite no reliable changes in mean or standard deviation measures for either task. The results suggest a functional reorganization of cognitive dynamics supporting successful task performance in dual-task conditions.

-----------------------
 Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0 degrees due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation.

 Cognitive performance exhibits patterns of trial-to-trial variation that can be described as 1/f or pink noise, as do repeated measures of locomotor performance. Although cognitive and locomotor performances are known to interact when performed concurrently, it is not known whether concurrent performance affects the tasks' pink noise dynamical structure. In this study, participants performed a cognitive task (repeatedly producing a temporal interval) and a motor task (walking on a treadmill) in single- and dual-task conditions. In single-task conditions both tasks exhibited pink noise structure. For concurrent performance the dynamical structure of the cognitive task changed reliably in the direction of white (random) noise. The dynamical structure of locomotion remained pink noise. The change in cognitive dynamics occurred despite no reliable changes in mean or standard deviation measures for either task. The results suggest a functional reorganization of cognitive dynamics supporting successful task performance in dual-task conditions.

-----------------------
 Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0 degrees due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation.

 Cognitive performance exhibits patterns of trial-to-trial variation that can be described as 1/f or pink noise, as do repeated measures of locomotor performance. Although cognitive and locomotor performances are known to interact when performed concurrently, it is not known whether concurrent performance affects the tasks' pink noise dynamical structure. In this study, participants performed a cognitive task (repeatedly producing a temporal interval) and a motor task (walking on a treadmill) in single- and dual-task conditions. In single-task conditions both tasks exhibited pink noise structure. For concurrent performance the dynamical structure of the cognitive task changed reliably in the direction of white (random) noise. The dynamical structure of locomotion remained pink noise. The change in cognitive dynamics occurred despite no reliable changes in mean or standard deviation measures for either task. The results suggest a functional reorganization of cognitive dynamics supporting successful task performance in dual-task conditions.

-----------------------
 The task-switching paradigm is being increasingly used as a tool for studying cognitive control and task coordination. Different procedural variations have been developed. They have in common that a comparison is made between transitions in which the previous task is repeated and transitions that involve a change toward another task. In general, a performance switch cost is observed such that switching to a new task results in a slower and more error-prone execution of the task. The present article reviews the theoretical explanations of the switch cost and the findings collected in support of those explanations. Resolution and protection from interference by previous events explain part of the switching cost, but processes related to task setting and task preparation also play a prominent role, as testified by faster execution and lower switch costs when the preparation time is longer. The authors discuss the evidence in favor of each of these sets of accounts and raise a number of questions that situate task switching in a broader context of cognitive control processes. The role of several aspects of the task set, including task variations, task-set overlap, and task-set structure, is addressed, as is the role of knowledge about probability of task changes and about the structure of task sequences.

 While recent years have witnessed a growing interest in Voluntary Task Switching (VTS), the control mechanisms that are required in order to switch tasks on a voluntary basis remain to be identified. Starting from the finding that in VTS the proportion of task repetitions is usually higher than the proportion of task switches (task-repetition bias), the present electrophysiological study tests and confirms the hypothesis that, during VTS, one initially re-selects the previously executed task, before correcting this bias and selecting the alternative task. On the one hand, these findings allow us to describe how people switch cognitive tasks voluntarily. On the other hand, our approach underlines the usefulness of electrophysiological measures in understanding the processes by which voluntary behavior occurs.

-----------------------
 Dense-array electrocortical and functional hemodynamic measures of human brain activity were collected to assess the relationship between 2 established neural measures of emotional reactivity. Recorded in parallel sessions, the slow-wave late positive potential (LPP) and visual cortical blood oxygen level-dependent (BOLD) signals were both modulated by the rated intensity of picture arousal. The amplitude of the LPP correlated significantly with BOLD intensity in lateral occipital, inferotemporal, and parietal visual areas across picture contents. Estimated strength of modeled regional sources did not correlate significantly with regional BOLD intensity. These data suggest that the enhanced positive slow wave seen over posterior sites during emotional picture processing represents activity in a circuit of visual cortical structures, reflecting a perceptual sensitivity to the motivational relevance of visual scenes.

 Models of visual emotional perception suggest a reentrant organization of the ventral visual system with the amygdala. Using focused functional magnetic resonance imaging in humans with a sampling rate of 100 ms, here we determine the relative timing of emotional discrimination in amygdala and ventral visual cortical structures during emotional perception. Results show that amygdala and inferotemporal visual cortex differentiate emotional from nonemotional scenes similar to 1 s before extrastriate occipital cortex, whereas primary occipital cortex shows consistent activity across all scenes. This pattern of discrimination is consistent with a reentrant organization of emotional perception in visual processing, in which transaction between rostral ventral visual cortex and amygdala originates the identification of emotional relevance.

-----------------------
 This study tested the impact of prefrontal-cortex lesion on learning hierarchically structured action sequences. Using a visual-manual serial reaction time task, we had subjects first perform five blocks of trials with a hierarchically structured 14-element action sequence and then tested for sequence-specific learning by introducing a pseudo-random transfer sequence. Relative to control subjects (N = 39), we found that both lateral frontal (N = 16) and medial frontal (N = 18) patients showed reduced overall performance benefits across the training phase. In contrast, the negative transfer test showed significantly increased reaction times in all patient groups, indicating robust sequence-specific learning. This learning was not significantly different from that of the control group. Taken together, the data suggest that learning hierarchically structured action sequences is unimpaired in patients with prefrontal-cortex lesion.

 Apraxia caused by left hemispheric stroke typically impairs skilled sequential movements. After stroke, apraxic patients need to reacquire motor skills by motor learning. The current study assessed for the first time incidental motor sequence learning in apraxic patients. Forty-eight human subjects (henceforth called "patients") with left hemispheric stroke affecting the middle cerebral artery territory (18 with apraxia and 30 without apraxia) and 17 age-matched healthy controls were tested on a visuomanual serial reaction time task. Subjects performed four blocks consisting of repetitions of a complex six element sequence containing ambiguous pairwise transitions before a new and unfamiliar sequence was introduced in block 5. Reaction time (RT) disadvantages in this fifth block indicated incidental sequence-specific motor learning. The intentional retrieval of the learned motor knowledge was assessed subsequently with a free recall task. Voxel-based lesion-symptom mapping (VLSM) was performed to investigate for the first time the lesion correlates of deficits in learning and retrieving sequential motor knowledge. Despite generally prolonged RTs, apraxic patients showed sequence-specific motor learning as could be observed in nonapraxic patients and healthy controls. However, apraxic patients showed reduced intentional retrieval of the learned sequence. VLSM revealed that impaired intentional retrieval of motor sequence knowledge resulted from dorsal premotor cortex lesions. Apraxic patients showed a dissociation of preserved incidental motor (sequence) learning and deficient intentional retrieval of this incidentally learned motor knowledge. The data suggest that novel approaches for treating apraxia should focus on incidental motor learning, but that automatic rather than intentional retrieval strategies should be enforced.

-----------------------
 Performance of task sequences is assumed to rely on activation and inhibition of tasks. An empirical marker of task inhibition is the so-called n - 2 repetition cost, which is assessed by comparing performance in trial n - 2 task repetitions (i.e., ABA) with that in n - 2 task switches (i.e., CBA). Current theoretical accounts assume that inhibition acts on the level of task representations (i.e., task sets). However, another potential target of task inhibition could be the representation of the task cue. To decide between these two alternatives, the authors used a 2:1 cue-to-task mapping design. They found significant n - 2 task repetition costs both with n - 2 cue repetitions and n - 2 cue switches. These costs were about equal (Experiment 1), and this data pattern was found for both short and long cuing intervals (Experiment 2). Together, the data suggest that task inhibition acts on task sets and not on cue representations.

 Three experiments investigated the cognitive mechanisms underlying the restart cost and mixing cost in task switching. To this aim, the predictability of task order was varied (unpredictable in Experiment 1 and predictable in Experiments 2 and 3) across experiments, which employed a multiple-trial paradigm. Verbal cues for color and shape matching tasks were presented before a run of four trials. Focusing on task-repetition runs only, we measured restart cost as the difference in performance between trials 1 and 2 and mixing cost as the difference in performance on the non-cued trials under mixed-tasks conditions (Experiments 1 and 2) and single-task conditions (Experiment 3). The restart cost was observed under mixed-tasks conditions with both unpredictable and predictable task orders but not under the single-task condition. In contrast, the mixing cost was observed under the mixed-tasks condition with unpredictable task order only (Experiment 1). This finding implies that the optimal task execution on repetition trials depends on how predictable the identity of the approaching task is. Therefore, we suggest that mixing cost arises from limited preparation on repetition trials when task order is unpredictable, while restart cost arises from processes involved in cue-based task activation that is needed to resolve task interference. Together, these data suggest that restart cost and mixing cost are based on dissociable mechanisms.

-----------------------
 Following transcranial magnetic stimulation (TMS) at stimulation strength of 1.5 times the resting motor threshold a silent period (SP) of similar to 180 ms duration can be observed in surface EMG-registrations of tonically activated small hand muscles. This SP is believed to be generated cortically and can be prolonged in stroke patients, but it is nor known whether a prolongation of the SP has arty functional significance. In order to answer the question of whether enhanced cortical inhibition can contribute to pathophysiology of motor dysfunction we studied stroke patients with clearly prolonged SP durations in the first dorsal interosseus muscle (&gt;2 times that of the intact side), but with normal magnetically evoked motor potentials. Sixteen patients out of a cohort of 174 consecutive patients presenting with acute hemiparetic stroke fulfilled the inclusion criteria. Serial TMS investigations were performed for up to 2 years post-stroke. In all patients, the SP duration decreased in parallel with clinical improvement In two patients, intermittent clinical deterioration was accompanied by an increase in the SP duration. In four patients, in addition to a markedly prolonged SP duration, the phenomenon of a complete inability to initiate voluntary muscle activity for several seconds, following TMS, could be observed in a number of trials ('motor arrest'). Detailed clinical analysis revealed that, in addition to hemiparesis, distinct motor disturbances in patients with SP prolongation could be observed. These motor disturbances resembled those of motor neglect and were characterized by motivationally dependent under-utilization of the affected arm, impairment of movement initiation, inability to maintain a constant force level and to scale forces, and impairment of individual finger movements. In 12 of the 16 patients at least one additional behavioural manifestation of neglect was present. We suggest that in stroke patients severe motor dysfunction. may be caused by hyperactivity of cortical inhibitory interneurons rather than by direct lesions of descending motor tracts Cortical hyperinhibition may, in turn, results from damage to any of a number of afferent pathways to the motor cortex which modulate local interneuronal activity.

 Background: Following an ischemic brain lesion, the affected cortex undergoes structural and functional changes that may lead to increased cortical excitability or decreased inhibitory neuronal activity, resulting in the occurrence of poststroke epileptic seizures in 6 to 10% of patients with stroke. Methods: To assess motor cortical excitability, transcranial magnetic stimulation (TMS) was used to determine the silent period (SP) duration in 84 consecutive patients with ischemic stroke. Results: In a subpopulation of six patients (38 to 72 years old) a significant decrease of the SP duration (mean 116 +/- 14 msec) was detected in either the arm or the leg on the affected side as compared to the corresponding unaffected limb (mean 231 +/- 32 msec). This electrophysiologic abnormality was clinically associated with focal motor seizures in five of the six patients, whereas none of the other 76 patients with normal or prolonged SP durations developed seizures or epilepsy. Conclusions: Silent period shortening in this group reflects decreased inhibitory activity that may partly be related to functional or structural impairment of GABAergic interneurons. TMS may be of value for determining patients with stroke at risk for developing poststroke seizures.

-----------------------
 Background: Most people show a remarkable deficit to report the second of two targets when presented in close temporal succession, reflecting an attentional restriction known as the 'attentional blink' (AB). However, there are large individual differences in the magnitude of the effect, with some people showing no such attentional restrictions.
 Background: Most people show a remarkable deficit in reporting the second of two targets (T2) when presented 200-500 ms after the first (T1), reflecting an 'attentional blink' (AB). However, there are large individual differences in the magnitude of the effect, with some people, referred to as 'non-blinkers', showing no such attentional restrictions.
-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Implicit motor sequence learning refers to an important human ability to acquire new motor skills through the repeated performance of a motor sequence. This learning process is characterized by slow, incremental gains of motor performance. The present fMRI study was developed to better delineate the areas supporting these temporal dynamics of learning. By using the serial color matching paradigm, our study focused on the motor level of sequence learning and tracked the time course of learning-related neural changes. Imaging results showed a significant contribution of the left anterior hippocampus in an early sequence acquisition stage (first scanning session) as well as during a later stage with stabilized learning effects (second scanning session). Hippocampal activation significantly correlated with the behavioral learning process and was affected by a change of the motor sequence. These results suggest a strong involvement of the hippocampus in implicit motor sequence learning. On the other hand, a very extensive and bilateral neural network of parietal, temporal and frontal cortical areas (including SMA, pre-SMA) together with parts of the cerebellum and striatum were found to play a role during random visuo-motor task performance.

 The dynamics of the neural network that underlies learning transitive structures of an ordered sequence remains poorly understood. To address this, in the present study we used fMRI to track the time course of transitive inference learning. The hippocampus and the angular gyrus were each shown to be closely related to the learning trajectory, but differentially so. Hippocampal activity was shown to consistently increase with learning but no correlation was found between performance and hippocampal activation, suggesting a general role for the hippocampus. Left angular gyrus activity was also found to consistently increase with training, but, in addition, correlated significantly with behavioral performance. This suggests an involvement of the angular gyros in learning the ordinal associations between the stimuli. (c) 2007 Elsevier Inc. All rights reserved.

-----------------------
 Background. Major bile duct injuries remain a potentially devastating complication after laparoscopic cholecystectomy. A retrospective review was conducted, of patients who underwent a biliary-enteric reconstruction of a biliary injury to assess their long-term outcome.
 Goals: Compare patient characteristics and outcome and also physician referral patterns between surgically and nonsurgically managed patients with pancreatic pseudocysts.
-----------------------
 Fixation durations in reading are longer for within-word fixation positions close to word center than for positions near word boundaries. This counterintuitive result was termed the Inverted-Optimal Viewing Position (IOVP) effect. We proposed an explanation of the effect based on error-correction of mislocated fixations [Nuthmann, A., Engbert, R., &amp; Kliegl, R. (2005). Mislocated fixations during reading and the inverted optimal viewing position effect. Vision Research, 45, 2201-2217], that suggests that the IOVP effect is not related to word processing. Here we demonstrate the existence of an IOVP effect in "mindless reading", a G-string scanning task. We compare the results from experimental data with results obtained from computer simulations of a simple model of the IOVP effect and discuss alternative accounts. We conclude that oculornotor errors, which often induce mislocalized fixations, represent the most important source of the IOVP effect. (c) 2006 Elsevier Ltd. All rights reserved.

 The launch-site effect, a systematic variation of within-word landing position as a function of launch-site distance, is among the most important oculomotor phenomena in reading. Here we show that the launch-site effect is strongly modulated in word skipping, a finding which is inconsistent with the view that the launch-site effect is caused by a saccadic-range error. We observe that distributions of landing positions in skipping saccades show an increased leftward shift compared to non-skipping saccades at equal launch-site distances. Using an improved algorithm for the estimation of mislocated fixations, we demonstrate the reliability of our results. (C) 2010 Elsevier Ltd. All rights reserved.

-----------------------
 Time-accuracy functions for tasks involving single-digit mental addition and subtraction were derived in a sample of 18 younger (mean age = 21.7 years) and 16 older adults (mean age = 68.8 years). Sequential complexity was manipulated by varying the number of operations (5 vs. 10); coordinative complexity was induced by bracketing. Age differences were apparent in the coordinative conditions, even though no age difference was present in the sequential conditions. This indicates that the age difference under conditions of high coordinative demands could not be attributed solely to a decline in basic speed of processing. The Age x Complexity interaction was due to larger onset times and lower asymptotic performance by the older adults in the coordinative conditions but not due to to rate of approach to the asymptote. This implies that coordinative demands do not differentially hurt access from semantic memory in older adults; however, coordinative demands do have disproportionately negative consequences for computation speed and self-monitoring in elder adults.

 This study investigated whether older adults could acquire the ability to perform 2 cognitive operations in parallel in a paradigm in which young adults had been shown to be able to do so (K. Oberauer &amp; R. Kliegl, 2004). Twelve young and 12 older adults practiced a numerical and a visuospatial continuous memory updating task in single-task and dual-task conditions for 16 to 24 sessions. After practice, 9 young adults were able to process the 2 tasks without dual-task costs, but none of the older adults had reached the criterion of parallel processing. The results suggest a qualitative difference between young and older adults in how they approach dual-task situations.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Dense-array electrocortical and functional hemodynamic measures of human brain activity were collected to assess the relationship between 2 established neural measures of emotional reactivity. Recorded in parallel sessions, the slow-wave late positive potential (LPP) and visual cortical blood oxygen level-dependent (BOLD) signals were both modulated by the rated intensity of picture arousal. The amplitude of the LPP correlated significantly with BOLD intensity in lateral occipital, inferotemporal, and parietal visual areas across picture contents. Estimated strength of modeled regional sources did not correlate significantly with regional BOLD intensity. These data suggest that the enhanced positive slow wave seen over posterior sites during emotional picture processing represents activity in a circuit of visual cortical structures, reflecting a perceptual sensitivity to the motivational relevance of visual scenes.

 Models of visual emotional perception suggest a reentrant organization of the ventral visual system with the amygdala. Using focused functional magnetic resonance imaging in humans with a sampling rate of 100 ms, here we determine the relative timing of emotional discrimination in amygdala and ventral visual cortical structures during emotional perception. Results show that amygdala and inferotemporal visual cortex differentiate emotional from nonemotional scenes similar to 1 s before extrastriate occipital cortex, whereas primary occipital cortex shows consistent activity across all scenes. This pattern of discrimination is consistent with a reentrant organization of emotional perception in visual processing, in which transaction between rostral ventral visual cortex and amygdala originates the identification of emotional relevance.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Using diffusion tensor imaging and tractography, we found that a disruption in structural connectivity in ventral occipitotemporal cortex may be the neurobiological basis for the lifelong impairment in face recognition that is experienced by individuals who suffer from congenital prosopagnosia. Our findings suggest that white-matter fibers in ventral occipitotemporal cortex support the integrated function of a distributed cortical network that subserves normal face processing.

 The summed activity of multiple nodes of a distributed cortical network supports face recognition in humans, including "core" ventral occipitotemporal cortex (VOTC) regions [1-3], and "extended" regions outside VOTC [4, 5]. Many individuals with congenital prosopagnosia-an impairment in face processing [6-9]-exhibit normal blood oxygenation level-dependent (BOLD) activation in the core VOTC regions [10, 11]. These individuals evince a reduction in the structural integrity of the white matter tracts connecting VOTC to anterior temporal and frontal cortices [12], part of the "extended" face network. The impairment in congenital prosopagnosia may arise, therefore, not from a dysfunction of the core VOTC areas but from a failure to propagate signals between the intact VOTC and the extended nodes of the network. Using the fMR adaptation paradigm with famous and unknown faces, we show that individuals with congenital prosopagnosia evince normal adaptation effects in VOTC, indicating sensitivity to facial identity, but show no differential activation for familiar versus unknown faces outside VOTC, particularly in the precuneus/posterior cingulate cortex and the anterior paracingulate cortex. Normal BOLD activation in VOTC is thus insufficient to subserve intact face recognition, and disrupted information propagation between VOTC and the extended face processing network may explain the functional impairment in congenital prosopagnosia.

-----------------------
 Using diffusion tensor imaging and tractography, we found that a disruption in structural connectivity in ventral occipitotemporal cortex may be the neurobiological basis for the lifelong impairment in face recognition that is experienced by individuals who suffer from congenital prosopagnosia. Our findings suggest that white-matter fibers in ventral occipitotemporal cortex support the integrated function of a distributed cortical network that subserves normal face processing.

 The summed activity of multiple nodes of a distributed cortical network supports face recognition in humans, including "core" ventral occipitotemporal cortex (VOTC) regions [1-3], and "extended" regions outside VOTC [4, 5]. Many individuals with congenital prosopagnosia-an impairment in face processing [6-9]-exhibit normal blood oxygenation level-dependent (BOLD) activation in the core VOTC regions [10, 11]. These individuals evince a reduction in the structural integrity of the white matter tracts connecting VOTC to anterior temporal and frontal cortices [12], part of the "extended" face network. The impairment in congenital prosopagnosia may arise, therefore, not from a dysfunction of the core VOTC areas but from a failure to propagate signals between the intact VOTC and the extended nodes of the network. Using the fMR adaptation paradigm with famous and unknown faces, we show that individuals with congenital prosopagnosia evince normal adaptation effects in VOTC, indicating sensitivity to facial identity, but show no differential activation for familiar versus unknown faces outside VOTC, particularly in the precuneus/posterior cingulate cortex and the anterior paracingulate cortex. Normal BOLD activation in VOTC is thus insufficient to subserve intact face recognition, and disrupted information propagation between VOTC and the extended face processing network may explain the functional impairment in congenital prosopagnosia.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Research suggests that the reaching hand automatically deviates toward a target that changes location jumps) during the reach. In the current Study, we investigated whether movement intention can influence the target jump's impact on the hand. We compared the degree of trajectory deviation to a jumped target under three instruction conditions: (I) GO, in which participants were told to go to the target if it jumped, (2) STOP, in which participants were told to immediately stop their movement if the target jumped, and (3) IGNORE, in which participants were told to ignore the target if it jumped and to continue to its initial location. We observed a reduced response to the jump in the IGNORE condition relative to the other conditions, suggesting that the response to the jump is contingent on the jump being a task-relevant event. (C) 2009 Elsevier Inc. All rights reserved.

 Pisella et al. (2000) have shown that fast aiming movements are automatically modified on-line in response to a change in target position. Specifically, when a movement is less than 300 ms in duration the reach is completed to a target's new location even when one never intended to respond to the target jump. In contrast, when movements are slower, the reach is completed according to instructions. At present, it is unclear if it is possible for one's intentions to guide the initial stages of these slow movements. To determine if the intentional control mechanism can guide the initial stages of a slow aiming movement, participants aimed to targets that could jump at movement onset, with a slow and very slow movement time goal. In particular, participants were to point towards ("pro-point") or away from ("anti-point") the target jump, with a movement time goal of 500 or 1200 ms. Results showed that in the anti-point condition, movement trajectories first deviated in the same direction as the target jump, followed by a response in the intended (opposite) direction. This suggests that while movement outcome is controlled by the intentional system, even in these slow aiming movements the automatic system is engaged at movement onset. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Stochastic vestibular stimulation(SVS) can be used to study the postural responses to unpredictable vestibular perturbations. The present study seeks to determine if stochastic vestibular stimulation elicits lower limb muscular responses and to estimate the frequency characteristics of these vestibulo-motor responses in humans. Fourteen healthy subjects were exposed to unpredictable galvanic currents applied on their mastoid processes while quietly standing ( +/- 3 mA, 0-50 Hz). The current amplitude and stimulation configuration as well as the subject's head position relative to their feet were manipulated in order to determine that: (1) the muscle responses evoked by stochastic currents are dependent on the amplitude of the current, (2) the muscle responses evoked by stochastic currents are specific to the percutaneous stimulation of vestibular afferents and (3) the lower limb muscle responses exhibit polarity changes with different head positions as previously described for square-wave galvanic vestibular stimulation (GVS) pulses. Our results revealed significant coherence (between 0 and 20 Hz) and cumulant density functions (peak responses at 65 and 103 ms) between SVS and the lower limbs' postural muscle activity. The polarity of the cumulant density functions corresponded to that of the reflexes elicited by square-wave GVS pulses. The SVS-muscle activity coherence and time cumulant functions were modulated by current amplitude, electrode position and head orientation with respect to the subject's feet. These findings strongly support the vestibular origin of the lower limb muscles evoked by SVS. In addition, specific frequency bandwidths in the stochastic vestibular signal contributed to the early (12-20 Hz) and late components (2-10 Hz) of the SVS-evoked muscular responses. These frequency-dependent SVS-evoked muscle responses support the view that the biphasic muscle response is conveyed by two distinct physiological processes.

 The application of galvanic vestibular stimulation (GVS) evokes distinct responses in lower limb muscles involved in the control of balance. The purpose of this study was to investigate the balance and lower limb muscle responses to short-duration GVS and to determine whether these responses are modulated by small changes in center of gravity (CoG) and baseline muscle activity occurring during quiet standing. Twelve subjects stood quietly on a force plate with their feet together and were instructed to look straight ahead. One thousand twenty-four GVS stimuli (4 mA, 20-ms pulses) were delivered bilaterally to the mastoid processes in a bipolar, binaural configuration. Bilateral surface electromyography (EMG) from soleus (Sol) and tibialis anterior (TA) and ground reaction forces were recorded. EMG and force responses were trigger averaged at the onset of the GVS pulse. Short-duration GVS applied during quiet standing with the head facing forward evoked characteristic balance responses and biphasic modulation of all muscles with the same polarity for ipsilateral Sol and TA. The amplitude of the GVS-evoked muscle responses was modulated by both the estimated position of the subject's CoG and the background activation of the recorded muscle. Muscle-dependent modulations of the GVS-evoked muscle responses were observed: the Sol responses decreased, while the TA responses increased when the CoG position shifted toward the heels. The well-defined balance responses evoked by short-duration GVS are important to acknowledge when studying the vestibulo-motor responses in healthy subjects and patient populations.

-----------------------
 Discriminatory information about person identity is multimodal. Yet, most person recognition systems are unimodal, e.g. the use of facial appearance. With a view to exploiting the complementary nature of different modes of information and increasing pattern recognition robustness to test signal degradation, we developed a multiple expert biometric person identification system that combines information from three experts: face, visual speech, and audio. The system uses multimodal fusion in an automatic unsupervised manner, adapting to the local performance and output reliability of each of the experts. The expert weightings are chosen automatically such that the reliability measure of the combined scores is maximized. To test system robustness to train/test mismatch, we used a broad range of Gaussian noise and JPEG compression to degrade the audio and visual signals, respectively. Experiments were carried out on the XM2VTS database. The multimodal expert system out performed each of the single experts in all comparisons. At severe audio and visual mismatch levels tested, the audio, mouth, face, and tri-expert fusion accuracies were 37.1%, 48%, 75%, and 92.7% respectively, representing a relative improvement of 23.6% over the best performing expert.

 Information about person identity is multimodal. Yet, most person-recognition systems limit themselves to only a single modality, such as facial appearance. With a view to exploiting the complementary nature of different modes of information and increasing pattern recognition robustness to test signal degradation, we developed a multiple expert biometric person identification system that combines information from three experts: audio, visual speech, and face. The system uses multimodal fusion in an automatic unsupervised manner, adapting to the local performance (at the transaction level) and output reliability of each of the three experts. The expert weightings are chosen automatically such that the reliability measure of the combined scores is maximized. To test system robustness to train/test mismatch, we used a broad range of acoustic babble noise and JPEG compression to degrade the audio and visual signals, respectively. Identification experiments were carried out on a 248-subject subset of the XM2VTS database. The multimodal expert system outperformed each of the single experts in all comparisons. At severe audio and visual mismatch levels tested, the audio, mouth, face, and tri-expert fusion accuracies were 16.1%, 48%, 75%, and 89.9%, respectively, representing a relative improvement of 19.9% over the best performing expert.

-----------------------
 Previous work showed that older adults' choice performance can be wiser than that of younger adults (Tentori, Osherson, Hasher, &amp; May, 2001). We contrasted two possible interpretations: a general expertise/wisdom view that suggests that older adults are generally more skilled at making decisions than younger adults and a domain-specific expertise view that suggests that older adults are more skilled decision makers only in domains in which they have greater knowledge. These hypotheses were contrasted using attraction effect tasks in two different domains: carning extra credit in a course and grocery shopping, domains presumed to be of different levels of knowledge to younger and older adults. Older adults showed consistent choice for both domains; younger adults showed consistent choice only for the extra credit problem. Several explanations of these findings are considered, including Damasio's somatic marker theory and age differences in reliance on heuristic versus analytic styles.

 A decision-maker is "irregular" if she would choose B from {A, B, C} but not from {A, B} (for example, preferring vanilla ice cream in a choice between vanilla and chocolate, but chocolate in a choice among vanilla, chocolate and strawberry). Similarly to previous studies we observed irregular choices by college students faced with hypothetical discount cards for supermarkets. However, older adults showed no such tendency. The same pattern was observed in three separate studies. We interpret the results in terms of a choice strategy by older adults that protects them from excessive spending. (C) 2001 Elsevier Science B.V. All ri-hts reserved.

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 The purpose of the present study was to provide normative data for foot loading patterns and foot shape parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood. In a longitudinal design, 90 healthy German children were followed over the course of 6-48 months. The children had a mean age of 15 months at the first appointment and 63 months at the last appointment. The children participated in 11 measurement appointments every 3 months or 6 months. Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot shape. During the investigation period a significant increase of peak pressure in almost every foot region was observed. Peak pressure of the total foot increased by 92%. Only for the midfoot region a significantly decreased impulse by about 15% per year and a significant decrease of the contact area by 9% per year could be observed. A significant influence of gender was observed for peak pressures and impulses in some foot regions as well as for the midfoot width and the foot shape index. The established database can be used as comparative values for clinical decisions about the normal development. (c) 2006 Elsevier B.V. All rights reserved.

 The purpose of the present study was to provide normative data for foot loading patterns and foot form parameters in order to support decisions about the normal or abnormal development of the growing foot during childhood In a longitudinal design 36 healthy German children were followed over the course of nine years The children had a mean age of 146 +/- 1 8 months at the first appointment and 122 8 +/- 2 0 months at the last appointment The children participated in 17 measurement appointments every 3 6 or 12 months Dynamic foot loading was evaluated with plantar pressure measurements during walking and static footprints were taken to determine changes in foot form During the investigation period an increase of peak pressures of the total foot by 190% of the relative maximum force of the total foot by 20% and the foot length by 90% was observed A decrease for the relative maximum force under the midfoot (63%) and for the arch index (49%) could also be demonstrated Furthermore body height showed a significant influence on foot length and midfoot width Body weight had a significant influence on the static parameter midfoot width Between genders boys showed a significant wider midfoot and a smaller forefoot contact area as compared to girls The established database can be used as comparative values for clinical decisions about the normal foot development (C) 2010 Elsevier B V All rights reserved

-----------------------
 It is often intuitively assumed that disconnected image fragments result in a representation of separate objects. When objects are partly occluded, disconnected image fragments can still result in a representation of a single object, based on visual completion. In a simultaneous matching task, displays showing one object, partly occluded objects, or two objects were compared with each other. When only a translation was required to match pairs of displays, one-object displays were matched faster than both occluded-object and two-object displays, which did not differ significantly from each other. When mental rotation and translation were required, the one-object displays were again matched the fastest. In addition, an advantage for occluded-object displays compared with two-object displays was found. We conclude that when the generation of a mental representation is likely, object-based connectedness determines object matching. Mental rotation then seems to depend on the number of objects rather than on the number of image fragments. (C) 2004 Elsevier B.V. All rights reserved.

 The presupposed advantage of symmetrical objects over asymmetrical objects was investigated in an object matching task, using accidental and non-accidental viewpoints. In addition, the accidental views could be symmetric or asymmetric. When two non-accidental views were presented, symmetrical objects were matched faster than asymmetrical objects. When an accidental view was presented first (followed by a non-accidental view), the matching of symmetrical objects was equal to that of asymmetrical objects. When a non-accidental view was presented first (followed by an accidental view), matching was again equal for the symmetrical and asymmetrical objects, although much faster compared with the opposite sequence of presented views. No effects of image symmetry in the accidental viewpoints were found. Apparently, the advantage of symmetrical objects over asymmetrical objects is only present in object matching when 3-D object structures are visible.

-----------------------
 It is often intuitively assumed that disconnected image fragments result in a representation of separate objects. When objects are partly occluded, disconnected image fragments can still result in a representation of a single object, based on visual completion. In a simultaneous matching task, displays showing one object, partly occluded objects, or two objects were compared with each other. When only a translation was required to match pairs of displays, one-object displays were matched faster than both occluded-object and two-object displays, which did not differ significantly from each other. When mental rotation and translation were required, the one-object displays were again matched the fastest. In addition, an advantage for occluded-object displays compared with two-object displays was found. We conclude that when the generation of a mental representation is likely, object-based connectedness determines object matching. Mental rotation then seems to depend on the number of objects rather than on the number of image fragments. (C) 2004 Elsevier B.V. All rights reserved.

 The presupposed advantage of symmetrical objects over asymmetrical objects was investigated in an object matching task, using accidental and non-accidental viewpoints. In addition, the accidental views could be symmetric or asymmetric. When two non-accidental views were presented, symmetrical objects were matched faster than asymmetrical objects. When an accidental view was presented first (followed by a non-accidental view), the matching of symmetrical objects was equal to that of asymmetrical objects. When a non-accidental view was presented first (followed by an accidental view), matching was again equal for the symmetrical and asymmetrical objects, although much faster compared with the opposite sequence of presented views. No effects of image symmetry in the accidental viewpoints were found. Apparently, the advantage of symmetrical objects over asymmetrical objects is only present in object matching when 3-D object structures are visible.

-----------------------
 The timing of repetitive movements was assessed in a callosotomy patient under unimanual and bimanual conditions. Similar to neurologically healthy individuals, the patient exhibited strong temporal coupling in the bimanual condition. Moreover, for both the left and right hands, within-hand temporal variability was reduced in the bimanual condition compared to the unimanual conditions. This bimanual advantage is hypothesized to reflect the temporal integration of separable timing signals, one associated with the left hand and one associated with the right hand (Helmuth, L. L., &amp; Ivry, R. B. (1996). When two hands are better than one: Reduced timing variability during bimanual movements. Journal of Experimental Psychology. Human Perception and Performance, 2, 278-293). The fact that it persists following callosotomy is inconsistent with models that attribute bimanual coordination in these patients to the control of a single hemisphere. Rather, the results suggest that motor commands from the two hemispheres are integrated subcortically. (C) 1999 Elsevier Science B.V. All rights reserved.

 Rhythmic bimanual movements are highly constrained in the temporal domain, with the gestures of the two hands tightly synchronized. Previous studies have implicated a subcortical locus for temporal coupling based on the observation that these constraints persist in callosotomy patients. We now report that such coupling is restricted to movements entailing a discrete event (such as a movement onset). Three callosotomy patients exhibited a striking lack of temporal coupling during continuous movements, with the two hands oscillating at non-identical frequencies. We propose a subcortical locus of temporal coupling for movements involving discrete events. In contrast, synchronization between the hands during continuous movements depends on interhemispheric transmission across the corpus callosum.

-----------------------
 Participants were required to detect spot stimuli briefly presented to the upper, central, or lower visual fields. The stimuli were presented either on a green or a red background. Results showed that reaction time (RT) was shorter for the lower visual field (LVF) compared to the upper visual field (UVF). Furthermore, this LVF advantage was significantly reduced in the red background condition compared to the green one. A red light is known to suppress activity of the magno-dominated stream. Therefore, the LVF advantage in RT can be explained as resulting from the biased representation of the magno-dominated stream in the LVF. (C) 2004 Elsevier Inc. All rights reserved.

 Right-handed participants performed simple visual judgments on nonverbal stimuli presented either to the left visual field-right hemisphere (LVF-RH) or to the right visual field-left hemisphere (RVF-LH). The stimuli were exposed for 40-120 msec, followed by a backward mask. When the stimuli were presented against a green background, an RVF-LH advantage was observed for the shortest exposure duration. This result supports the notion that the LH has finer temporal resolution than the RH. Imposition of a red background disrupted performance and eliminated the RVF-LH advantage for the shortest exposure duration. Because the red background attenuates functions of the magnocellular pathway, these results suggest that the magnocellular pathway contributes to the LH advantage for fine temporal resolution.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 Previous research has demonstrated a relationship between the regularity of motor production and the ability to make accurate perceptual judgments. The current study investigated the temporal abilities of two groups of patients with known movement problems (musicians' and writers' cramp), some of whom have had many years of training in temporal discrimination. Patients and controls (musician and nonmusician, respectively) judged whether the last of six sequential auditory or tactile stimuli occurred earlier or later in comparison to five previously and regularly presented stimuli. In both sensory domains, patients with musicians' cramp detected the early stimulus better than controls. When detecting the onset of late stimuli, only in the auditory domain were patients worse than controls. Patients with writers' cramp, however, did not show any significant group differences in either auditory or tactile domains, suggesting that such patients are not deficient in processing sequential stimuli. In conclusion, compared to controls, patients with musician's cramp demonstrated generalized timing anomalies, occurring in the symptomatic (tactile) and the asymptomatic (auditory) sensory domains. This timing problem is likely to be a consequence of the dystonic symptoms rather than the cause. (C) 2003 Movement Disorder Society.

 Auditory evoked potentials (AEPs) were examined in patients with musician's cramp (focal dystonia) in order to determine whether these patients have electrophysiological changes in a sensory system that is not usually associated with symptoms. All participants were professional guitarists and were required to listen to 2000 monaurally presented stimuli (middle C, with duration of 7 ms). During one block, 250 stimuli were presented to one ear. Once a block was finished, another block was presented in the other ear; in total there were eight blocks of stimuli. During this task, EEGs from 10 scalp electrodes and one bipolar eye channel were continuously recorded. There were no significant latency or topographical differences in the electrophysiological recordings. However, there was a significant group difference in the peak-to-peak amplitude of the P1-N1a component. The patients had a larger peak-to-peak difference than controls (1.63 vs. 0.62 mu V). The P1 and N1a are cortically generated potentials. Patients with focal dystonia had an increase in activity compared to controls when processing simple auditory stimuli. Such changes in electrophysiological responses may be a result of increases in excitation or lack of inhibition; alternatively the changes may represent cross-modal maladaptive plasticity from the somatosensory modality to the auditory modality. Thus, this study provides further evidence that patients with focal dystonia have alterations of the central nervous system that are not limited to their symptomatic sensory domain.

-----------------------
 Target identification is impaired when targets are presented during the planning or execution of a compatible response (e.g., right-pointing arrow during a right keypress) relative to an incompatible response (Musseler &amp; Hommel, 1997 a, b). Examinations of this blindness to response-compatible stimuli have typically used arrowheads as targets ("&lt;" and "&gt;"). The importance of the target symbol was examined by manipulating subjects' interpretation of that symbol (i.e., "&gt;" interpreted as a right-pointing arrow or as a headlight shining to the left). Targets were presented at varying times during the planning or execution of a response in order to examine the time-course of the effect. Results showed that the interpretation, and not the physical identity, of the target was important for the blindness effect. Although the blindness effect was largest during the planning and execution of a response, it was not always confined to that temporal interval.

 Targets are identified more accurately when they are presented during an incongruent response (e.g., a left-pointing arrowhead presented during a right key press) than during a congruent response (e.g., a left-pointing arrowhead presented during a left key press). This effect, referred to here as congruency-induced blindness, has been hypothesised to result from the occupation of feature codes. According to the code occupation hypothesis (Behav. Brain Sci.; J. Experim. Psychol.: Human Percept. Perform.; Vis. Cogn., in press), only costs of congruency between features of a planned or executed action and a to-be-perceived target should be observed; neither costs nor benefits of incongruency are predicted by this account. In the present study, we investigated costs and benefits in identifying left and right targets directly by manipulating neutral response type and the symbols used to cue the neutral response, which produced four neutral conditions. Three important results emerged: (1) a significant main effect of RSI (suggesting that increasing temporal overlap between a planned action and target presentation interferes with perceptual reports of the target), (2) a significant main effect of congruency (showing that impairment is code-specific), and (3) clear-cut costs with little evidence for benefits. Other complex patterns of results provided additional information relevant for extant theories of perception-action interactions. (C) 2002 Elsevier Science B.V. All rights reserved.

-----------------------
 Event-related brain potentials (ERPs) were used in two experiments to examine the neural correlates of processes underlying task switching in the information-reduction task switching paradigm. Each experiment included 22 participants. The paradigm included two cues for each task. This element of the design allowed us to differentiate the ERP correlates of cue retrieval, task set reconfiguration, and rule mapping. The ERP data revealed a parietal slow wave that was sensitive to processes associated with cue retrieval and task set reconfiguration and a frontal-polar slow wave that was sensitive to processes associated with rule mapping. These findings further the proposal that an endogenous act of control supporting processes related to task set reconfiguration and rule mapping may facilitate performance of the explicit cue task switching paradigm.

 Event-related brain potentials were used to examine the neural correlates of task switching directed by task cues and transition cues. Task cues signal both a change of task set and the task to implement; in contrast, transition cues signal a change of task set but do not indicate the required task. The data from two experiments revealed that the frontal P2 and reconfiguration slow wave were elicited by task and transition cues and may reflect processes associated with the change detector and task set configuration. Experiment 2 revealed that the frontal positivity and transition parietal slow wave are associated with the retrieval of the prior task set from memory. These data indicate that distinct neural processes that are related to the change detector, task set configuration, and the retrieval of a recently utilized task set from memory support task switching that is guided by task and transition cues.

-----------------------
 Song JH, McPeek RM. Eye-hand coordination during target selection in a pop-out visual search. J Neurophysiol 102: 2681-2692, 2009. First published September 2, 2009; doi: 10.1152/jn.91352.2008. We examined the coordination of saccades and reaches in a visual search task in which monkeys were rewarded for reaching to an odd-colored target among distractors. Eye movements were unconstrained, and monkeys typically made one or more saccades before initiating a reach. Target selection for reaching and saccades was highly correlated with the hand and eyes landing near the same final stimulus both for correct reaches to the target and for incorrect reaches to a distractor. Incorrect reaches showed a bias in target selection: they were directed to the distractor in the same hemifield as the target more often than to other distractors. A similar bias was seen in target selection for the initial saccade in correct reaching trials with multiple saccades. We also examined the temporal coupling of saccades and reaches. In trials with a single saccade, a reaching movement was made after a fairly stereotyped delay. In multiple-saccade trials, a reach to the target could be initiated near or even before the onset of the final target-directed saccade. In these trials, the initial trajectory of the reach was often directed toward the fixated distractor before veering toward the target around the time of the final saccade. In virtually all cases, the eyes arrived at the target before the hand, and remained fixated until reach completion. Overall, these results are consistent with flexible temporal coupling of saccade and reach initiation, but fairly tight coupling of target selection for the two types of action.

 Song J-H, McPeek RM. Roles of narrow- and broad-spiking dorsal premotor area neurons in reach target selection and movement production. J Neurophysiol 103: 2124-2138, 2010. First published February 17, 2010; doi: 10.1152/jn.00238.2009. Most visual scenes are complex and crowded, with several different objects competing for attention and action. Thus a complete understanding of the production of goal-directed actions must incorporate the higher-level process of target selection. To examine the neural substrates of target selection for visually guided reaching, we recorded the activity of isolated neurons in the dorsal premotor area (PMd) of monkeys performing a reaction-time visual search task. In this task, monkeys reached to an odd-colored target presented with three distractors. We found that PMd neurons typically discriminate the target before movement onset, similar to 150-200 ms after the appearance of the search array. In one subset of neurons, discrimination occurred at a consistent time after search array onset regardless of when the reaching movement occurred, suggesting that these neurons are involved in target selection. In a second group of neurons, discrimination time depended on reach reaction time, consistent with involvement in movement production but not in target selection. To look for physiological corroboration of these two functionally defined groups, we analyzed the extracellular spike waveforms of recorded neurons. This analysis showed a population of neurons with narrow action potentials that carried signals related to target selection. A second population with broader action potentials was more heterogeneous, with some neurons showing activity related to target selection and others showing only movement production activity. These results suggest that PMd contains signals related to target selection and movement execution and that different signals are carried by distinct neural subpopulations.

-----------------------
 Following transcranial magnetic stimulation (TMS) at stimulation strength of 1.5 times the resting motor threshold a silent period (SP) of similar to 180 ms duration can be observed in surface EMG-registrations of tonically activated small hand muscles. This SP is believed to be generated cortically and can be prolonged in stroke patients, but it is nor known whether a prolongation of the SP has arty functional significance. In order to answer the question of whether enhanced cortical inhibition can contribute to pathophysiology of motor dysfunction we studied stroke patients with clearly prolonged SP durations in the first dorsal interosseus muscle (&gt;2 times that of the intact side), but with normal magnetically evoked motor potentials. Sixteen patients out of a cohort of 174 consecutive patients presenting with acute hemiparetic stroke fulfilled the inclusion criteria. Serial TMS investigations were performed for up to 2 years post-stroke. In all patients, the SP duration decreased in parallel with clinical improvement In two patients, intermittent clinical deterioration was accompanied by an increase in the SP duration. In four patients, in addition to a markedly prolonged SP duration, the phenomenon of a complete inability to initiate voluntary muscle activity for several seconds, following TMS, could be observed in a number of trials ('motor arrest'). Detailed clinical analysis revealed that, in addition to hemiparesis, distinct motor disturbances in patients with SP prolongation could be observed. These motor disturbances resembled those of motor neglect and were characterized by motivationally dependent under-utilization of the affected arm, impairment of movement initiation, inability to maintain a constant force level and to scale forces, and impairment of individual finger movements. In 12 of the 16 patients at least one additional behavioural manifestation of neglect was present. We suggest that in stroke patients severe motor dysfunction. may be caused by hyperactivity of cortical inhibitory interneurons rather than by direct lesions of descending motor tracts Cortical hyperinhibition may, in turn, results from damage to any of a number of afferent pathways to the motor cortex which modulate local interneuronal activity.

 Background: Following an ischemic brain lesion, the affected cortex undergoes structural and functional changes that may lead to increased cortical excitability or decreased inhibitory neuronal activity, resulting in the occurrence of poststroke epileptic seizures in 6 to 10% of patients with stroke. Methods: To assess motor cortical excitability, transcranial magnetic stimulation (TMS) was used to determine the silent period (SP) duration in 84 consecutive patients with ischemic stroke. Results: In a subpopulation of six patients (38 to 72 years old) a significant decrease of the SP duration (mean 116 +/- 14 msec) was detected in either the arm or the leg on the affected side as compared to the corresponding unaffected limb (mean 231 +/- 32 msec). This electrophysiologic abnormality was clinically associated with focal motor seizures in five of the six patients, whereas none of the other 76 patients with normal or prolonged SP durations developed seizures or epilepsy. Conclusions: Silent period shortening in this group reflects decreased inhibitory activity that may partly be related to functional or structural impairment of GABAergic interneurons. TMS may be of value for determining patients with stroke at risk for developing poststroke seizures.

-----------------------
 Following transcranial magnetic stimulation (TMS) at stimulation strength of 1.5 times the resting motor threshold a silent period (SP) of similar to 180 ms duration can be observed in surface EMG-registrations of tonically activated small hand muscles. This SP is believed to be generated cortically and can be prolonged in stroke patients, but it is nor known whether a prolongation of the SP has arty functional significance. In order to answer the question of whether enhanced cortical inhibition can contribute to pathophysiology of motor dysfunction we studied stroke patients with clearly prolonged SP durations in the first dorsal interosseus muscle (&gt;2 times that of the intact side), but with normal magnetically evoked motor potentials. Sixteen patients out of a cohort of 174 consecutive patients presenting with acute hemiparetic stroke fulfilled the inclusion criteria. Serial TMS investigations were performed for up to 2 years post-stroke. In all patients, the SP duration decreased in parallel with clinical improvement In two patients, intermittent clinical deterioration was accompanied by an increase in the SP duration. In four patients, in addition to a markedly prolonged SP duration, the phenomenon of a complete inability to initiate voluntary muscle activity for several seconds, following TMS, could be observed in a number of trials ('motor arrest'). Detailed clinical analysis revealed that, in addition to hemiparesis, distinct motor disturbances in patients with SP prolongation could be observed. These motor disturbances resembled those of motor neglect and were characterized by motivationally dependent under-utilization of the affected arm, impairment of movement initiation, inability to maintain a constant force level and to scale forces, and impairment of individual finger movements. In 12 of the 16 patients at least one additional behavioural manifestation of neglect was present. We suggest that in stroke patients severe motor dysfunction. may be caused by hyperactivity of cortical inhibitory interneurons rather than by direct lesions of descending motor tracts Cortical hyperinhibition may, in turn, results from damage to any of a number of afferent pathways to the motor cortex which modulate local interneuronal activity.

 Background: Following an ischemic brain lesion, the affected cortex undergoes structural and functional changes that may lead to increased cortical excitability or decreased inhibitory neuronal activity, resulting in the occurrence of poststroke epileptic seizures in 6 to 10% of patients with stroke. Methods: To assess motor cortical excitability, transcranial magnetic stimulation (TMS) was used to determine the silent period (SP) duration in 84 consecutive patients with ischemic stroke. Results: In a subpopulation of six patients (38 to 72 years old) a significant decrease of the SP duration (mean 116 +/- 14 msec) was detected in either the arm or the leg on the affected side as compared to the corresponding unaffected limb (mean 231 +/- 32 msec). This electrophysiologic abnormality was clinically associated with focal motor seizures in five of the six patients, whereas none of the other 76 patients with normal or prolonged SP durations developed seizures or epilepsy. Conclusions: Silent period shortening in this group reflects decreased inhibitory activity that may partly be related to functional or structural impairment of GABAergic interneurons. TMS may be of value for determining patients with stroke at risk for developing poststroke seizures.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 The stop-signal paradigm is a popular method for examining response inhibition and impulse control in psychology, cognitive neuroscience, and clinical domains because it allows the estimation of the covert latency of the stop process: the stop-signal reaction time (SSRT). In three sets of simulations, we examined to what extent SSRTs that were estimated with the popular mean and integration methods were influenced by the skew of the reaction time distribution and the gradual slowing of the response latencies. We found that the mean method consistently overestimated SSRT. The integration method tended to underestimate SSRT when response latencies gradually increased. This underestimation bias was absent when SSRTs were estimated with the integration method for smaller blocks of trials. Thus, skewing and response slowing can lead to spurious inhibitory differences. We recommend that the mean method of estimating SSRT be abandoned in favor of the integration method.

 Background: Response inhibition is a prototypical executive function of considerable clinical relevance to psychiatry. Nevertheless, our understanding of its pharmacological modulation remains incomplete.
-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 It has been found that disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested domestic chicks ha the some task and found that they mere able to conjoin geometric and nongeometric (landmark) information to reorient themselves in both the large and the small space used. Moreover; chicks reoriented immediately when displaced from a large to a small experimental space and vice versa, suggesting that they used the relative metrics of the environment. However; when tested with a transformation (affine transformation) that alters the geometric relations between the target and the shape of the environment, chicks tended to make more errors based on geometric information when tested in the small than in the large space. These findings suggest that the reliance of the use of geometric information on the spatial scale of the environment is not restricted to the human species.

 Disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested fish in the same task and found that they were able to conjoin geometric and non-geometric (landmark) information to reorient themselves in both the large and the small space used. Moreover, fish proved able to reorient immediately when dislocated from a large to a small experimental space and vice versa, suggesting that they encoded the relative rather than the absolute metrics of the environment. However, fish tended to make relatively more errors based on geometric information when transfer occurred from a small to a large space, and to make relatively more errors based on landmark information when transfer occurred from a large to a small space. The hypothesis is discussed that organisms are prepared to use only distant featural information as landmarks. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Background: This study evaluated the effectiveness of an intervention for reading-delayed children in Year-1 classes. Methods: A sample (N = 77) of children drawn from 14 schools representing those with the weakest reading skills were randomly allocated to one of two groups. A 20-week intervention group received the intervention for two consecutive 10-week periods, while a 10-week intervention group only received the intervention for the second 10 weeks of the study. The programme was delivered in daily 20-minute sessions that alternated between small group (N = 3) and individual teaching. The programme combined phoneme awareness training, word and text reading, and phonological linkage exercises. Results: The children receiving the intervention during the first 10-week period made significantly more progress on measures of letter knowledge, single word reading, and phoneme awareness than children not receiving the intervention. However, the children who only received the intervention during the second 10-week period made rapid progress and appeared to catch up with the children who had been given the more prolonged intervention. Failure to respond to the intervention was predicted by poor initial literacy skills and being in receipt of free school meals. Conclusion: A reading intervention programme delivered on a daily basis by trained teaching assistants is an effective intervention for children who show reading delays at the end of their first year in school. However, around one-quarter of the children did not respond to this intervention and these children would appear to need more intensive or more prolonged help to improve their reading skills.

 The authors examine the reading profile in children with Down syndrome by comparing the nonword decoding skills in children with Down syndrome and typically developing children matched for word recognition level. Journal articles published before 04.05.2010 were identified by using the keyword Down* cross-referenced to 'reading', 'literacy', 'decoding', and 'reading comprehension' were selected. A total of eight papers met the criteria for inclusion. Each study was reviewed and coded on both inclusion criteria and coding protocol before the analysis was performed. Children with Down syndrome had equivalent nonword decoding skills to typically developing children matched for word recognition level, but showed deficits on measures of two important underlying skills, vocabulary and phonological awareness. Differences in vocabulary, but not phonological awareness, were predictive of differences in nonword decoding skills. The practical and theoretical implications of these findings are discussed. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Hand allograft is a method in the stage of clinical experimentation, which is reserved in France for the treatment of bilateral traumatic amputees. This study reports the Lyon team experience, which is pioneer in this domain. Four patients Q mates and 1 female) underwent seven (one unilateral and three bilateral) hand transplantations from September 1998 to February 2007. The Level of amputation was at the wrist or at the mid-forearm. Delay since hand loss ranged from 2.5 to 9 years. The surgical protocol was elaborated and planned case by case. All. recipients received the same immunosuppressive treatment. Episodes of acute rejection were observed in the first 3 months after transplantation, which were easily managed after a few days increasing oral prednisone doses and applying topical immunosuppressants. Currently the patients receive the doses of immunosuppressants comparable to those in kidney-grafted patients. We have not registered any severe complication of immunosuppressive treatment up tilt now (7 years follow-up for the earliest graft). We performed analytical and functional clinical, as well as questionnaire evaluation of patients. The first case (unilateral graft) resulted in graft failure at 2 years due to non-compliance of the patient. The three bilateral graftees demonstrate a favorable evolution despite. some immunological (hyperglycemia, serum sickness) and surgical (thrombosis, osteomyelitis, skin toss) complications, which could be managed. The middle and long-term follow-up evaluation revealed good to excellent sensorimotor recovery of 4 hands in both mate recipients (4 and 7 years) with satisfactory social adaptation, higher or equal to those expected after post-traumatic replantations at the equivalent level and higher to those obtained with currently available myoelectric prosthesis. The last patient, a young female who has been grafted in February 2007, receives ongoing reeducation course and shows normal progress of functional restoration of both hands. The encouraging results of this clinical experimentation make us currently consider hand allografting as reasonable and useful both for the patients and for evolution of research in composite tissues allotransplantation (CTA). Further long-term careful research and worldwide monitoring of all patients with hand allografts is required to, on the one part, state on the authorization of this surgery, and, on the other part, to better elucidate the mechanisms of successful CTA. (c) 2007 Publie par Elsevier Masson SAS.

 Neuropsychological studies have revealed that schizophrenic (SZ) patients have severe impairments in the cognitive integration of static and moving perceptual stimuli. Research on knowledge structures has shown that sequences of continuous actions are represented in memory as clusters of goal-directed events in a hierarchical manner. In the present study, we investigated the ability to segment familiar sequences of dynamic goal-directed actions into small and large meaningful units in a group of patients with schizophrenia (N = 16) and a group of healthy control subjects (N = 17). While viewing two videotaped movies , participants were requested to detect the transitions between component events at both low and high levels of the action categorical structure. Both groups detected significantly more events under the small-oriented condition as compared to the large-oriented condition. Differently from normal controls, patients recalled the event scenes in a detailed and fragmentary manner and showed considerable difficulties in detecting large action units. Moreover, low performance on action boundary detection significantly correlated with higher levels of disorganisation symptoms in patients with schizophrenia. A defective conceptual organisation of perceptive action knowledge would help to explain the severe everyday difficulties of these patients both in monitoring their own actions and in understanding others intentions. (C) 2004 Elsevier Ireland Ltd. All rights reserved.

-----------------------
 Hand allograft is a method in the stage of clinical experimentation, which is reserved in France for the treatment of bilateral traumatic amputees. This study reports the Lyon team experience, which is pioneer in this domain. Four patients Q mates and 1 female) underwent seven (one unilateral and three bilateral) hand transplantations from September 1998 to February 2007. The Level of amputation was at the wrist or at the mid-forearm. Delay since hand loss ranged from 2.5 to 9 years. The surgical protocol was elaborated and planned case by case. All. recipients received the same immunosuppressive treatment. Episodes of acute rejection were observed in the first 3 months after transplantation, which were easily managed after a few days increasing oral prednisone doses and applying topical immunosuppressants. Currently the patients receive the doses of immunosuppressants comparable to those in kidney-grafted patients. We have not registered any severe complication of immunosuppressive treatment up tilt now (7 years follow-up for the earliest graft). We performed analytical and functional clinical, as well as questionnaire evaluation of patients. The first case (unilateral graft) resulted in graft failure at 2 years due to non-compliance of the patient. The three bilateral graftees demonstrate a favorable evolution despite. some immunological (hyperglycemia, serum sickness) and surgical (thrombosis, osteomyelitis, skin toss) complications, which could be managed. The middle and long-term follow-up evaluation revealed good to excellent sensorimotor recovery of 4 hands in both mate recipients (4 and 7 years) with satisfactory social adaptation, higher or equal to those expected after post-traumatic replantations at the equivalent level and higher to those obtained with currently available myoelectric prosthesis. The last patient, a young female who has been grafted in February 2007, receives ongoing reeducation course and shows normal progress of functional restoration of both hands. The encouraging results of this clinical experimentation make us currently consider hand allografting as reasonable and useful both for the patients and for evolution of research in composite tissues allotransplantation (CTA). Further long-term careful research and worldwide monitoring of all patients with hand allografts is required to, on the one part, state on the authorization of this surgery, and, on the other part, to better elucidate the mechanisms of successful CTA. (c) 2007 Publie par Elsevier Masson SAS.

 Neuropsychological studies have revealed that schizophrenic (SZ) patients have severe impairments in the cognitive integration of static and moving perceptual stimuli. Research on knowledge structures has shown that sequences of continuous actions are represented in memory as clusters of goal-directed events in a hierarchical manner. In the present study, we investigated the ability to segment familiar sequences of dynamic goal-directed actions into small and large meaningful units in a group of patients with schizophrenia (N = 16) and a group of healthy control subjects (N = 17). While viewing two videotaped movies , participants were requested to detect the transitions between component events at both low and high levels of the action categorical structure. Both groups detected significantly more events under the small-oriented condition as compared to the large-oriented condition. Differently from normal controls, patients recalled the event scenes in a detailed and fragmentary manner and showed considerable difficulties in detecting large action units. Moreover, low performance on action boundary detection significantly correlated with higher levels of disorganisation symptoms in patients with schizophrenia. A defective conceptual organisation of perceptive action knowledge would help to explain the severe everyday difficulties of these patients both in monitoring their own actions and in understanding others intentions. (C) 2004 Elsevier Ireland Ltd. All rights reserved.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 Witnesses and victims of serious crime are often required to construct a facial composite from their memory, a visual likeness of a suspect's face. The traditional method is for them to select individual facial features to build a face, but often these images are of poor quality. We have developed a new method whereby witnesses repeatedly select instances from an array of complete faces and a composite is evolved over time by searching a face model built using Principal Components Analysis. While past research suggests that the new approach is superior, performance is far from ideal. In the current research, face models are built which match a witness's description of a target. It is found that such 'tailored' models promote better quality composites, presumably due to a more effective search, and also that smaller models may be even better. The work has implications for researchers who are using statistical modelling techniques for recognising faces.

 Witnesses and victims of serious crime are often required to construct a facial composite, a visual likeness of a suspect's face. The traditional method is for them to select individual facial features to build a face, but often these images are of poor quality. We have developed a new method whereby witnesses repeatedly select instances from an array of complete faces and a composite is evolved over time by searching a face model built using PCA. While past research suggests that the new approach is superior, performance is far from ideal. In the current research, face models are built which match a witness's description of a target. It is found that such 'tailored' models promote better quality composites, presumably due to a more effective search, and also that smaller models may be even better. The work has implications for researchers who are using statistical modelling techniques for recognising faces.

-----------------------
 A major problem for developmental science is understanding how the cognitive and emotional networks important in carrying out mental processes can be related to individual differences. The last five years have seen major advances in establishing links between alleles of specific genes and the neural networks underlying aspects of attention. These findings have the potential of illuminating important aspects of normal development and its pathologies. We need to learn how genes and experience combine to influence the structure of neural networks and the efficiency with which they are exercised. Methods for addressing these issues are central to progress in the decade ahead.

 Young children's increasing ability to regulate their thoughts, feelings, and behavior is a hallmark of development that is of critical importance to their socialization. Recent advances in neuroimaging and molecular genetics hold promise for drawing together different levels of analysis of the emergence and growth of self-regulation. In this article, we review research relevant to our approach to understanding self-regulation, beginning with an examination, of the temperament construct of Effortful Control (EC). We trace the development of EC and its links to an anatomically defined attentional network and identify genes that may contribute to individual differences in the efficiency of this network. We also report on how intervention may influence a central component of self-regulation, the executive attention. network. Although much more work remains to be done, we believe that the importance of the questions addressed and the recent progress in understanding self-regulation make this a very exciting area of research.

-----------------------
 The link between perception and action allows us to interact fluently with the world. Objects which 'afford' an action elicit a visuomotor response, facilitating compatible responses. In addition, positioning objects to interact with one another appears to facilitate grouping, indicated by patients with extinction being better able to identify interacting objects (e.g. a corkscrew going towards the top of a wine bottle) than the same objects when positioned incorrectly for action (Riddoch, Humphreys, Edwards, Baker, &amp; Willson, Nature Neuroscience, 6, 82-89, 2003). Here, we investigate the effect of action relations on the perception of normal participants. We found improved identification of briefly-presented objects when in correct versus incorrect co-locations for action. For the object that would be 'active' in the interaction (the corkscrew), this improvement was enhanced when it was oriented for use by the viewer's dominant hand. In contrast, the position-related benefit for the 'passive' object was stronger when the objects formed an action-related pair (corkscrew and bottle) compared with an unrelated pair (corkscrew and candle), and it was reduced when spatial cues disrupted grouping between the objects. We propose that these results indicate two separate effects of action relations on normal perception: a visuomotor response to objects which strongly afford an action; and a grouping effect between objects which form action-related pairs.

 Configural coding is known to take place between the parts of individual objects but has never been shown between separate objects. We provide novel evidence here for configural coding between separate objects through a study of the effects of action relations between objects on extinction. Patients showing visual extinction were presented with pairs of objects that were or were not co-located for action. We first confirmed the reduced extinction effect for objects co-located for action. Consistent with prior results showing that inversion disrupts configural coding, we found that inversion disrupted the benefit for action-related object pairs. This occurred both for objects with a standard canonical orientation (e.g., teapot and teacup) and those without, but where grasping and using the objects was made more difficult by inversion (e.g., spanner and nut). The data suggest that part of the affordance effect may reflect a visuo-motor response to the configural relations between stimuli. Experiment 2 showed that distorting the relative sizes of the objects also reduced the advantage for action-related pairs. We conclude that action-related pairs are processed as configurations.

-----------------------
 The timing of repetitive movements was assessed in a callosotomy patient under unimanual and bimanual conditions. Similar to neurologically healthy individuals, the patient exhibited strong temporal coupling in the bimanual condition. Moreover, for both the left and right hands, within-hand temporal variability was reduced in the bimanual condition compared to the unimanual conditions. This bimanual advantage is hypothesized to reflect the temporal integration of separable timing signals, one associated with the left hand and one associated with the right hand (Helmuth, L. L., &amp; Ivry, R. B. (1996). When two hands are better than one: Reduced timing variability during bimanual movements. Journal of Experimental Psychology. Human Perception and Performance, 2, 278-293). The fact that it persists following callosotomy is inconsistent with models that attribute bimanual coordination in these patients to the control of a single hemisphere. Rather, the results suggest that motor commands from the two hemispheres are integrated subcortically. (C) 1999 Elsevier Science B.V. All rights reserved.

 Rhythmic bimanual movements are highly constrained in the temporal domain, with the gestures of the two hands tightly synchronized. Previous studies have implicated a subcortical locus for temporal coupling based on the observation that these constraints persist in callosotomy patients. We now report that such coupling is restricted to movements entailing a discrete event (such as a movement onset). Three callosotomy patients exhibited a striking lack of temporal coupling during continuous movements, with the two hands oscillating at non-identical frequencies. We propose a subcortical locus of temporal coupling for movements involving discrete events. In contrast, synchronization between the hands during continuous movements depends on interhemispheric transmission across the corpus callosum.

-----------------------
 Recent experiments have shown that placing dissimilar items on lists of phonologically similar items enhances accuracy of ordered recall of the dissimilar items [Farrell, S., &amp; Lewandowsky, S. (2003). Dissimilar items benefit from phonological similarity in serial recall. Journal of Experimental Psychology: Learning, Memory, and Cognition, 29, 838-849.]. Two explanations have been offered for this effect: an encoding explanation, in which items similar to current memory contents are given less encoding weight and offer less competition for recall; and a retrieval explanation, which suggests that the long-term similarity structure of the items leads to dissimilar items being more distinct on mixed lists. These theories are compared in an experiment in which a filled delay was introduced between study and test. Simulations show the prominent enhancing effects of similarity after a delay are captured by a model that assumes encoding is sensitive to the similarity of items to other list items [Farrell, S., &amp; Lewandowsky, S. (2002). An endogenous distributed model of ordering in serial recall. Psychonomic Bulletin &amp; Review, 9, 59-79.], but are not handled by a retrieval model [the Start-End Model; Henson, R. N. A. (1998). Short-term memory for serial order: the Start-End Model. Cognitive Psychology, 36, 73-137.]. (c) 2006 Elsevier Inc. All rights reserved.

 In short-term serial recall, similar-sounding items are remembered more poorly than items that do not sound alike. When lists mix similar and dissimilar items, performance on the dissimilar items is of considerable theoretical interest. Farrell and Lewandowsky [Farrell, S., &amp; Lewandowsky, S. (2003). Dissimilar items benefit from phonological similarity in serial recall. Journal of Experimental Psychology: Learning, Memory, and Cognition, 29, 838-849.] recently showed that if guessing strategies are controlled, dissimilar items on mixed lists are recalled more accurately than on pure dissimilar lists, a finding that challenges several current theories of serial recall. This article presents two experiments that extend the generality of the mixed-list advantage for dissimilar items and then applies three theories of memory-the primacy model, SIMPLE, and SOB-to the data. The simulations show that the data are best explained by the SOB theory [Farrell, S. (2006). Mixed-list phonological similarity effects in delayed serial recall. Journal of Memory and Language, 55, 587-600; Farrell, S., &amp; Lewandowsky, S. (2002). An endogenous distributed model of ordering in serial recall. Psychonomic Bulletin &amp; Review, 9, 59-79.] which, unlike most other current theories, posits that similarity has an effect at the time of encoding. (C) 2007 Elsevier Inc. All rights reserved.

-----------------------
 The authors evaluated 4 sequential sampling models for 2-choice decisions-the Wiener diffusion, Ornstein-Uhlenbeck (OU) diffusion, accumulator, and Poisson counter models-by fitting them to the response time (RT) distributions and accuracy data from 3 experiments. Each of the models was augmented with assumptions of variability across trials in the rate of accumulation of evidence from stimuli, the values of response criteria, and the value of base RT across trials. Although there was substantial model mimicry, empirical conditions were identified under which the models make discriminably different predictions. The best accounts of the data were provided by the Wiener diffusion model, the OU model with small-to-moderate decay, and the accumulator model with long-tailed (exponential) distributions of criteria, although the last was unable to produce error RTs shorter than correct RTs. The relationship between these models and 3 recent, neurally inspired models was also examined.

 In this letter, we examine the computational mechanisms of reinforcement-based decision making. We bridge the gap across multiple levels of analysis, from neural models of corticostriatal circuits-the basal ganglia (BG) model (Frank, 2005, 2006) to simpler but mathematically tractable diffusion models of two-choice decision making. Specifically, we generated simulated data from the BG model and fit the diffusion model (Ratcliff, 1978) to it. The standard diffusion model fits underestimated response times under conditions of high response and reinforcement conflict. Follow-up fits showed good fits to the data both by increasing nondecision time and by raising decision thresholds as a function of conflict and by allowing this threshold to collapse with time. This profile captures the role and dynamics of the subthalamic nucleus in BG circuitry, and as such, parametric modulations of projection strengths from this nucleus were associated with parametric increases in decision boundary and its modulation by conflict. We then present data from a human reinforcement learning experiment involving decisions with low-and high-reinforcement conflict. Again, the standard model failed to fit the data, but we found that two variants similar to those that fit the BG model data fit the experimental data, thereby providing a convergence of theoretical accounts of complex interactive decision-making mechanisms consistent with available data. This work also demonstrates how to make modest modifications to diffusion models to summarize core computations of the BG model. The result is a better fit and understanding of reinforcement-based choice data than that which would have occurred with either model alone.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Eye movements were monitored as subjects read sentences containing high- or low-predictable target words. The extent to which target words were predictable from prior context was varied: Half of the target words were predictable, and the other half were unpredictable. In addition, the length of the target word varied: The target words were short (4-6 letters), medium (7-9 letters), or long (10-12 letters). Length and predictability both yielded strong effects on the probability of skipping the target words and on the amount of time readers fixated the target words (when they were not skipped). However, there was no interaction in any of the measures examined for either skipping or fixation time. The results demonstrate that word predictability (due to contextual constraint) and word length have strong and independent influences on word skipping and fixation durations. Furthermore, because the long words extended beyond the word identification span, the data indicate that skipping can occur on the basis of partial information in relation to word identity.

 We examined the initial landing position of the eyes in target words that were either predictable or unpredictable from the preceding sentence context. Although readers skipped over predictable words more than unpredictable words and spent less time on predictable words when they did fixate on them, there was no difference in the launch site of the saccade to the target word. Moreover, there was only a very small difference in the initial landing position on the target word as a function of predictability when the target words were fixated which is most parsimoniously explained by positing that a few programmed skips of the target word fell short of their intended target. These results suggest: that low-level processing is primarily responsible for landing position effects in reading. (C) 2001 Elsevier Science Ltd. All rights reserved.

-----------------------
 We examined target selection for visually guided reaching movements in visual search, in which participants reached to an odd-colored target presented with two homogenous distractors. The colors of the target and distractors were randomly switched for each trial between red and green, and the location of the target was varied. Therefore either color could be a distractor or target, and the identity was resolved by grouping two distractors having the same color. Thus, there was ongoing competition between a target and distractors. In some trials, reaches were directed to the target, and in other trials, reaches were initially directed towards a distractor and corrected in mid-flight, showing highly curved trajectories. Interestingly, trials with highly curved trajectories were no less efficient in terms of accuracy or total time. The extra time taken up in movement duration was offset by shorter initial latencies. By analyzing curved trajectories, we demonstrated that corrective movements occur shortly after the onset of initial movement, suggesting that a corrective new target is selected even before initial movement is executed. This provides an explanation as to why misdirected reaches, hastily initiated, can be corrected with minimal loss in overall efficiency. In addition, our results show that the details of movement trajectories allow us to visualize the dynamics of target selection as they unfold in time. (c) 2007 Elsevier Ltd. All rights reserved.

 We investigated whether the N2pc is unequivocally linked to distractor-suppression mechanisms, as is commonly assumed. According to the distractor-suppression account of the N2pc, no suppression, and thus no N2pc, should occur when homogeneous distractors help in selecting the target, such as when the target feature is unpredictable. Participants performed a simple detection or a finer discrimination on a singleton target, which had either a variable or a constant color. Contrary to the distractor-suppression account, an N2pc was present for both the variable and the constant conditions, and for both tasks. Additionally, target feature consistency correlated with earlier N2pc onsets relative to variable blocks. Both results indicate that the N2pc is not unequivocally linked to distractor-suppression mechanisms, but may index mechanisms involved in identifying and localizing relevant stimuli through enhancement of their features.

-----------------------
 An experiment with a multiple-cue judgment task tested the hypothesis that humans can only abstract explicit representations of cue-criterion relations when the cues are related to the criterion by an additive function. It is proposed that the sequential and capacity-constrained nature of controlled, explicit thought can only induce and execute linear additive cue integration; non-additive environments require exemplar memory. The results showed that an additive task induced processes of cue abstraction and cue integration, while a multiplicative task induced exemplar processes. The results suggest flexible interplay between distinct representation-levels, a preference to abstract explicit "rules" whenever possible, although this capacity is constrained to additive cue-criterion relations.

 The majority of previous studies on multiple-cue judgment with continuous cues have involved comparisons between judgments and multiple linear regression models that integrated cues into a judgment. The authors present an experiment indicating that in a judgment task with additive combination of multiple continuous cues, people indeed displayed abstract knowledge of the cue criterion relations that was mentally integrated into a judgment, but in a task with multiplicative combination of continuous cues, people instead relied on retrieval of memory traces of similar judgment cases (exemplars). These results suggest that people may adopt qualitatively distinct forms of knowledge, depending on the structure of a multiple-cue judgment task. The authors discuss implications for theories of multiple-cue judgment.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 Dual-process theory, which holds that recognition decisions can be based on recollection or familiarity, has long seemed incompatible with signal detection theory, which holds that recognition decisions are based on a singular, continuous memory-strength variable. Formal dual-process models typically regard familiarity as a continuous process (i.e., familiarity comes in degrees), but they construe recollection as a categorical process (i.e., recollection either occurs or does not occur). A continuous process is characterized by a graded relationship between confidence and accuracy, whereas a categorical process is characterized by a binary relationship such that high confidence is associated with high accuracy but all lower degrees of confidence are associated with chance accuracy. Using a source-memory procedure, we found that the relationship between confidence and source-recollection accuracy was graded. Because recollection, like familiarity, is a continuous process, dual-process theory is more compatible with signal detection theory than previously thought.

 The dual-process theory of recognition memory holds that recognition decisions can be based on recollection or familiarity, and the remember/know procedure is widely used to investigate those 2 processes. Dual-process theory in general and the remember/know procedure in particular have been challenged by an alternative strength-based interpretation based on signal-detection theory, which holds that remember judgments simply reflect stronger memories than do know judgments. Although supported by a considerable body of research, the signal-detection account is difficult to reconcile with G. Mandler's (1980) classic "butcher-on-the-bus" phenomenon (i.e., strong. familiarity-based recognition). In this article, a new signal-detection model is proposed that does not deny either the validity of dual-process theory or the possibility that remember/know judgments can when used in the right way help to distinguish between memories that are largely recollection based from those that are largely familiarity based. It does, however, agree with all prior signal-detection-based critiques of the remember/know procedure, which hold that, as it is ordinarily used, the procedure mainly distinguishes strong memories from weak memories (not recollection from familiarity).

-----------------------
 Dual-process theory, which holds that recognition decisions can be based on recollection or familiarity, has long seemed incompatible with signal detection theory, which holds that recognition decisions are based on a singular, continuous memory-strength variable. Formal dual-process models typically regard familiarity as a continuous process (i.e., familiarity comes in degrees), but they construe recollection as a categorical process (i.e., recollection either occurs or does not occur). A continuous process is characterized by a graded relationship between confidence and accuracy, whereas a categorical process is characterized by a binary relationship such that high confidence is associated with high accuracy but all lower degrees of confidence are associated with chance accuracy. Using a source-memory procedure, we found that the relationship between confidence and source-recollection accuracy was graded. Because recollection, like familiarity, is a continuous process, dual-process theory is more compatible with signal detection theory than previously thought.

 The dual-process theory of recognition memory holds that recognition decisions can be based on recollection or familiarity, and the remember/know procedure is widely used to investigate those 2 processes. Dual-process theory in general and the remember/know procedure in particular have been challenged by an alternative strength-based interpretation based on signal-detection theory, which holds that remember judgments simply reflect stronger memories than do know judgments. Although supported by a considerable body of research, the signal-detection account is difficult to reconcile with G. Mandler's (1980) classic "butcher-on-the-bus" phenomenon (i.e., strong. familiarity-based recognition). In this article, a new signal-detection model is proposed that does not deny either the validity of dual-process theory or the possibility that remember/know judgments can when used in the right way help to distinguish between memories that are largely recollection based from those that are largely familiarity based. It does, however, agree with all prior signal-detection-based critiques of the remember/know procedure, which hold that, as it is ordinarily used, the procedure mainly distinguishes strong memories from weak memories (not recollection from familiarity).

-----------------------
 This paper considers the nature of second language dialogues, involving at least one non-native (L2) speaker. We assume that dialogue is characterised by a process in which interlocutors develop similar mental states to each other (Pickering Garrod, 2004). We first consider various means in which interlocutors align their mental states, and suggest why such alignment may be different in second language dialogues from dialogues involving native (L1) speakers. Specifically, we consider alignment in L2 speakers conversing with L1 speakers, L1 speakers conversing with L2 speakers, and L2 speakers conversing with each other, and sketch a range of experimental predictions.

 We present an overview of recent research conducted in the field of language production based on papers presented at the first edition of the International Workshop on Language Production (Marseille, France, September 2004). This article comprises two main parts. In the first part, consisting of three sections, we review the articles that are included in this Special Issue. These three sections deal with three different topics of general interest for models of language production: (A) the general organisational principles of the language production system, (B) several aspects of the lexical selection process and (C) the representations and processes used during syntactic encoding. In the second part, we discuss future directions for research in the field of language production, given the considerable developments that have occurred in recent years.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 This paper used self-paced reading to test processing preferences in pronoun interpretation in English two clause sentences. The results demonstrate that people's preferences can be reversed by changing the coherence relation between the clauses. The results are not compatible with the existence of a single all-purpose strategy in pronoun resolution. Rather, the results support Kehler's (2002) hypothesis that the processing patterns observed in pronoun processing are a byproduct of more general cognitive inference processes underlying the establishment of coherence, such that discourse coherence guides pronoun reference, and pronoun reference guides discourse coherence.

 The present study compares the processing of unambiguous restrictive and non-restrictive relative clauses (RCs) within both a null context and a supportive discourse using a self-paced reading methodology. Individuals read restrictive RCs more slowly than non-restrictive RCs in a null context, but processed restrictive RCs faster than non-restrictive RCs in supportive context, resulting in an interaction between context and RC type. These results provide evidence for two theoretical points. First, principles analogous to those in referential theory [Altmann G. T. M., T Steedman, M. (1988). Interaction with context during human sentence processing. Cognition, 30, 191-238; Crain, S., T Steedman, M. (1985). On not being led up the garden path: The use of context by the psychological parser. In D. Dowty, L. Karttunnen, A. Zwicky (Eds.), Natural language parsing. Cambridge, UK: Cambridge University Press] apply not only in resolving ambiguity but also in processing unambiguous sentences. Second, the discourse context can guide and facilitate interpretive processing. This result suggests that intrasentential factors such as syntax are not autonomous from contextual processing, contrary to the modularity hypothesis [Fodor, J. A. (1983). Modularity of mind. Cambridge, MA: MIT Press]. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 Children at risk for familial dyslexia (n = 107) and their controls (n = 93) have been followed from birth to school entry in the Jyvaskyla Longitudinal study of Dyslexia (JLD) on developmental factors linked to reading and dyslexia. At the point of school entry, the majority of the at-risk children displayed decoding ability that fell at least 1 SD below the mean of the control group. Measures of speech processing were the earliest indices to show both group differences in infancy and also significant predictive associations with reading acquisition. A number of measures of language, including phonological and morphological skill collected repeatedly from age three, revealed group differences and predictive correlations. Both the group differences and the predictive associations to later language and reading ability strengthened as a function of increasing age. The predictions, however, tend to be stronger and the spectrum of significant correlations wider in the at-risk group. These results are crucial to early identification and intervention of dyslexia in at-risk children.

 We review the main findings of the Jyvaskyla Longitudinal study of Dyslexia (JLD) which follows the development of children at familial risk for dyslexia (N=107) and their controls (N=93). We will illustrate the development of these two groups of children at ages from birth to school entry in the skill domains that have been connected to reading and reading disability in the prior literature. At school entry, the highest score on the decoding task among the poorer half (median) of the at risk children-i.e. of those presumably being most likely genetically affected-is 1 SD below the mean of the control group. Thus, the familial risk for dyslexia shows expected consequences. Among the earliest measures in which group differences as well as significant predictive associations with the first steps in reading have emerged, are indices of speech processing in infancy. Likewise, various measures of early language including pronunciation accuracy, phonological, and morphological skills (but not performance IQ) show both group differences and predictive correlations, the majority of which become stronger as the reliability of the measures increases by age. Predictive relationships tend to be strong in general but higher in the at risk group because of its larger variance in both the predictor variables and in the dependent measures, such as early acquisition of reading. The results are thus promising in increasing our understanding needed for early identification and prevention of dyslexia. Copyright (C) 2004 John Wiley Sons, Ltd.

-----------------------
 We explore the relationships between basic auditory processing, phonological awareness, vocabulary, and word reading in a sample of 95 children, 55 typically developing children, and 40 children with low IQ. All children received nonspeech auditory processing tasks, phonological processing and literacy measures, and a receptive vocabulary task. Compared to age-matched controls, the children with low IQ and low reading skills were significantly impaired in auditory and phonological processing, whereas the children with low IQ and preserved reading skills were not. There were also significant predictive relations between auditory processing and single word reading. Poor auditory processing was not dependent on low IQ, as auditory processing was age appropriate in the low-IQ children who were good readers.

 In the present study, we investigated the relationship between general auditory processing, Chinese tone processing, English phonemic processing and English reading skill in a group of Chinese-English bilingual children with a tonal L1 and Korean-English counterparts with a non-tonal L1. We found that general auditory processing contributed to variance in English word reading skill for Chinese children after controlling for Chinese tone and English phoneme deletion skill. English phonemic processing, on the other hand, explained a significant amount of unique variance in English reading for Korean children after controlling for general auditory and Chinese tone skill. These findings suggest that Chinese children relied more on general auditory processing in reading English, whereas Korean children relied more on phonemic skill in reading English. These findings are discussed in terms of the impact of cross-language differences in bilingual reading acquisition.

-----------------------
 IN an effort to define human cortical gustatory areas we reviewed functional neuroimaging data for which coordinates standardized in Talairach proportional space were available. We observed a wide distribution of peaks within the insula and parietal and frontal opercula, suggesting multiple gustatory regions within this cortical area. Multiple peaks also emerged in the orbitofrontal cortex. However, only two peaks, both in the right hemisphere, were observed in the caudolateral orbitofrontal cortex, the region likely homologous to the secondary taste area described in monkeys. Overall significantly more peaks originated from the right hemisphere suggesting asymmetrical cortical representation of taste favoring the right hemisphere. (C) 1999 Lippincott Williams &amp; Wilkins.

 The neural basis of human pitch perception is not fully understood. It has been argued that the auditory cortices in the two hemispheres are specialized, such that certain right auditory cortical regions have a relatively finer resolution in the frequency domain than homologous regions in the left auditory cortex, but this concept has not been tested directly. Here, we used functional magnetic resonance imaging (fMRI) to test this specific prediction. Healthy volunteers were scanned while passively listening to pure-tone melodic-like sequences in which the pitch distance between consecutive tones was varied in a parametric fashion. As predicted, brain activation in a region of right lateral auditory cortex, corresponding to the planum temporale, was linearly responsive to increasing, pitch distance, even across the fine changes in pitch. In contrast, the BOLD signal at the homologous left cortical region was relatively constant as a function of pitch distance, except at the largest pitch change. The results support the model of relative hemispheric specialization and indicate that the right secondary auditory cortex has a finer pitch resolution than the left. (C) 2007 Elsevier Ltd. All rights reserved.

-----------------------
 Objective: This study examined implicit sequence learning in patients with obsessive-compulsive disorder (OCD) under dual-task conditions. Frontal-striatal networks support implicit learning and are implicated in the pathophysiology of OCD. Neuroimaging data suggest that during implicit learning, OCD patients use neural systems normally active during explicit learning to compensate for striatal dysfunction.

 alpha-Synuclein (SNCA) plays an important role in the regulation of dopaminergic neurotransmission and neurodegeneration in Parkinson disease. We investigated reward and punishment learning in asymptomatic carriers of a rare SNCA gene duplication who were healthy siblings of patients with Parkinson disease. Results revealed that healthy SNCA duplication carriers displayed impaired reward and intact punishment learning compared with noncarriers. These results demonstrate that a copy number variation of the SNCA gene is associated with selective impairments on reinforcement learning in asymptomatic carriers without the motor symptoms of Parkinson disease.

-----------------------
 The segregation of sensory information into distinct cortical areas is an important organizational feature of mammalian sensory systems. Here, we provide functional magnetic resonance imaging (fMRI) evidence for the functional delineation of somatosensory representations in the human central sulcus region. Data were collected with a 3-Tesla scanner during two stimulation protocols, a punctate tactile condition without a kinesthetic/motor component, and a kinesthetic/motor condition without a punctate tactile component. With three-dimensional (3-D) anatomical reconstruction techniques, we analyzed data in individual subjects, using the pattern of activation and the anatomical position of specific cortical areas to guide the analysis. As a complimentary analysis, we used a brain averaging technique that emphasized the similarity of cortical features in the morphing of individual subjects and thereby minimized the distortion of the location of cortical activation sites across individuals. A primary finding of this study was differential activation of the cortex on the fundus of the central sulcus, the position of area 3a, during the two tasks. Punctate tactile stimulation of the palm, administered at 3 Hz with a 5.88(log10.mg) von Frey filament, activated discrete regions within the precentral (PreCG) and postcentral (PoCG) gyri, corresponding to areas 6, 3b, 1, and 2, but did not activate area 3a. Conversely, kinesthetic/motor stimulation, 3-Hz flexion and extension of the digits, activated area 3a, the PreCG (areas 6 and 4), and the PoCG (areas 3b, 1, and 2). These activation patterns were observed in individual subjects and in the averaged data, providing strong evidence for the existence of a distinct representation within area 3a in humans. The percentage signal changes in the PreCG and PoCG regions activated by tactile stimulation, and in the intervening gap region, support this functional dissociation. In addition to this distinction within the fundus of the central sulcus, the combination of high-resolution imaging and 3-D analysis techniques permitted localization of activation within areas 6, 4, 3a, 3b, 1, and 2 in the human. With the exception of area 4, which showed inconsistent activation during punctate tactile stimulation, activation in these areas in the human consistently paralleled the pattern of activity observed in previous studies of monkey cortex.

 To test the hypothesis that cortical remapping supports phantom sensations, we examined referred phantom sensations and cortical activation in humans after spinal-cord injury (SCI) at the thoracic level (T3-T12). Of 12 SCI subjects, 9 reported phantom sensations, and 2 reported referred phantom sensations. in both of these subjects, referred phantom sensations were evoked by contact in reference zones (RZ) that were not adjacent in the periphery and were not predicted to be adjacent in the postcentral gyrus (PoCG), suggesting that representations separated by centimeters of cortical space were simultaneously engaged. This finding was supported by functional MRI (fMRI). In a subject with a T6-level complete SCI, contact in RZ on the left or right forearm projected referred phantom sensations to the ipsilateral chest. During fMRI, contact in either forearm RZ evoked activity in the central PoCG (the position of the forearm representation) and the medial PoCG (the position of the chest representation) with greater than or equal to1.6 cm of nonresponsive cortex intervening. In contrast, stimulation in non-RZ forearm and palm regions in this subject and in lesion-matched SCI subjects evoked central but not medial PoCG activation. Our findings support a relation between PoCG activation and the percept of referred phantom sensations. These results, however, present an alternative to somatotopic cortical reorganization, namely, cortical plasticity expressed in coactivation of nonadjacent representations. The observed pattern suggests that somatotopic: subcortical remapping, projected to the cortex, can support perceptual and cortical reorganization after deafferentation in humans.

-----------------------
 Long-term cocaine consumption is associated with brain structural and functional changes. While the animal literature on cocaine use and dependence has traditionally focused on the striatum, previous human studies using voxel-based morphometry have reported reduced volumes of gray matter in several brain areas, but not in the striatum. Brain magnetic resonance imaging was performed with 20 cocaine-dependent patients and 16 healthy age-, education- and intelligence-matched control men. The cocaine-dependent group had lower gray matter volumes in the striatum and right supramarginal gyrus compared to controls. Within the cocaine-dependent group, years of cocaine use were inversely associated with the volume of the bilateral middle frontal gyms, left superior frontal gyrus. parahippocampus, posterior cingulate. amygdala, insula. right middle temporal gyrus and cerebellum. These results show that cocaine dependence is associated with reduced gray matter volumes in the target structures of the dopaminergic system. These findings are the first to suggest reduced gray matter in the striatum by means of voxel-basecl morphometry in human users, thereby linking human results to animal models of addiction. In addition, the relationship between years of use and gray matter volumes in numerous brain regions are consistent with these volume reductions arising as a consequence of the cocaine use. (C) 2011 Elsevier Inc. All rights reserved.

 Extensive evidence indicates that current and recently abstinent cocaine abusers compared to drug-naive controls have decreased grey matter in regions such as the anterior cingulate, lateral prefrontal and insular cortex. Relatively little is known, however, about the persistence of these deficits in long-term abstinence despite the implications this has for recovery and relapse. Optimized voxel based morphometry was used to assess how local grey matter volume varies with years of drug use and length of abstinence in a cross-sectional study of cocaine users with various durations of abstinence (1-102 weeks) and years of use (0.3-24 years). Lower grey matter volume associated with years of use was observed for several regions including anterior cingulate, inferior frontal gyrus and insular cortex. Conversely, higher grey matter volumes associated with abstinence duration were seen in non-overlapping regions that included the anterior and posterior cingulate, insular, right ventral and left dorsal prefrontal cortex. Grey matter volumes in cocaine dependent individuals crossed those of drug-naive controls after 35 weeks of abstinence, with greater than normal volumes in users with longer abstinence. The brains of abstinent users are characterized by regional grey matter volumes, which on average, exceed drug-naive volumes in those users who have maintained abstinence for more than 35 weeks. The asymmetry between the regions showing alterations with extended years of use and prolonged abstinence suggest that recovery involves distinct neurobiological processes rather than being a reversal of disease-related changes. Specifically, the results suggest that regions critical to behavioral control may be important to prolonged, successful, abstinence.

-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Masked stimuli call cause partial motor activation and prime responses to subsequent stimuli. Under certain conditions, a biphasic pattern appears, such that positive priming precedes a negative phase, which has been interpreted as evidence of an inhibitory mechanism that suppresses the motor activity caused by the prime. In this article, the authors report evidence for a further reversal in priming: In two experiments, the authors found that the negative compatibility effect was followed by a small but repeatable positive priming effect at an interval of approximately 500 ms between prime and target. Thus. masked primes appear to produce a triphasic pattern of priming, which is consistent with the notion that oscillation between facilitation and inhibition may be a fundamental property of the competitive interactions between response alternatives in the motor system.

 In masked priming, responses are often speeded when primes are similar to targets ('positive compatibility effect'). However, sometimes similarity of prime and target impairs responses ('negative compatibility effect'). A similar distinction has been found for the curvature of saccade trajectories. Here, we test whether the same inhibition processes are involved in the two phenomena, by directly comparing response times and saccade curvature within the same masked priming paradigm. Interestingly, we found a dissociation between the directions of masked priming and saccade curvature, which could indicate that multiple types of inhibition are involved in the suppression of unwanted responses. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Memory distortion occurs in the laboratory and in everyday life. This article focuses on false recognition, a common type of memory distortion in which individuals incorrectly claim to have encountered a novel object or event. By considering evidence from neuropsychology, neuroimaging, and electrophysiology, we address three questions. (1) Are there patterns of neural activity that can distinguish between true and false recognition? (2) Which brain regions contribute to false recognition? (3) Which brain regions play a role in monitoring or reducing false recognition? Neuroimaging and electrophysiological studies suggest that sensory activity is greater for true recognition compared to false recognition. Neuropsychological and neuroimaging results indicate that the hippocampus and several cortical regions contribute to false recognition. Evidence from neuropsychology, neuroimaging, and electrophysiology implicates the prefrontal cortex in retrieval monitoring that can limit the rate of false recognition.

 False recognition, a type of memory distortion where one claims to remember something that never happened, can occur in response to items that are similar but not identical to previously seen items (i.e., related false recognition) or in response to novel items (i.e., unrelated false recognition). It is unknown whether these 2 types of memory errors arise from the same or distinct neural substrates. Using functional magnetic resonance imaging, we compared the neural activity associated with true recognition, related false recognition, and unrelated false recognition for abstract shapes. True recognition and related false recognition were associated with similar patterns of neural activity, including activity in the prefrontal cortex, the parietal cortex, and the medial temporal lobe. By contrast, unrelated false recognition was associated with activity in language-processing regions. These results indicate that false recognition is not a unitary phenomenon, but rather can reflect the operation of 2 distinct cognitive and neural processes.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The purpose of the present study was to gain a deeper understanding of the role of the basal ganglia in learning and memory by examining learning strategies among patients with basal ganglia dysfunction. Using a probabilistic category learning task (the "weather prediction" task) previously shown to be sensitive to basal ganglia function, the authors examined patterns of performance during learning and used mathematical models to capture different learning strategies. Results showed that patients with Parkinson's disease exhibit different patterns of strategy use. Specifically, most controls initially used a simple, but suboptimal, strategy that focused on single-cue-outcome associations; eventually, however, most controls adopted a more complex, optimal learning strategy, integrating single-cue associations to predict outcomes for multiple-cue stimuli. In contrast, the majority of individuals with Parkinson's disease continued to rely on simple single-cue learning strategies throughout the experiment.

 Human anterograde amnesia can result float a variety of etiologies, including hypoxic brain injury and anterior communicating artery (ACoA) aneurysm rupture. Although each etiology can cause a similarly severe disruption in declarative memory for verbal and visual material, there may be differences in incrementally acquired. feedback-based learning, as well as generalization. Here, 6 individuals who survived hypoxic brain injury, 7 individuals who survived ACoA aneurysm rupture, and 13 matched controls were tested oil 2 tasks that included a feedback-based learning phase followed by a transfer phase in which familiar information is presented in new ways. In both tasks, the ACoA group was slow on initial learning, but those patients who completed the learning phase went on to transfer as well as controls. In the hypoxic group, 1 patient failed to complete either task: the remaining hypoxic group did not differ from controls during learning of either task, but was impaired on transfer. These results highlight a difference in feedback-based learning in 2 amnesic etiologies, despite similar levels of declarative memory impairment.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 Phosphorylation on the activation loop of AGC kinases is typically mediated by PDK1. The precise mechanism for this in-trans phosphorylation is unknown; however, docking of a hydrophobic (HF) motif in the C-tail of the substrate kinase onto the N-lobe of PDK1 is likely an essential step. Using a peptide array of PKA to identify other PDK1-interacting sites, we discovered a second AGC-conserved motif in the C-tail that interacts with PDK1. Since this motif [FD(X)(1-2)Y/F] lies in the active site tether region and in PKA contributes to ATP binding, we call it the Adenosine binding (Ade) motif. The Ade motif is conserved as a PDK1-interacting site in Akt and PRK2, and we predict it will be a PDK1-interacting site for most AGC kinases. In PKA, the HF motif is only recognized when the turn motif Ser338 is phosphorylated, possibly serving as a phosphorylation "switch'' that regulates how the Ade and HF motifs interact with PDK1. These results demonstrate that the extended AGC C-tail serves as a polyvalent element that trans-regulates PDK1 for catalysis. Modeling of the PKA C-tail onto PDK1 structure creates two chimeric sites; the ATP binding pocket, which is completed by the Ade motif, and the C-helix, which is positioned by the HF motif. Together, they demonstrate substrate-assisted catalysis involving two kinases that have co-evolved as symbiotic partners. The highly regulated turn motifs are the most variable part of the AGC C-tail. Elucidating the highly regulated cis and trans functions of the AGC tail is a significant future challenge.

 Eukaryotic protein kinases (EPKs) feature two coevolved structural segments, the Activation segment, which starts with the Asp-Phe-Gly (DFG) and ends with the Ala-Pro-Glu (APE) motifs, and the helical GHE subdomain that comprises alpha G-alpha H-alpha I helices. Eukaryotic-like kinases have a much shorter Activation segment and lack the GHI subdomain. They thus lack the conserved salt bridge interaction between the APE Glu and an Arg from the GHI subdomain, a hallmark signature of EPKs. Although the conservation of this salt bridge in EPKs is well known and its implication in diseases has been illustrated by polymorphism analysis, its function has not been carefully studied. In this work, we use murine cAMP-dependent protein kinase (protein kinase A) as the model enzyme (Glu208 and Arg280) to examine the role of these two residues. We showed that Ala replacement of either residue caused a 40- to 120-fold decrease in catalytic efficiency of the enzyme due to an increase in K-m(ATP) and a decrease in k(cat). Crystal structures, as well as solution studies, also demonstrate that this ion pair contributes to the hydrophobic network and stability of the enzyme. We show that mutation of either Glu or Arg to Ala renders both mutant proteins less effective substrates for upstream kinase phosphoinositide-dependent kinase 1. We propose that the Glu208 Arg280 pair serves as a center hub of connectivity between these two structurally conserved elements in EPKs. Mutations of either residue disrupt communication not only between the two segments but also within the rest of the molecule, leading to altered catalytic activity and enzyme regulation. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Phosphorylation on the activation loop of AGC kinases is typically mediated by PDK1. The precise mechanism for this in-trans phosphorylation is unknown; however, docking of a hydrophobic (HF) motif in the C-tail of the substrate kinase onto the N-lobe of PDK1 is likely an essential step. Using a peptide array of PKA to identify other PDK1-interacting sites, we discovered a second AGC-conserved motif in the C-tail that interacts with PDK1. Since this motif [FD(X)(1-2)Y/F] lies in the active site tether region and in PKA contributes to ATP binding, we call it the Adenosine binding (Ade) motif. The Ade motif is conserved as a PDK1-interacting site in Akt and PRK2, and we predict it will be a PDK1-interacting site for most AGC kinases. In PKA, the HF motif is only recognized when the turn motif Ser338 is phosphorylated, possibly serving as a phosphorylation "switch'' that regulates how the Ade and HF motifs interact with PDK1. These results demonstrate that the extended AGC C-tail serves as a polyvalent element that trans-regulates PDK1 for catalysis. Modeling of the PKA C-tail onto PDK1 structure creates two chimeric sites; the ATP binding pocket, which is completed by the Ade motif, and the C-helix, which is positioned by the HF motif. Together, they demonstrate substrate-assisted catalysis involving two kinases that have co-evolved as symbiotic partners. The highly regulated turn motifs are the most variable part of the AGC C-tail. Elucidating the highly regulated cis and trans functions of the AGC tail is a significant future challenge.

 Eukaryotic protein kinases (EPKs) feature two coevolved structural segments, the Activation segment, which starts with the Asp-Phe-Gly (DFG) and ends with the Ala-Pro-Glu (APE) motifs, and the helical GHE subdomain that comprises alpha G-alpha H-alpha I helices. Eukaryotic-like kinases have a much shorter Activation segment and lack the GHI subdomain. They thus lack the conserved salt bridge interaction between the APE Glu and an Arg from the GHI subdomain, a hallmark signature of EPKs. Although the conservation of this salt bridge in EPKs is well known and its implication in diseases has been illustrated by polymorphism analysis, its function has not been carefully studied. In this work, we use murine cAMP-dependent protein kinase (protein kinase A) as the model enzyme (Glu208 and Arg280) to examine the role of these two residues. We showed that Ala replacement of either residue caused a 40- to 120-fold decrease in catalytic efficiency of the enzyme due to an increase in K-m(ATP) and a decrease in k(cat). Crystal structures, as well as solution studies, also demonstrate that this ion pair contributes to the hydrophobic network and stability of the enzyme. We show that mutation of either Glu or Arg to Ala renders both mutant proteins less effective substrates for upstream kinase phosphoinositide-dependent kinase 1. We propose that the Glu208 Arg280 pair serves as a center hub of connectivity between these two structurally conserved elements in EPKs. Mutations of either residue disrupt communication not only between the two segments but also within the rest of the molecule, leading to altered catalytic activity and enzyme regulation. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Fixation durations in reading are longer for within-word fixation positions close to word center than for positions near word boundaries. This counterintuitive result was termed the Inverted-Optimal Viewing Position (IOVP) effect. We proposed an explanation of the effect based on error-correction of mislocated fixations [Nuthmann, A., Engbert, R., &amp; Kliegl, R. (2005). Mislocated fixations during reading and the inverted optimal viewing position effect. Vision Research, 45, 2201-2217], that suggests that the IOVP effect is not related to word processing. Here we demonstrate the existence of an IOVP effect in "mindless reading", a G-string scanning task. We compare the results from experimental data with results obtained from computer simulations of a simple model of the IOVP effect and discuss alternative accounts. We conclude that oculornotor errors, which often induce mislocalized fixations, represent the most important source of the IOVP effect. (c) 2006 Elsevier Ltd. All rights reserved.

 The launch-site effect, a systematic variation of within-word landing position as a function of launch-site distance, is among the most important oculomotor phenomena in reading. Here we show that the launch-site effect is strongly modulated in word skipping, a finding which is inconsistent with the view that the launch-site effect is caused by a saccadic-range error. We observe that distributions of landing positions in skipping saccades show an increased leftward shift compared to non-skipping saccades at equal launch-site distances. Using an improved algorithm for the estimation of mislocated fixations, we demonstrate the reliability of our results. (C) 2010 Elsevier Ltd. All rights reserved.

-----------------------
 A search for diphoton events with large missing transverse momentum has been performed using proton-proton collision data at is root s = 7 TeV recorded with the ATLAS detector, corresponding to an integrated luminosity of 4.8 fb(-1). No excess of events was observed above the Standard Model prediction and model-dependent 95% confidence level exclusion limits are set. In the context of a generalised model of gauge-mediated supersymmetry breaking with a bino-like lightest neutralino of mass above 50 GeV, gluinos (squarks) below 1.07 TeV (0.87 TeV) are excluded, while a breaking scale Lambda below 196 TeV is excluded for a minimal model of gauge-mediated supersymmetry breaking. For a specific model with one universal extra dimension, compactification scales 1/R &lt; 1.40 TeV are excluded. These limits provide the most stringent tests of these models to date. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 We propose a neural model of multiattribute-decision processes, based on an attractor neural network with dynamic thresholds. The model may be viewed as a generalization of the elimination by aspects model, whereby simultaneous selection of several aspects is allowed. Depending on the amount of synoptic inhibition, various kinds of scanning strategies may be performed, leading in some cases to vacillations among the alternatives. The model predicts that decisions of a longer time duration exhibit a lower violation of the simple scalability low, as opposed to shorter decisions. Furthermore, the model is suggested as a general attribute-based decision module. Accordingly, various decision strategies are manifested depending on the module's parameters.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 Theories of perceptual transparency have typically been developed within the context of a physical model that generates the percept of transparency (F. Metelli's episcotister model, 1974b). Here 2 fundamental questions are investigated: (a) When does the visual system initiate the percept of one surface seen through another? (b) How does it assign surface properties to a transparent layer? Results reveal systematic deviations from the predictions of Metelli's model, both for initiating image decomposition into multiple surfaces and for assigning surface attributes. Specifically, results demonstrate that the visual system uses Michelson contrast as a critical image variable to initiate percepts of transparency and to assign transmittance to transparent surfaces. Findings are discussed in relation to previous theories of transparency, lightness, brightness, and contrast-contrast.

 A series of experiments was performed to determine how the visual system computes the transmittance of inhomogeneous surfaces and media. Previous work (Anderson, B. L. (1999) Stereoscopic surface perception. Neuron, 26, 919-928; Anderson, B. L. (2003) The role of occlusion in the perception of depth, lightness, and opacity. Psychological Review, 110, 762-784) has suggested that the visual system employs a transmittance anchoring principle in determining when transparency is perceived. This principle states that the visual system interprets the highest contrast region along contours and surfaces as a region in plain view and uses this anchor as a reference point for transparency computations. In particular, recent work has shown that the transmittance of homogeneous transparent surfaces is well described by a ratio of contrasts model (Singh, M., &amp; Anderson, B. L. (2002). Toward a perceptual theory of transparency. Psychological Review, 109, 492-519). In this model, the transmittance of a transparent surface is determined by the contrast of a transparent image region normalized by the contrast of the region in plain view. Here, a series of experiments is reported that assesses this model for inhomogeneous transparent surfaces that vary in both space and time. The results of these experiments reveal that transmittance anchoring has both a spatial and temporal component, and that the perceived transmittance of transparent surfaces is well described by a ratio of perceived contrasts model. (c) 2005 Elsevier Ltd. All rights reserved.

-----------------------
 Theories of perceptual transparency have typically been developed within the context of a physical model that generates the percept of transparency (F. Metelli's episcotister model, 1974b). Here 2 fundamental questions are investigated: (a) When does the visual system initiate the percept of one surface seen through another? (b) How does it assign surface properties to a transparent layer? Results reveal systematic deviations from the predictions of Metelli's model, both for initiating image decomposition into multiple surfaces and for assigning surface attributes. Specifically, results demonstrate that the visual system uses Michelson contrast as a critical image variable to initiate percepts of transparency and to assign transmittance to transparent surfaces. Findings are discussed in relation to previous theories of transparency, lightness, brightness, and contrast-contrast.

 A series of experiments was performed to determine how the visual system computes the transmittance of inhomogeneous surfaces and media. Previous work (Anderson, B. L. (1999) Stereoscopic surface perception. Neuron, 26, 919-928; Anderson, B. L. (2003) The role of occlusion in the perception of depth, lightness, and opacity. Psychological Review, 110, 762-784) has suggested that the visual system employs a transmittance anchoring principle in determining when transparency is perceived. This principle states that the visual system interprets the highest contrast region along contours and surfaces as a region in plain view and uses this anchor as a reference point for transparency computations. In particular, recent work has shown that the transmittance of homogeneous transparent surfaces is well described by a ratio of contrasts model (Singh, M., &amp; Anderson, B. L. (2002). Toward a perceptual theory of transparency. Psychological Review, 109, 492-519). In this model, the transmittance of a transparent surface is determined by the contrast of a transparent image region normalized by the contrast of the region in plain view. Here, a series of experiments is reported that assesses this model for inhomogeneous transparent surfaces that vary in both space and time. The results of these experiments reveal that transmittance anchoring has both a spatial and temporal component, and that the perceived transmittance of transparent surfaces is well described by a ratio of perceived contrasts model. (c) 2005 Elsevier Ltd. All rights reserved.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 Two hypotheses on deductive reasoning are under development: mental logic and mental models. It is often accepted that there are overwhelming arguments to reject the mental logic hypothesis. I revise these arguments and claim that they are either not conclusive, or point at problems which are troublesome for the mental model hypothesis as well.

 We report two studies investigating how naive reasoners evaluate the probability that a conditional assertion is true, and the conditional probability that the consequent of the conditional is true given that the antecedent is true. The mental model theory predicts that individuals should evaluate the probability of a conditional on the basis of the mental models representing the conditional, and that evaluations calling for a greater number of models should be more difficult. It follows that the probability of a conditional should differ from the corresponding conditional probability. The results of the studies corroborated these predictions, and contrast with alternative accounts of naive evaluations of the probability of conditionals.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Using diffusion tensor imaging and tractography, we found that a disruption in structural connectivity in ventral occipitotemporal cortex may be the neurobiological basis for the lifelong impairment in face recognition that is experienced by individuals who suffer from congenital prosopagnosia. Our findings suggest that white-matter fibers in ventral occipitotemporal cortex support the integrated function of a distributed cortical network that subserves normal face processing.

 The summed activity of multiple nodes of a distributed cortical network supports face recognition in humans, including "core" ventral occipitotemporal cortex (VOTC) regions [1-3], and "extended" regions outside VOTC [4, 5]. Many individuals with congenital prosopagnosia-an impairment in face processing [6-9]-exhibit normal blood oxygenation level-dependent (BOLD) activation in the core VOTC regions [10, 11]. These individuals evince a reduction in the structural integrity of the white matter tracts connecting VOTC to anterior temporal and frontal cortices [12], part of the "extended" face network. The impairment in congenital prosopagnosia may arise, therefore, not from a dysfunction of the core VOTC areas but from a failure to propagate signals between the intact VOTC and the extended nodes of the network. Using the fMR adaptation paradigm with famous and unknown faces, we show that individuals with congenital prosopagnosia evince normal adaptation effects in VOTC, indicating sensitivity to facial identity, but show no differential activation for familiar versus unknown faces outside VOTC, particularly in the precuneus/posterior cingulate cortex and the anterior paracingulate cortex. Normal BOLD activation in VOTC is thus insufficient to subserve intact face recognition, and disrupted information propagation between VOTC and the extended face processing network may explain the functional impairment in congenital prosopagnosia.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 The existence of cross-linguistic universals in color naming is currently contested. Early empirical studies, based principally on languages of industrialized societies, suggested that all languages may draw on a universally shared repertoire of color categories. Recent work, in contrast, based on languages from nonindustrialized societies, has suggested that color categories may not be universal. No comprehensive objective tests have yet been conducted to resolve this issue. We conduct such tests on color naming data from languages of both industrialized and nonindustrialized societies and show that strong universal tendencies in color naming exist across both sorts of language.

 It is widely held that named color categories in the world's languages are organized around universal focal colors and that these focal colors tend to be chosen as the best examples of color terms across languages. However, this notion has been supported primarily by data from languages of industrialized societies. In contrast, recent research on a language from a nonindustrialized society has called this idea into question. We examine color-naming data from languages of 110 nonindustrialized societies and show that (i) best-example choices for color terms in these languages cluster near the prototypes for English white, black, red, green, yellow, and blue, and (it) best-example choices cluster more tightly across languages than do the centers of category extensions, suggesting that universal best examples (foci) may be the source of universal tendencies in color naming.

-----------------------
 Peptides based upon the non-prime side residues of the NS4A-4B cleavage site of hepatitis C virus (HCV) NS3-4A proteinase containing an alpha -ketoamide moiety in place of the scissile amide bond are potent inhibitors of this enzyme. (C) 2001 Elsevier Science Ltd. All rights reserved.

 A versatile route for the synthesis of homochiral alpha-ketoamide analogues of amino acids is described. Incorporation of this functionality into peptide sequences using either solution or solid-phase chemistry resulted in potent inhibitors of the Hepatitis C Virus NS3 proteinase, (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 Peptides based upon the non-prime side residues of the NS4A-4B cleavage site of hepatitis C virus (HCV) NS3-4A proteinase containing an alpha -ketoamide moiety in place of the scissile amide bond are potent inhibitors of this enzyme. (C) 2001 Elsevier Science Ltd. All rights reserved.

 A versatile route for the synthesis of homochiral alpha-ketoamide analogues of amino acids is described. Incorporation of this functionality into peptide sequences using either solution or solid-phase chemistry resulted in potent inhibitors of the Hepatitis C Virus NS3 proteinase, (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 Clinical and epidemiologic studies have established that posttraumatic stress disorder (PTSD) is highly comorbid with other mental disorders. However, such studies have largely relied on adults' retrospective reports to ascertain comorbidity. The authors examined the developmental mental health histories of adults with PTSD using data on mental disorders assessed across the first 3 decades of life among members of the longitudinal Dunedin Multidisciplinary Health and Development Study; 100% of those diagnosed with past-year PTSD and 93.5% of those with lifetime PTSD at age 26 had met criteria for another mental disorder between ages 11 and 21. Most other mental disorders had first onsets by age 15. Of new cases of PTSD arising between ages 26 and 32, 96% had a prior mental disorder and 77% had been diagnosed by age 15. These data suggest PTSD almost always develops in the context of other mental disorders. Research on the etiology of PTSD may benefit from taking lifetime developmental patterns of comorbidity into consideration. Juvenile mental-disorder histories may help indicate which individuals are most likely to develop PTSD in populations at high risk of trauma exposure.

 The present study examined the influence of stable personality traits on romantic relationships using longitudinal data on a large, representative sample of young adults. Relationship experiences (quality, conflict, and abuse) showed relatively small mean-level changes over time and significant levels of rank-order stability, even across different relationship partners. Antecedent personality traits (assessed at age 18) predicted relationship experiences at age 26 and change in relationship experiences from age 21 to 26. Conversely, relationship experiences also predicted change in personality. Importantly, these findings generally held across relationship partners, suggesting that some people tend to be generally happy (or unhappy) across relationships, and this is due, in part, to stable individual differences in personality. Discussion focuses on the broader implications of the findings, in particular the need for relationship researchers to consider the importance of personality for why relationships thrive or fail and the need for personality researchers to consider the impact of relationship experiences on intraindividual personality development.

-----------------------
 Two experiments examined the impact of attention on sensorimotor skills. In Experiment 1, experienced golfers putted under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step putting performance. Dual-task condition putting was more accurate. In Experiment 2, right-footed novice and experienced soccer players dribbled through a slalom course under dual-task or skill-focused conditions. When using their dominant right foot, experts again performed better in the dual-task condition. However, when using their less proficient left foot, experts performed better in the skill-focused condition. Novices performed better under skill-focus regardless of foot. Whereas novices and the less-proficient performances of experts benefit from online attentional monitoring of step-by-step performance, high-level skill execution is harmed.

 In two experiments, we examined the attentional mechanisms governing sensorimotor skill execution across levels of expertise, In Experiment 1, novice and expert golfers took a series of putts under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step performance. Novices performed better under skill-focused than under dual-task conditions. Experts showed the opposite pattern. In Experiment 2, novice and expert golfers putted under instructions that emphasized either putting accuracy or speed-the latter intended to reduce the time available to monitor and explicitly adjust execution parameters. Novices putted better under accuracy instructions. Experts were more accurate under speed instructions. In agreement with theories of skill acquisition and automaticity, novice performance is enhanced by conditions that allow for on-line attentional monitoring (e.g., skill-focused or accuracy instructions) in comparison with conditions that prevent explicit attentional control of skill execution (e.g., dud-task or speed constraints). In contrast, the proceduralized skill of experts benefits from environments that limit, rather than encourage, attention to execution.

-----------------------
 Flashing a homogeneous light mask after the presentation of a masked target reduces the deleterious effects of the mask, a phenomenon often called target recovery. Target recovery has been studied using masking paradigms in which a target object is presented in isolation prior to the presentation of a mask, thus capturing attention. In the present study, we examined whether target recovery is possible when a target does not benefit from attentional capture. We hypothesized that target recovery would be eliminated when a target must compete with distractors for perceptual attention. Replicating classic studies, we observed target recovery when pattern and light masks followed an isolated target. However, target recovery was not observed when a light mask followed a masked visual search target. Furthermore, using an attentional-capture paradigm we found that sudden onset search targets were recoverable whereas nononset targets were not. The present findings indicate that attentional capture by a target prior to masking plays a critical role in the subsequent recovery of the target.

 We investigated how saccade target selection by humans and macaque monkeys reacts to unexpected changes of the image. This was explored using double step and search step tasks in which a target, presented alone or as a singleton in a visual search array, steps to a different location on infrequent, random trials. We report that human and macaque monkey performance are qualitatively indistinguishable. Performance is stochastic with the probability of producing a compensated saccade to the final target location decreasing with the delay of the step. Compensated saccades to the final target location are produced with latencies relative to the step that are comparable to or less than the average latency of saccades on trials with no target step. Noncompensated errors to the initial target location are produced with latencies less than the average latency of saccades on trials with no target step. Noncompensated saccades to the initial target location are followed by corrective saccades to the final target location following an intersaccade interval that decreases with the interval between the target step and the initiation of the noncompensated saccade. We show that this pattern of results cannot be accounted for by a race between two stochastically independent processes producing the saccade to the initial target location and another process producing the saccade to the final target location. However, performance can be accounted for by a race between three stochastically independent processes-a GO process producing the saccade to the initial target location, a STOP process interrupting that GO process, and another GO process producing the saccade to the final target location. Furthermore, if the STOP process and second GO process start at the same time, then the model can account for the incidence and latency of mid-flight corrections and rapid corrective saccades. This model provides a computational account of saccade production when the image changes unexpectedly. (c) 2007 Elsevier Ltd. All riehts reserved.

-----------------------
 Short-term memory storage can be divided into separate subsystems for verbal information and visual information(1), and recent studies have begun to delineate the neural substrates of these working-memory systems(2-6). Although the verbal storage system has been well characterized, the storage capacity of visual working memory has not yet been established for simple, suprathreshold features or for conjunctions of features. Here we demonstrate that it is possible to retain information about only four colours or orientations in visual working memory at one time. However, it is also possible to retain both the colour and the orientation of four objects, indicating that visual working memory stores integrated objects rather than individual features. Indeed, objects defined by a conjunction of four features can be retained in working memory just as well as single-feature objects, allowing sixteen individual features to be retained when distributed across four objects. Thus, the capacity of visual working memory must be understood in terms of integrated objects rather than individual features, which places significant constraints on cognitive and neurobiological models of the temporary storage of visual information(7).

 Working memory can be divided into separate subsystems for verbal and visual information. Although the verbal system has been well characterized, the storage capacity of visual working memory has not yet been established for simple features or for conjunctions of features. The authors demonstrate that it is possible to retain information about only 3-4 colors or orientations in visual working memory at one time. Observers are also able to retain both the color and the orientation of 3-4 objects, indicating that visual working memory stores integrated objects rather than individual features. Indeed, objects defined by a conjunction of four features can be retained in working memory just as well as single-feature objects, allowing many individual features to be retained when distributed across a small number of objects. Thus, the capacity of visual working memory must be understood in terms of integrated objects rather than individual features.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 fMRI studies have reported three regions in human ventral visual cortex that respond selectively to faces: the occipital face area (OFA), the fusiform face area (FFA), and a face-selective region in the superior temporal sulcus (fSTS). Here, we asked whether these areas respond to two first-order aspects of the face argued to be important for face perception, face parts (eyes, nose, and mouth), and the T-shaped spatial configuration of these parts. Specifically, we measured the magnitude of response in these areas to stimuli that (i) either contained real face parts, or did not, and (ii) either had veridical face configurations, or did not. The OFA and the fSTS were sensitive only to the presence of real face parts, not to the correct configuration of those parts, whereas the FFA was sensitive to both face parts and face configuration. Further, only in the FFA was the response to configuration and part information correlated across voxels, suggesting that the FFA contains a unified representation that includes both kinds of information. In combination with prior results from fMRI, TMS, MEG, and patient studies, our data illuminate the functional division of labor in the OFA, FFA, and fSTS.

 Neuroimaging studies have identified multiple face-selective regions in human cortex but the functional division of labor between these regions is not yet clear. A central hypothesis, with some empirical support, is that face-selective regions in the superior temporal sulcus (STS) are particularly responsive to dynamic information in faces, whereas the fusiform face area (FFA) computes the static or invariant properties of faces. Here we directly tested this hypothesis by measuring the magnitude of response in each region to both dynamic and static stimuli. Consistent with the hypothesis, we found that the response to movies of faces was not significantly different from the response to static images of faces from these same movies in the right FFA and right occipital face area (OFA). By contrast the face-selective region in the right posterior STS (pSTS) responded nearly three times as strongly to dynamic faces as to static faces, and a face-selective region in the right anterior STS (aSTS) responded to dynamic faces only. Both of these regions also responded more strongly to moving faces than to moving bodies, indicating that they are preferentially engaged in processing dynamic information from faces, not in more general processing of any dynamic social stimuli. The response to dynamic and static faces was not significantly different in a third face-selective region in the posterior continuation of the STS (pcSTS). The strong selectivity of face-selective regions in the pSTS and aSTS, but not the FFA, OFA or pcSTS, for dynamic face information demonstrates a clear functional dissociation between different face-selective regions, and provides further clues into their function. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 Swallowing is a complex physiological process involving voluntary and reflexive motor activity, sensorimotor integration, salivation, and visceral regulation. Despite the numerous processes required for normal deglutition, traditional models of the central control of swallowing only emphasize the involvement of the brainstem and the inferior precentral gyrus (IPCG). However a number of neurological disorders involving other brain regions also cause dysphagia. To determine the brain regions participating in voluntary swallowing, we assayed regional cerebral blood flow (rCBF) with positron emission tomography (PET) while healthy human subjects swallowed, performed lateral tongue movements, or rested with their eyes closed. Voluntary swallowing produced strong rCBF increases within the IPCG bilaterally, the right anterior insula/claustrum, and the left cerebellum. The maxima in these regions differed from those induced by lateral tongue movements. Swallowing also produced rCBF increases in the putamen, thalamus, and several additional cortical areas, but these foci were not as clearly distinguishable from activity arising during tongue movements. These findings indicate that swallowing involves the recruitment of a large-scale distributed neural network that includes the anterior insula and cerebellum. The distributed nature of this network helps to explain why so many neurological conditions produce dysphagia.

 Legal decision-making in criminal contexts includes two essential functions performed by impartial "third parties:" assessing responsibility and determining an appropriate punishment. To explore the neural underpinnings of these processes, we scanned subjects with fMRI while they determined the appropriate punishment for crimes that varied in perpetrator responsibility and crime severity. Activity within regions linked to affective processing (amygdala, medial prefrontal and posterior cingulate cortex) predicted punishment magnitude for a range of criminal scenarios. By contrast, activity in right dorsolateral prefrontal cortex distinguished between scenarios on the basis of criminal responsibility, suggesting that it plays a key role in third-party punishment. The same prefrontal region has previously been shown to be involved in punishing unfair economic behavior in two-party interactions, raising the possibility that the cognitive processes supporting third-party legal decision-making and second-party economic norm enforcement may be supported by a common neural mechanism in human prefrontal cortex.

-----------------------
 Following transcranial magnetic stimulation (TMS) at stimulation strength of 1.5 times the resting motor threshold a silent period (SP) of similar to 180 ms duration can be observed in surface EMG-registrations of tonically activated small hand muscles. This SP is believed to be generated cortically and can be prolonged in stroke patients, but it is nor known whether a prolongation of the SP has arty functional significance. In order to answer the question of whether enhanced cortical inhibition can contribute to pathophysiology of motor dysfunction we studied stroke patients with clearly prolonged SP durations in the first dorsal interosseus muscle (&gt;2 times that of the intact side), but with normal magnetically evoked motor potentials. Sixteen patients out of a cohort of 174 consecutive patients presenting with acute hemiparetic stroke fulfilled the inclusion criteria. Serial TMS investigations were performed for up to 2 years post-stroke. In all patients, the SP duration decreased in parallel with clinical improvement In two patients, intermittent clinical deterioration was accompanied by an increase in the SP duration. In four patients, in addition to a markedly prolonged SP duration, the phenomenon of a complete inability to initiate voluntary muscle activity for several seconds, following TMS, could be observed in a number of trials ('motor arrest'). Detailed clinical analysis revealed that, in addition to hemiparesis, distinct motor disturbances in patients with SP prolongation could be observed. These motor disturbances resembled those of motor neglect and were characterized by motivationally dependent under-utilization of the affected arm, impairment of movement initiation, inability to maintain a constant force level and to scale forces, and impairment of individual finger movements. In 12 of the 16 patients at least one additional behavioural manifestation of neglect was present. We suggest that in stroke patients severe motor dysfunction. may be caused by hyperactivity of cortical inhibitory interneurons rather than by direct lesions of descending motor tracts Cortical hyperinhibition may, in turn, results from damage to any of a number of afferent pathways to the motor cortex which modulate local interneuronal activity.

 Background: Following an ischemic brain lesion, the affected cortex undergoes structural and functional changes that may lead to increased cortical excitability or decreased inhibitory neuronal activity, resulting in the occurrence of poststroke epileptic seizures in 6 to 10% of patients with stroke. Methods: To assess motor cortical excitability, transcranial magnetic stimulation (TMS) was used to determine the silent period (SP) duration in 84 consecutive patients with ischemic stroke. Results: In a subpopulation of six patients (38 to 72 years old) a significant decrease of the SP duration (mean 116 +/- 14 msec) was detected in either the arm or the leg on the affected side as compared to the corresponding unaffected limb (mean 231 +/- 32 msec). This electrophysiologic abnormality was clinically associated with focal motor seizures in five of the six patients, whereas none of the other 76 patients with normal or prolonged SP durations developed seizures or epilepsy. Conclusions: Silent period shortening in this group reflects decreased inhibitory activity that may partly be related to functional or structural impairment of GABAergic interneurons. TMS may be of value for determining patients with stroke at risk for developing poststroke seizures.

-----------------------
 Four experiments examined the discrimination performance of four pigeons in two means-end tasks. The pigeons were required to discriminate between two ribbon/food dish assemblies in a simultaneous discrimination. In the connected condition, the ribbon was attached to an out-of-reach food dish, allowing it to be retrieved by the pigeon pulling it with its beak. In the unconnected condition, the ribbon was unattached to the dish, preventing it from being used to retrieve the food. In experiment 1, the pigeons learned this means-end task in fewer than 160 trials. Experiment 2 established that this discrimination was controlled by the gap between the ribbon and dish, and, to a limited extent, by the ribbon's colour. Experiment 3 showed no transfer of this prior means-end training to a second means-end task using a different physical arrangement of 'connectedness'. Experiment 4 revealed that control of this second task was also related to the perceptual features of the gap between the ribbons. The results indicate that the pigeons did not comprehend the conceptual nature of connectedness across these means-end tasks, but did successfully learn each discrimination based on its relevant perceptual features. (c) 2006 The Association for the Study of Animal Behaviour. Published by Elsevier Ltd. All rights reserved.

 To understand how animals serially organize complex competing behaviors, we tested pigeons in a sequential task-switching procedure. Daily sessions involved two conditional discrimination tasks that were presented in sequence. In Experiment 1, the first half of a session employed a matching-to-sample task, and the second half tested an oddity-from-sample task. Because the same colors were used for both tasks, these tasks could be solved only by employing a modulating sequential cue. The results of the first experiment revealed that the pigeons could learn this task-switching procedure and that an internal clock was the critical modulator between the tasks. In Experiment 2, we tested a three-alternative choice task. By examining the pattern of errors among choices, the results of this experiment revealed that pigeons learned and used different representations of the choice rules for each half of the experiment. This modulation of the pigeons' internal states by time has implications for how animals organize their behavior in different settings and holds clues as to the evolution of the serial organization of behavior.

-----------------------
 Four experiments examined the discrimination performance of four pigeons in two means-end tasks. The pigeons were required to discriminate between two ribbon/food dish assemblies in a simultaneous discrimination. In the connected condition, the ribbon was attached to an out-of-reach food dish, allowing it to be retrieved by the pigeon pulling it with its beak. In the unconnected condition, the ribbon was unattached to the dish, preventing it from being used to retrieve the food. In experiment 1, the pigeons learned this means-end task in fewer than 160 trials. Experiment 2 established that this discrimination was controlled by the gap between the ribbon and dish, and, to a limited extent, by the ribbon's colour. Experiment 3 showed no transfer of this prior means-end training to a second means-end task using a different physical arrangement of 'connectedness'. Experiment 4 revealed that control of this second task was also related to the perceptual features of the gap between the ribbons. The results indicate that the pigeons did not comprehend the conceptual nature of connectedness across these means-end tasks, but did successfully learn each discrimination based on its relevant perceptual features. (c) 2006 The Association for the Study of Animal Behaviour. Published by Elsevier Ltd. All rights reserved.

 To understand how animals serially organize complex competing behaviors, we tested pigeons in a sequential task-switching procedure. Daily sessions involved two conditional discrimination tasks that were presented in sequence. In Experiment 1, the first half of a session employed a matching-to-sample task, and the second half tested an oddity-from-sample task. Because the same colors were used for both tasks, these tasks could be solved only by employing a modulating sequential cue. The results of the first experiment revealed that the pigeons could learn this task-switching procedure and that an internal clock was the critical modulator between the tasks. In Experiment 2, we tested a three-alternative choice task. By examining the pattern of errors among choices, the results of this experiment revealed that pigeons learned and used different representations of the choice rules for each half of the experiment. This modulation of the pigeons' internal states by time has implications for how animals organize their behavior in different settings and holds clues as to the evolution of the serial organization of behavior.

-----------------------
 At least 3 different types of computational model have been shown to account for various facets of both normal and impaired single word reading: (a) the connectionist triangle model, (b) the dual-route cascaded model, and (c) the connectionist dual process model. Major strengths and weaknesses of these models are identified. In the spirit of nested incremental modeling, a new connectionist dual process model (the CDP+ model) is presented. This model builds on the strengths of 2 of the previous models while eliminating their weaknesses. Contrary to the dual-route cascaded model, CDP+ is able to learn and produce graded consistency effects. Contrary to the triangle and the connectionist dual process models, CDP+ accounts for serial effects and has more accurate nonword reading performance. CDP+ also beats all previous models by an order of magnitude when predicting individual item-level variance on large databases. Thus, the authors show that building on existing theories by combining the best features of previous models-a nested modeling strategy that is commonly used in other areas of science but often neglected in psychology-results in better and more powerful computational models.

 This article describes the Dual Route Cascaded (DRC) model, a computational model of visual word recognition and reading aloud. The DRC is a computational realization of the dual-route theory of reading, and is the only computational model of reading that can perform the 2 tasks most commonly used to study reading: lexical decision and reading aloud. For both tasks, the authors show that a wide variety of variables that influence human latencies influence the DRC model's latencies in exactly the same way. The DRC model simulates a number of such effects that other computational models of reading do not, but there appear to be no effects that any other current computational model of reading can simulate but that the DRC model cannot. The authors conclude that the DRC model is the most successful of the existing computational models of reading.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 Objective, To develop a simple score to help assess the presence or absence of infection in critically ill patients using routinely available variables.
 Objective. To determine the incidence of body temperature (BT) alterations in critically ill patients, and their relationship with infection and outcome. Design. Prospective, observational study. Setting. Thirty-one bed, medico-surgical department of intensive care. Patients. Adult patients admitted consecutively to the ICU for at least 24 h, during 6 summer months. Interventions. None. esults. Fever (BTgreater than or equal to38.3degreesC) occurred in 139 (28.2%) patients and hypothermia (BTless than or equal to36degreesC) in 45 (9.1%) patients, at some time during the ICU stay. Fever was present in 52 of 100 (52.0%) infected patients without septic shock, and in 24 of 38 (63.2%) patients with septic shock. Hypothermia occurred in 5 of 100 (5.0%) infected patients without septic shock and in 5 of 38 (13.1%) patients with septic shock. Patients with hypothermia and fever had higher Sequential Organ Failure Assessment (SOFA) scores on admission (6.3+/-3.7 and 6.4+/-3.3 vs 4.6+/-3.2; p&lt;0.01), maximum SOFA scores during ICU stay (7.6+/-5.2 and 8.2+/-4.7 vs 5.4+/-3.8; p&lt;0.01) and mortality rates (33.3 and 35.3% vs 10.3%; p&lt;0.01). The length of stay (LOS) was longer in febrile patients than in hypothermic and normothermic (36degreesC&lt;BT&lt;38.3degreesC) patients [median 6 (1-57) vs 5 (2-28) and 3 (1-33) days, p=0.02 and p=0.01, respectively). Among the septic patients hypothermic patients were older than febrile patients (69+/-9 vs 54+/-7 years, p=0.01). Patients with septic shock had a higher mortality if they were hypothermic than if they were febrile (80 vs 50%, p&lt;0.01). onclusions. Both hypothermia and fever are associated with increased morbidity and mortality rates. Patients with hypothermia have a worse prognosis than those with fever.

-----------------------
 Case-based Reasoning (CBR) began as a theory of human cognition, but has attracted relatively little direct experimental or theoretical investigation in psychology. However, psychologists have developed a range of instance-based theories of cognition and have extensively studied how similarity to past cases can guide categorization of new cases. This paper considers the relation between CBR and psychological research, focussing on similarity in human and artificial case-based reasoning in law. We argue that CBR, psychology and legal theory have complementary contributions to understanding similarity, and describe what each offers. This allows us to establish criteria for assessing existing CBR systems in law and to establish what we consider to be the crucial goals for further research on similarity, both from a psychological and a CBR perspective.

 Normative theories provide essential tools for understanding behaviour, not just for reasoning, judgement, and decision-making, but many other areas of cognition as well; and their utility extends to the development of process theories. Furthermore, the way these tools are used has nothing to do with the is-ought fallacy. There therefore seems no basis for the claim that research would be better off without them.

-----------------------
 A series of experiments assessed priming for single letters and words in a letter-by-letter reader (IH) when primes were displayed briefly (between 100-500msec) and masked. Consistent with previous claims that letter-by-letter readers have difficulties accessing orthographic letter codes, IH failed to show normal cross-case priming for single letters in a naming task (e.g. a/A). Nevertheless, IH showed robust cross-case priming for four-letter words that have few if any perceptual features in common between upper and lower case (e.g. read/READ; the letters r/R, e/E, a/A, and d/D are visually dissimilar in lower/upper case), even at prime durations that failed to support priming for single letters. Furthermore, priming extended to pseudowords (e.g. DEAT), and was highly specific given that no priming was obtained between orthographic neighbours (e.g. face did not prime FACT). Based on this pattern of results, we argue that IH gains relatively normal access to orthographic representations, and that his letter-by-letter reading reflects a partial disconnection between orthographic and phonological representations. Within the context of a disconnection account, we provide an explanation of the paradoxical finding of robust word priming in the absence of single letter priming.

 Five experiments were carried out to test the claim that the modality-specific and modality-nonspecific components of long-term priming are differentially sensitive to word frequency, with the specific component being less affected. In contrast with this claim, specific and nonspecific priming were similarly reduced for high-frequency words in three lexical decision and two perceptual identification experiments. These findings highlight the important role of frequency in modulating priming as well as provide a basic constraint for future theories of priming. In addition, the roles of task and student population in modulating priming are examined.

-----------------------
 In an effort to clarify how deductive reasoning is accomplished, an fMRI study was performed to observe the neural substrates of logical reasoning and mathematical calculation. Participants viewed a problem statement and three premises, and then either a conclusion or a mathematical formula. They had to indicate whether the conclusion followed from the premises, or to solve the mathematical formula. Language areas of the brain (Broca's and Wernicke's area) responded as the premises and the conclusion were read, but solution of the problems was then carried out by non-language areas. Regions in right prefrontal cortex and inferior parietal lobe were more active for reasoning than for calculation, whereas regions in left prefrontal cortex and superior parietal lobe were more active for calculation than for reasoning. In reasoning, only those problems calling for a search for counterexamples to conclusions recruited right frontal pole. These results have important implications for understanding how higher cognition, including deduction, is implemented in the brain. Different sorts of thinking recruit separate neural substrates, and logical reasoning goes beyond linguistic regions of the brain. (C) 2008 Elsevier B.V. All rights reserved.

 Deductive reasoning is traditionally viewed as a unitary process involving either rule-based or visuo-spatial mechanisms. However, there is a disagreement in the neuroimaging literature on whether the data support one alternative over the other. Here we test the hypothesis that discrepancies in the literature result from the reasoning materials themselves. Using functional magnetic resonance imaging, we measure brain activity of participants while they integrate the premises of conditional arguments (primarily Modus Tollens: If P then Q; not-Q) and Relational Syllogisms (i.e., linear arguments of the sort P is to the left of Q; Q is to the left of R). We find that reasoning with Modus Tollens activates the left inferior frontal gyrus to a greater extent than the Relational Syllogisms. In contrast, the Relational Syllogisms engage the right temporo-parieto-occipital junction more than conditional arguments. This suggests that conditional reasoning relies more on so-called syntactic processes than relational reasoning, while relational reasoning may rely on visuo-spatial processes and mental imagery more than conditional reasoning. This investigative approach, together with its results, clarifies some apparently inconsistent findings in this literature by showing that the nature of the logical argument, whether it is relational or conditional, determines which neural system is engaged. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Implicit motor sequence learning refers to an important human ability to acquire new motor skills through the repeated performance of a motor sequence. This learning process is characterized by slow, incremental gains of motor performance. The present fMRI study was developed to better delineate the areas supporting these temporal dynamics of learning. By using the serial color matching paradigm, our study focused on the motor level of sequence learning and tracked the time course of learning-related neural changes. Imaging results showed a significant contribution of the left anterior hippocampus in an early sequence acquisition stage (first scanning session) as well as during a later stage with stabilized learning effects (second scanning session). Hippocampal activation significantly correlated with the behavioral learning process and was affected by a change of the motor sequence. These results suggest a strong involvement of the hippocampus in implicit motor sequence learning. On the other hand, a very extensive and bilateral neural network of parietal, temporal and frontal cortical areas (including SMA, pre-SMA) together with parts of the cerebellum and striatum were found to play a role during random visuo-motor task performance.

 The dynamics of the neural network that underlies learning transitive structures of an ordered sequence remains poorly understood. To address this, in the present study we used fMRI to track the time course of transitive inference learning. The hippocampus and the angular gyrus were each shown to be closely related to the learning trajectory, but differentially so. Hippocampal activity was shown to consistently increase with learning but no correlation was found between performance and hippocampal activation, suggesting a general role for the hippocampus. Left angular gyrus activity was also found to consistently increase with training, but, in addition, correlated significantly with behavioral performance. This suggests an involvement of the angular gyros in learning the ordinal associations between the stimuli. (c) 2007 Elsevier Inc. All rights reserved.

-----------------------
 Artificial face recognition systems typically do not attempt to handle very variable images. By comparison, human perceivers can recognize familiar faces over much more varied conditions. We describe a prototype face representation based on simple image-averaging. We have argued that this forms a good candidate for understanding human face perception. Here we examine the stability of these representations by asking (i) how quickly they converge; and (ii) how resistant they are to contamination due to previous misidentifications. We conclude that face averages provide promising representations for use in artificial recognition.

 The Bruce and Young (1986) framework makes a number of important distinctions between the types of representation needed to recognize a familiar face. Here, we return to these, focussing particularly on face recognition units. We argue that such representations need to incorporate idiosyncratic within-person variability, asking questions such as 'What counts as a picture of Harrison Ford?'. We describe a mechanism for achieving this, and discuss the relation between image variability and episodic face memories, in the context of behavioural and neurophysiological data.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 Connectionist and dynamical systems approaches explain human thought, language and behavior in terms of the emergent consequences of a large number of simple noncognitive processes. We view the entities that serve as the basis for structured probabilistic approaches as abstractions that are occasionally useful but often misleading: they have no real basis in the actual processes that give rise to linguistic and cognitive abilities or to the development of these abilities. Although structured probabilistic approaches can be useful in determining what would be optimal under certain assumptions, we propose that connectionist, dynamical systems, and related approaches, which focus on explaining the mechanisms that give rise to cognition, will be essential in achieving a full understanding of cognition and development.

 Studies of language change have begun to contribute to answering several pressing questions in cognitive sciences, including the origins of human language capacity, the social construction of cognition and the mechanisms underlying culture change in general. Here, we describe recent advances within a new emerging framework for the study of language change, one that models such change as an evolutionary process among competing linguistic variants. We argue that a crucial and unifying element of this framework is the use of probabilistic, data-driven models both to infer change and to compare competing claims about social and cognitive influences on language change.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Certain correspondences between the sound and meaning of words can be observed in subsets of the vocabulary. These sound-symbolic relationships have been suggested to result in easier language acquisition, but previous studies have explicitly tested effects of sound symbolism on learning category distinctions but not on word learning. In 2 word learning experiments, we varied the extent to which phonological properties related to a rounded-angular shape distinction and we distinguished learning of categories from learning of individual words. We found that sound symbolism resulted in an advantage for learning categories of sound-shape mappings but did not assist in learning individual word meanings. These results are consistent with the limited presence of sound symbolism in natural language. The results also provide a reinterpretation of the role of sound symbolism in language learning and language origins and a greater specification of the conditions under which sound symbolism proves advantageous for learning.

 Recent research has demonstrated that systematic mappings between phonological word forms and their meanings can facilitate language learning (e.g., in the form of sound symbolism or cues to grammatical categories). Yet, paradoxically from a learning viewpoint, most words have an arbitrary form-meaning mapping. We hypothesized that this paradox may reflect a division of labor between 2 different language learning functions: arbitrariness facilitates learning specific word meanings and systematicity facilitates learning to group words into categories. In a series of computational investigations and artificial language learning studies, we varied the extent to which the language was arbitrary or systematic. For both the simulations and the behavioral studies, we found that the optimal structure of the vocabulary for learning incorporated this division of labor. Corpus analyses of English and French indicate that these predicted patterns are also found in natural languages.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 This article reviews the rationale for using accumulative one-step-ahead prediction error (APE) as a data-driven method for model selection. Theoretically, APE is closely related to Bayesian model selection and the method of minimum description length (MDL). The sole requirement for using APE is that the models under consideration are capable of generating a prediction for the next, unseen data point. This means that APE may be readily applied to selection problems involving very complex models. APE automatically takes the functional form of parameters into account, and the 'plug-in' version of APE does not require the specification of priors. APE is particularly easy to compute for data that have a natural ordering, such as time series. Here, we explore the possibility of using APE to discriminate the short-range ARMA(1, 1) model from the long-range ARFIMA(0,d, 0) model. We also illustrate how APE may be used for model meta-selection, allowing one to choose between different model selection methods. (c) 2006 Elsevier Inc. All rights reserved.

 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

 In this rejoinder we address two of Ratcliff's main concerns with respect to the EZ-diffusion model (Ratcliff, 2008). First, we introduce "robust-EZ," a mixture model approach to achieve robustness against the presence of response contaminants that might otherwise distort parameter estimates. Second, we discuss an extension of the EZ model that allows the estimation of starting point as an additional parameter. Together with recently developed, user-friendly software programs for fitting the full diffusion model (Vandekerckhove &amp; Tuerlinckx, 2007, Voss &amp; Voss, 2007), the development of the EZ model and its extensions is part of a larger effort to make diffusion model analyses accessible to a broader audience, an effort that is long overdue.

-----------------------
 If A caused B and B caused C, did A caused C? Although causality is generally regarded as transitive, some philosophers have questioned this assumption, and models of causality in artificial intelligence are often agnostic with respect to transitivity: They define causation, then check whether the definition makes all, or only some, causal arguments transitive. We consider two formal models of observation-based causation, which differ in the way they represent uncertainty. The quantitative model uses a standard probabilistic definition; the qualitative model uses a definition based on nonmonotonic consequence. The two models identify different sufficient conditions for the transitivity of causation: The Markov condition on events for the quantitative model, and a Saliency condition (if B is true then generally A is true) for the qualitative model. We explore the formal relations between these sufficient conditions, and between the underlying definitions of observation-based causation. These connections shed light on the range of applicability of both models.

 If A caused B and B caused C, did A cause C? Although laypersons commonly perceive causality as being transitive, some philosophers have questioned this assumption, and models of causality in artificial intelligence are often agnostic with respect to transitivity. We consider two formal models of causation that differ in the way they represent uncertainty. The quantitative model uses a crude probabilistic definition, arguably the common core of more sophisticated quantitative definitions; the qualitative model uses a definition based on nonmonotonic consequence relations. Different sufficient conditions for the transitivity of causation are laid bare by the two models: The Markov condition on events for the quantitative model, and a so-called saliency condition (A is perceived as a typical cause of B) for the qualitative model. We explore the formal and empirical relations between these sufficient conditions, and between the underlying definitions of perceived causation. These connections shed light on the range of applicability of each model, contrasting commonsense causal reasoning (supposedly qualitative) and scientific causation (more naturally quantitative). These speculations are supported by a series of three behavioral experiments.

-----------------------
 The commentators expressed concerns regarding the relevance and value of non-computational non-symbolic explanations of cognitive performance. But what counts as an explanation depends on the pre-theoretical assumptions behind the scenes of empirical science regarding the kinds of variables and relationships that are sought out in the first place, and some of the present disagreements stem from incommensurate assumptions. Traditional cognitive science presumes cognition to be a decomposable system of components interacting according to computational rules to generate cognitive performances (i.e., component-dominant dynamics). We assign primacy to interaction-dominant dynamics among components. Though either choice can be a good guess before the fact, the primacy of interactions is now supported by much recent empirical work in cognitive science. Consequently, in the main, the commentators have failed so far to address the growing evidence corroborating the theory-driven predictions of complexity science.

 Readers of TopiCS are invited to join a debate about the utility of ideas and methods of complexity science. The topics of debate include empirical instances of qualitative change in cognitive activity and whether this empirical work demonstrates sufficiently the empirical flags of complexity. In addition, new phenomena discovered by complexity scientists, and motivated by complexity theory, call into question some basic assumptions of conventional cognitive science such as stable equilibria and homogeneous variance. The articles and commentaries that appear in this issue also illustrate a new debate style format for topiCS.

-----------------------
 The commentators expressed concerns regarding the relevance and value of non-computational non-symbolic explanations of cognitive performance. But what counts as an explanation depends on the pre-theoretical assumptions behind the scenes of empirical science regarding the kinds of variables and relationships that are sought out in the first place, and some of the present disagreements stem from incommensurate assumptions. Traditional cognitive science presumes cognition to be a decomposable system of components interacting according to computational rules to generate cognitive performances (i.e., component-dominant dynamics). We assign primacy to interaction-dominant dynamics among components. Though either choice can be a good guess before the fact, the primacy of interactions is now supported by much recent empirical work in cognitive science. Consequently, in the main, the commentators have failed so far to address the growing evidence corroborating the theory-driven predictions of complexity science.

 Readers of TopiCS are invited to join a debate about the utility of ideas and methods of complexity science. The topics of debate include empirical instances of qualitative change in cognitive activity and whether this empirical work demonstrates sufficiently the empirical flags of complexity. In addition, new phenomena discovered by complexity scientists, and motivated by complexity theory, call into question some basic assumptions of conventional cognitive science such as stable equilibria and homogeneous variance. The articles and commentaries that appear in this issue also illustrate a new debate style format for topiCS.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Eye movements were monitored as subjects read sentences containing high- or low-predictable target words. The extent to which target words were predictable from prior context was varied: Half of the target words were predictable, and the other half were unpredictable. In addition, the length of the target word varied: The target words were short (4-6 letters), medium (7-9 letters), or long (10-12 letters). Length and predictability both yielded strong effects on the probability of skipping the target words and on the amount of time readers fixated the target words (when they were not skipped). However, there was no interaction in any of the measures examined for either skipping or fixation time. The results demonstrate that word predictability (due to contextual constraint) and word length have strong and independent influences on word skipping and fixation durations. Furthermore, because the long words extended beyond the word identification span, the data indicate that skipping can occur on the basis of partial information in relation to word identity.

 We examined the initial landing position of the eyes in target words that were either predictable or unpredictable from the preceding sentence context. Although readers skipped over predictable words more than unpredictable words and spent less time on predictable words when they did fixate on them, there was no difference in the launch site of the saccade to the target word. Moreover, there was only a very small difference in the initial landing position on the target word as a function of predictability when the target words were fixated which is most parsimoniously explained by positing that a few programmed skips of the target word fell short of their intended target. These results suggest: that low-level processing is primarily responsible for landing position effects in reading. (C) 2001 Elsevier Science Ltd. All rights reserved.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The modifier effect is the reduction in perceived likelihood of a generic property sentence, when the head noun is modified. We investigated the prediction that the modifier effect would be stronger for mutable than for central properties, without finding evidence for this predicted interaction over the course of five experiments. However Experiment 6, which provided a brief context for the modified concepts to lend them greater credibility, did reveal the predicted interaction. It is argued that the modifier effect arises primarily from a general lack of confidence in generic statements about the typical properties of unfamiliar concepts. Neither prototype nor classical models of concept combination receive support from the phenomenon. (C) 2010 Elsevier Inc. All rights reserved.

 Within-category induction is the projection of a generic property from a class (Apples are sweet) to a subtype of that class (Chinese apples are sweet). The modifier effect refers to the discovery reported by Connolly et al., that the subtype statement tends to be judged less likely to be true than the original unmodified sentence. The effect was replicated and shown to be moderated by the typicality of the modifier (Experiment 1). Likelihood judgements were also found to correlate between modified and unmodified versions of sentences. Experiment 2 elicited justifications, which suggested three types of reason for the effect-pragmatics, knowledge-based reasoning, and uncertainty about attribute inheritance. It is argued that the results provide clear evidence for the default inheritance of prototypical attributes in modified concepts, although a full account of the effect remains to be given.

-----------------------
 We argue that psycholinguistics should be concerned with both the representation and the processing of language. Recent experimental work on syntax in language comprehension has largely concentrated on the way in which language is processed and has assumed that theoretical linguistics serves to determine the representation of language. In contrast, we advocate experimental work on the mental representation of grammatical knowledge, and argue that syntactic priming is a promising way to do this. Syntactic priming is the phenomenon whereby exposure to a sentence with a particular syntactic construction can affect the subsequent processing of an otherwise unrelated sentence with the same (or, perhaps, related) structure, for reasons of that structure. We assess evidence for syntactic priming in corpora, and then consider experimental evidence for priming in production and comprehension and for bidirectional priming between comprehension and production. This in particular strongly suggests that priming is tapping into linguistic knowledge itself; and is not just facilitating particular processes. The final section discusses the importance of priming evidence for any account of language construed as the mental representation of human linguistic capacities.

 Repetition is a central phenomenon of behavior, and researchers have made extensive use of it to illuminate psychological functioning. In the language sciences, a ubiquitous form of such repetition is structural priming, a tendency to repeat or better process a current sentence because of its structural similarity to a previously experienced ("prime") sentence (J. K. Bock, 1986). The recent explosion of research in structural priming has made it the dominant means of investigating the processes involved in the production (and increasingly, comprehension) of complex expressions such as sentences. This review considers its implications for the representation of syntax and the mechanisms of production and comprehension and their relationship. It then addresses the potential functions of structural priming, before turning to its implications for first language acquisition, bilingualism, and aphasia. The authors close with theoretical and empirical recommendations for future investigations.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 We agree with Caplan &amp; Waters that there are problems with the single-resource theory of sentence comprehension. However, we challenge their dual-resource alternative on theoretical and empirical grounds and point to a more coherent solution that abandons the notion of working memory resources.

 M. A. Just and P. A. Carpenter's (1992) capacity theory of comprehension posits a linguistic working memory functionally separated from the representation of linguistic knowledge. G. S. Waters and D. Caplan's (1996) critique of this approach retained the notion of a separate working memory. In this article, the authors present an alternative account motivated by a connectionist approach to language comprehension. In their view, processing capacity emerges from network architecture and experience and is not a primitive that can vary independently. Individual differences in comprehension do not stem from variations in a separate working memory capacity; instead they emerge from an interaction of biological factors and language experience. This alternative is argued to provide a superior account of comprehension results previously attributed to a separate working memory capacity.

-----------------------
 An experiment with a multiple-cue judgment task tested the hypothesis that humans can only abstract explicit representations of cue-criterion relations when the cues are related to the criterion by an additive function. It is proposed that the sequential and capacity-constrained nature of controlled, explicit thought can only induce and execute linear additive cue integration; non-additive environments require exemplar memory. The results showed that an additive task induced processes of cue abstraction and cue integration, while a multiplicative task induced exemplar processes. The results suggest flexible interplay between distinct representation-levels, a preference to abstract explicit "rules" whenever possible, although this capacity is constrained to additive cue-criterion relations.

 The majority of previous studies on multiple-cue judgment with continuous cues have involved comparisons between judgments and multiple linear regression models that integrated cues into a judgment. The authors present an experiment indicating that in a judgment task with additive combination of multiple continuous cues, people indeed displayed abstract knowledge of the cue criterion relations that was mentally integrated into a judgment, but in a task with multiplicative combination of continuous cues, people instead relied on retrieval of memory traces of similar judgment cases (exemplars). These results suggest that people may adopt qualitatively distinct forms of knowledge, depending on the structure of a multiple-cue judgment task. The authors discuss implications for theories of multiple-cue judgment.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 SUSTAIN (Supervised and Unsupervised STratified Adaptive Incremental Network) is a model of how humans learn categories from examples. SUSTAIN initially assumes a simple category structure. If simple solutions prove inadequate and SUSTAIN is confronted with a surprising event (e.g., it is told that a bat is a mammal instead of a bird), SUSTAIN recruits an additional cluster to represent the surprising event. Newly recruited clusters are available to explain future events and can themselves evolve into prototypes-attractors-rules. SUSTAIN's discovery of category substructure is affected not only by the structure of the world but by the nature of the learning task and the learner's goals. SUSTAIN successfully extends category learning models to studies of inference learning, unsupervised learning, category construction, and contexts in which identification learning is faster than classification learning.

 Recent research shows that similarity comparisons involve an alignment process in which features are placed into correspondence. In 6 studies, the authors showed that alignment is involved in category learning as well. Within a category, aligned matches (feature matches occurring on the same dimension) facilitate learning more than nonaligned matches do (matches on different dimensions), although nonaligned matches still facilitate learning relative to nonmatches. Analogously, feature matches that cross category boundaries hurt learning more if they occur on the same versus a different dimension, and cross-category feature matches on different dimensions hurt learning relative to nonmatching features. Representational assumptions of category learning models must be modified to account for the differences between aligned and nonaligned feature matches.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Although the presence or absence of a pitch accent clearly can play an important role in signaling the discourse and information structure of an utterance, whether the form of an accent determines the type of information it conveys is more controversial. We used an eye-tracking paradigm to investigate whether H*, which has been argued to signal new information, evokes different eye fixations than L+H*, which has been argued to signal the presence of contrast. Our results demonstrate that although listeners interpret these accents differently, their interpretive domains overlap. L+H* creates a strong bias toward contrast referents whereas H* is compatible with both new and contrast referents.

 The notion of common ground is important for the production of referring expressions: In order for a referring expression to be felicitous, it has to be based on shared information. But determining what information is shared and what information is privileged may require gathering information from multiple sources, and constantly coordinating and updating them, which might be computationally too intensive to affect the earliest moments of production. Previous work has found that speakers produce overinformative referring expressions, which include privileged names, violating Grices Maxims, and concluded that this is because they do not mark the distinction between shared and privileged information. We demonstrate that speakers are in fact quite effective in marking this distinction in the form of their utterances. Nonetheless, under certain circumstances, speakers choose to overspecify privileged names.

-----------------------
 Theories can be found throughout cognitive science that give an explanatory role to similarity. Such theories can be contrasted with those that model thought using abstract rules, We lay out four possible explanatory roles for similarity. We then review the computational pros and cons of similarity-and rule-based models and outline the empirical work that speaks to the psychological plausibility of the two frameworks. We conclude that an adequate model of human thought must take advantage of both the flexibility of similarity-based inference and the compositionality and certainty associated with rule-based inference, (C) 1998 Elsevier Science B.V.

 We highlight one way in which Jones &amp; Love (J&amp;L) misconstrue the Bayesian program: Bayesian models do not represent a rejection of mechanism. This mischaracterization obscures the valid criticisms in their article. We conclude that computational-level Bayesian modeling should not be rejected or discouraged a priori, but should be held to the same empirical standards as other models.

-----------------------
 In this paper we examine the way disjunctive choices work in conversational context. We focus on disjunctive deontic rules, such as 'you must either submit an essay or attend an exam'. According to the Gricean maxim of orderliness, a derivative of the maxim of manner, people should interpret the first-mentioned option as the one preferred by the speaker. Two experiments demonstrated order bias, a non-logical preference for the first-mentioned option in the disjunctive situations of obligatory choice and free choice permissions, but not in the non-disjunctive situation of obligatory consequences. Order bias influences interpretations of the speaker's preferences, but not of the recipient's preferences; it was mainly a function of the nature of the situation, while linguistic form had only limited effect. We offer a general theoretical framework for order bias as a communicative device, constraining both interpretation and generation of utterances.

 The ability to entertain possibilities and draw inferences about them is essential to human intelligence. We examine the hypothesis that conditional if-then statements trigger a mental simulation process in which people suppose the antecedent (if statement) to be true and evaluate the consequent (then statement) in that context. On the assumption that supposing an event to be true increases belief that the event has occurred or will occur, this hypothesis is consistent with the claim that evaluating a conditional will heighten belief in its antecedent more than in its consequent. Two experiments, employing conditionals of the form If animal A has property X, then animal B will have property X, in which X was a property that people could not readily relate to the animals, supported this claim. The effect was stronger following the evaluation of conditionals with dissimilar animal categories.

-----------------------
 Theories can be found throughout cognitive science that give an explanatory role to similarity. Such theories can be contrasted with those that model thought using abstract rules, We lay out four possible explanatory roles for similarity. We then review the computational pros and cons of similarity-and rule-based models and outline the empirical work that speaks to the psychological plausibility of the two frameworks. We conclude that an adequate model of human thought must take advantage of both the flexibility of similarity-based inference and the compositionality and certainty associated with rule-based inference, (C) 1998 Elsevier Science B.V.

 We highlight one way in which Jones &amp; Love (J&amp;L) misconstrue the Bayesian program: Bayesian models do not represent a rejection of mechanism. This mischaracterization obscures the valid criticisms in their article. We conclude that computational-level Bayesian modeling should not be rejected or discouraged a priori, but should be held to the same empirical standards as other models.

-----------------------
 This paper presents a cognitive account of the process of evaluating scientific data. Our account assumes that when individuals evaluate data, they construct a mental model of a data-interpretation package, in which the data and theoretical interpretations of the data are integrated. We propose that individuals attempt to discount data by seeking alternative explanations for events within the mental model; data-interpretation packages are accepted when the individual cannot find alternative accounts for these events. Our analysis indicates that there are many levels at which data-interpretation packages can be accepted or denied.

 This article reports the results of a study investigating how undergraduates evaluate realistic scientific data in the domains of geology and paleontology. The results are used to test several predictions of a theory of data evaluation, which we call models-of-data theory. Models-of-data theory assumes that when evaluating data, the individual constructs a particular kind of cognitive model that integrates many features of the data with a theoretical interpretation of the data, The individual evaluates the model by attempting to generate alternative causal explanations for the events in the model. We contrast models-of-data theory with other proposals for how data are cognitively represented and show that models-of-data theory gives a good account of the pattern of written evaluations of data produced by the undergraduates in the study, We discuss theoretical and instructional implications of the theory.

-----------------------
 One of the central findings of speech perception is that identical acoustic signals can be perceived as different speech sounds depending on adjacent speech context. Although these phonetic context effects are ubiquitous in speech perception, their neural mechanisms remain largely unknown. The present work presents a review of recent data suggesting that spectral content of speech mediates phonetic context effects and argues that these effects are likely to be governed by general auditory processes. A descriptive framework known as spectral contrast is presented as a means of interpreting these findings. Finally, and most centrally, four behavioral experiments that begin to delineate the level of the auditory system at which interactions among stimulus components occur are described. Two of these experiments investigate the influence of diotic versus dichotic presentation upon two phonetic context effects. Results indicate that context effects remain even when context is presented to the ear contralateral to that of the target syllable. The other two experiments examine the time course of phonetic context effects by manipulating the silent interval between context and target syllables. These studies reveal that phonetic context effects persist for hundreds of milliseconds. Results are interpreted in terms of auditory mechanism with particular attention to the putative link between auditory enhancement and phonetic context effects. (C) 2002 Elsevier Science B.V. All rights reserved.

 Nonspeech stimuli influence phonetic categorization, but effects observed so far have been limited to precursors' influence on perception of following speech. However, both preceding and following speech affect phonetic categorization. This asymmetry raises questions about whether general auditory processes play a role in context-dependent speech perception. This study tested whether the asymmetry stems from methodological issues or genuine mechanistic limitations. To determine whether and how backward effects of nonspeech context on speech may occur, one experiment examined perception of CVC words with [ga]-[da] series onsets followed by one of two possible embedded tones and one of two possible final consonants. When the tone was separated from the target onset by 100 ms, contrastive effects of tone frequency similar to those of previous studies were observed; however, when the tone was moved closer to the target segment assimilative effects were observed. In another experiment, contrastive effects of a following tone were observed in both CVC words and CV nonwords, although the size of the effects depended on syllable structure. Results are discussed with respect to contrastive mechanisms not speech-specific but operating at a relatively high level, taking into account spectrotemporal patterns occurring over extended periods before and after target events. (c) 2005 Acoustical Society of America.

-----------------------
 One of the central findings of speech perception is that identical acoustic signals can be perceived as different speech sounds depending on adjacent speech context. Although these phonetic context effects are ubiquitous in speech perception, their neural mechanisms remain largely unknown. The present work presents a review of recent data suggesting that spectral content of speech mediates phonetic context effects and argues that these effects are likely to be governed by general auditory processes. A descriptive framework known as spectral contrast is presented as a means of interpreting these findings. Finally, and most centrally, four behavioral experiments that begin to delineate the level of the auditory system at which interactions among stimulus components occur are described. Two of these experiments investigate the influence of diotic versus dichotic presentation upon two phonetic context effects. Results indicate that context effects remain even when context is presented to the ear contralateral to that of the target syllable. The other two experiments examine the time course of phonetic context effects by manipulating the silent interval between context and target syllables. These studies reveal that phonetic context effects persist for hundreds of milliseconds. Results are interpreted in terms of auditory mechanism with particular attention to the putative link between auditory enhancement and phonetic context effects. (C) 2002 Elsevier Science B.V. All rights reserved.

 Nonspeech stimuli influence phonetic categorization, but effects observed so far have been limited to precursors' influence on perception of following speech. However, both preceding and following speech affect phonetic categorization. This asymmetry raises questions about whether general auditory processes play a role in context-dependent speech perception. This study tested whether the asymmetry stems from methodological issues or genuine mechanistic limitations. To determine whether and how backward effects of nonspeech context on speech may occur, one experiment examined perception of CVC words with [ga]-[da] series onsets followed by one of two possible embedded tones and one of two possible final consonants. When the tone was separated from the target onset by 100 ms, contrastive effects of tone frequency similar to those of previous studies were observed; however, when the tone was moved closer to the target segment assimilative effects were observed. In another experiment, contrastive effects of a following tone were observed in both CVC words and CV nonwords, although the size of the effects depended on syllable structure. Results are discussed with respect to contrastive mechanisms not speech-specific but operating at a relatively high level, taking into account spectrotemporal patterns occurring over extended periods before and after target events. (c) 2005 Acoustical Society of America.

-----------------------
 Previous research has suggested that the initial portion of a word activates similar sounding words that compete for recognition. Other research has shown that the number of similar sounding words that are activated influences the speed and accuracy of recognition. Words with few neighbors are processed more quickly and accurately than words with many neighbors. The influences of the number of lexical competitors in the initial part of the word were examined in a shadowing and a lexical-decision task. Target words with few neighbors that share the initial phoneme were responded to more quickly than target words with many neighbors that share the initial phoneme. The implications of onset-density effects for models of spoken-word recognition are discussed.

 Clustering coefficient-a measure derived from the new science of networks-refers to the proportion of phonological neighbors of a target word that are also neighbors of each other. Consider the words bat, hat, and can, all of which are neighbors of the word cat; the words bat and hat are also neighbors of each other. In a perceptual identification task, words with a low clustering coefficient (i.e., few neighbors are neighbors of each other) were more accurately identified than words with a high clustering coefficient (i.e., many neighbors are neighbors of each other). In a lexical decision task, words with a low clustering coefficient were responded to more quickly than words with a high clustering coefficient. These findings suggest that the structure of the lexicon (i.e., the similarity relationships among neighbors of the target word measured by clustering coefficient) influences lexical access in spoken word recognition. Simulations of the TRACE and Shortlist models of spoken word recognition failed to account for the present findings. A framework for a new model of spoken word recognition is proposed.

-----------------------
 This paper used self-paced reading to test processing preferences in pronoun interpretation in English two clause sentences. The results demonstrate that people's preferences can be reversed by changing the coherence relation between the clauses. The results are not compatible with the existence of a single all-purpose strategy in pronoun resolution. Rather, the results support Kehler's (2002) hypothesis that the processing patterns observed in pronoun processing are a byproduct of more general cognitive inference processes underlying the establishment of coherence, such that discourse coherence guides pronoun reference, and pronoun reference guides discourse coherence.

 The present study compares the processing of unambiguous restrictive and non-restrictive relative clauses (RCs) within both a null context and a supportive discourse using a self-paced reading methodology. Individuals read restrictive RCs more slowly than non-restrictive RCs in a null context, but processed restrictive RCs faster than non-restrictive RCs in supportive context, resulting in an interaction between context and RC type. These results provide evidence for two theoretical points. First, principles analogous to those in referential theory [Altmann G. T. M., T Steedman, M. (1988). Interaction with context during human sentence processing. Cognition, 30, 191-238; Crain, S., T Steedman, M. (1985). On not being led up the garden path: The use of context by the psychological parser. In D. Dowty, L. Karttunnen, A. Zwicky (Eds.), Natural language parsing. Cambridge, UK: Cambridge University Press] apply not only in resolving ambiguity but also in processing unambiguous sentences. Second, the discourse context can guide and facilitate interpretive processing. This result suggests that intrasentential factors such as syntax are not autonomous from contextual processing, contrary to the modularity hypothesis [Fodor, J. A. (1983). Modularity of mind. Cambridge, MA: MIT Press]. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 This paper used self-paced reading to test processing preferences in pronoun interpretation in English two clause sentences. The results demonstrate that people's preferences can be reversed by changing the coherence relation between the clauses. The results are not compatible with the existence of a single all-purpose strategy in pronoun resolution. Rather, the results support Kehler's (2002) hypothesis that the processing patterns observed in pronoun processing are a byproduct of more general cognitive inference processes underlying the establishment of coherence, such that discourse coherence guides pronoun reference, and pronoun reference guides discourse coherence.

 The present study compares the processing of unambiguous restrictive and non-restrictive relative clauses (RCs) within both a null context and a supportive discourse using a self-paced reading methodology. Individuals read restrictive RCs more slowly than non-restrictive RCs in a null context, but processed restrictive RCs faster than non-restrictive RCs in supportive context, resulting in an interaction between context and RC type. These results provide evidence for two theoretical points. First, principles analogous to those in referential theory [Altmann G. T. M., T Steedman, M. (1988). Interaction with context during human sentence processing. Cognition, 30, 191-238; Crain, S., T Steedman, M. (1985). On not being led up the garden path: The use of context by the psychological parser. In D. Dowty, L. Karttunnen, A. Zwicky (Eds.), Natural language parsing. Cambridge, UK: Cambridge University Press] apply not only in resolving ambiguity but also in processing unambiguous sentences. Second, the discourse context can guide and facilitate interpretive processing. This result suggests that intrasentential factors such as syntax are not autonomous from contextual processing, contrary to the modularity hypothesis [Fodor, J. A. (1983). Modularity of mind. Cambridge, MA: MIT Press]. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 The relationship between emotions and learning was investigated by tracking the affective states that college students experienced while interacting with AutoTutor, an intelligent tutoring system with conversational dialogue. An emotionally responsive tutor would presumably facilitate learning, but this would only occur if learner emotions can be accurately identified. After a learning session with AutoTutor, the affective states of the learner were classified by the learner and two accomplished teachers. The classification of the teachers was not very reliable and did not match the learners self reports. This result suggests that accomplished teachers may be limited in detecting the affective states of learners. This paper discusses the implications of our findings for theories of expert tutoring and for alternate methodologies for establishing convergent validity of affect measurement.

 Folk wisdom holds that being confused is detrimental to learning. However, research on emotions and learning suggest a somewhat more complex relationship between confusion and learning outcomes. In fact, it has been proposed that impasses that trigger states of cognitive disequilibrium and confusion can create opportunities for deep learning of conceptually difficult content. This paper discusses four computer learning environments that either naturally or artificially induce confusion in learners in order to create learning opportunities. First, an Intelligent Tutoring System called AutoTutor that engenders confusion through challenging problems and vague hints is described. The remaining three environments were specifically designed to induce confusion through a number of different interventions. These interventions include device breakdowns, contradictory information, and false feedback. The success and limitations of confusion induction and the impact of confusion resolution on learning are discussed. Potential methods to help learners productively manage their confusion instead of being hopelessly confused are also discussed. (C) 2012 Elsevier Inc. All rights reserved.

-----------------------
 The relationship between emotions and learning was investigated by tracking the affective states that college students experienced while interacting with AutoTutor, an intelligent tutoring system with conversational dialogue. An emotionally responsive tutor would presumably facilitate learning, but this would only occur if learner emotions can be accurately identified. After a learning session with AutoTutor, the affective states of the learner were classified by the learner and two accomplished teachers. The classification of the teachers was not very reliable and did not match the learners self reports. This result suggests that accomplished teachers may be limited in detecting the affective states of learners. This paper discusses the implications of our findings for theories of expert tutoring and for alternate methodologies for establishing convergent validity of affect measurement.

 Folk wisdom holds that being confused is detrimental to learning. However, research on emotions and learning suggest a somewhat more complex relationship between confusion and learning outcomes. In fact, it has been proposed that impasses that trigger states of cognitive disequilibrium and confusion can create opportunities for deep learning of conceptually difficult content. This paper discusses four computer learning environments that either naturally or artificially induce confusion in learners in order to create learning opportunities. First, an Intelligent Tutoring System called AutoTutor that engenders confusion through challenging problems and vague hints is described. The remaining three environments were specifically designed to induce confusion through a number of different interventions. These interventions include device breakdowns, contradictory information, and false feedback. The success and limitations of confusion induction and the impact of confusion resolution on learning are discussed. Potential methods to help learners productively manage their confusion instead of being hopelessly confused are also discussed. (C) 2012 Elsevier Inc. All rights reserved.

-----------------------
 We introduce selfrepairing neural networks as a model for recovery from brain damage. Small lesions are repaired through reinstatement of the redundancy in the network's connections. With mild lesions, this process can model autonomous recovery. Moderate lesions require patterned input. In this paper, we discuss implementations in three types of network of increasing biological plausibility. We also mention some results from random graph theory. Finally, we discuss the implications for rehabilitation theory.

 Computational modeling of the brain holds great promise as a bridge from brain to behavior. To fulfill this promise, however, it is not enough for models to be 'biologically plausible': models must be structurally accurate. Here, we analyze what this entails for so-called psychobiological models, models that address behavior as well as brain function in some detail. Structural accuracy may be supported by (1) a model's a priori plausibility, which comes from a reliance on evidence-based assumptions, (2) fitting existing data, and (3) the derivation of new predictions. All three sources of support require modelers to be explicit about the ontology of the model, and require the existence of data constraining the modeling. For situations in which such data are only sparsely available, we suggest a new approach. if several models are constructed that together form a hierarchy of models, higher-level models can be constrained by lower-level models, and low-level models can be constrained by behavioral features of the higher-level models. Modeling the same substrate at different levels of representation, as proposed here, thus has benefits that exceed the merits of each model in the hierarchy on its own.

-----------------------
 Two experimental tasks in psychology, the two-stage gambling game and the Prisoner's Dilemma game, show that people violate the sure thing principle of decision theory. These paradoxical findings have resisted explanation by classical decision theory for over a decade. A quantum probability model, based on a Hilbert space representation and Schrodinger's equation, provides a simple and elegant explanation for this behaviour. The quantum model is compared with an equivalent Markov model and it is shown that the latter is unable to account for violations of the sure thing principle. Accordingly, it is argued that quantum probability provides a better framework for modelling human decision-making.

 Naive observers typically perceive some groupings for a set of stimuli as more intuitive than others. The problem of predicting category intuitiveness has been historically considered the remit of models of unsupervised categorization. In contrast, this article develops a measure of category intuitiveness from one of the most widely supported models of supervised categorization, the generalized context model (GCM). Considering different category assignments for a set of instances, the authors asked how well the GCM can predict the classification of each instance on the basis of all the other instances. The category assignment that results in the smallest prediction error is interpreted as the most intuitive for the GCM-the authors refer to this way of applying the GCM as "unsupervised GCM." The authors systematically compared predictions of category intuitiveness from the unsupervised GCM and two models of unsupervised categorization: the simplicity model and the rational model. The unsupervised GCM compared favorably with the simplicity model and the rational model. This success of the unsupervised GCM illustrates that the distinction between supervised and unsupervised categorization may need to be reconsidered. However, no model emerged as clearly superior, indicating that there is more work to be done in understanding and modeling category intuitiveness.

-----------------------
 This article demonstrates the potential of using hierarchical Bayesian methods to relate models and data in the cognitive sciences. This is done using a worked example that considers an existing model of category representation, the Varying Abstraction Model (VAM), which attempts to infer the representations people use from their behavior in category learning tasks. The VAM allows for a wide variety of category representations to be inferred, but this article shows how a hierarchical Bayesian analysis can provide a unifying explanation of the representational possibilities using 2 parameters. One parameter controls the emphasis on abstraction in category representations, and the other controls the emphasis on similarity. Using 30 previously published data sets, this work shows how inferences about these parameters, and about the category representations they generate, can be used to evaluate data in terms of the ongoing exemplar versus prototype and similarity versus rules debates in the literature. Using this concrete example, this article emphasizes the advantages of hierarchical Bayesian models in converting model selection problems to parameter estimation problems, and providing one way of specifying theoretically based priors for competing models.

 In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making. (C) 2010 Elsevier B.V. All rights reserved.

 Thirty previously published data sets, from seminal category learning tasks, are reanalyzed using the varying abstraction model (VAM). Unlike a prototype-versus-exemplar analysis, which focuses on extreme levels of abstraction only, a VAM analysis also considers the possibility of partial abstraction. Whereas most data sets support no abstraction when only the extreme possibilities are considered, we show that evidence for abstraction can be provided using the broader view on abstraction provided by the VAM. The present results generalize earlier demonstrations of partial abstraction (Vanpaemel &amp; Storms, 2008), in which only a small number of data sets was analyzed. Following the dominant modus operandi in category learning research, Vanpaemel and Storms evaluated the models on their best fit, a practice known to ignore the complexity of the models under consideration. In the present study, in contrast, model evaluation not only relies on the maximal likelihood, but also on the marginal likelihood, which is sensitive to model complexity. Finally, using a large recovery study, it is demonstrated that, across the 30 data sets, complexity differences between the models in the VAM family are small. This indicates that a (computationally challenging) complexity-sensitive model evaluation method is uncalled for, and that the use of a (computationally straightforward) complexity-insensitive model evaluation method is justified.

-----------------------
 This article demonstrates the potential of using hierarchical Bayesian methods to relate models and data in the cognitive sciences. This is done using a worked example that considers an existing model of category representation, the Varying Abstraction Model (VAM), which attempts to infer the representations people use from their behavior in category learning tasks. The VAM allows for a wide variety of category representations to be inferred, but this article shows how a hierarchical Bayesian analysis can provide a unifying explanation of the representational possibilities using 2 parameters. One parameter controls the emphasis on abstraction in category representations, and the other controls the emphasis on similarity. Using 30 previously published data sets, this work shows how inferences about these parameters, and about the category representations they generate, can be used to evaluate data in terms of the ongoing exemplar versus prototype and similarity versus rules debates in the literature. Using this concrete example, this article emphasizes the advantages of hierarchical Bayesian models in converting model selection problems to parameter estimation problems, and providing one way of specifying theoretically based priors for competing models.

 In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 This article demonstrates the potential of using hierarchical Bayesian methods to relate models and data in the cognitive sciences. This is done using a worked example that considers an existing model of category representation, the Varying Abstraction Model (VAM), which attempts to infer the representations people use from their behavior in category learning tasks. The VAM allows for a wide variety of category representations to be inferred, but this article shows how a hierarchical Bayesian analysis can provide a unifying explanation of the representational possibilities using 2 parameters. One parameter controls the emphasis on abstraction in category representations, and the other controls the emphasis on similarity. Using 30 previously published data sets, this work shows how inferences about these parameters, and about the category representations they generate, can be used to evaluate data in terms of the ongoing exemplar versus prototype and similarity versus rules debates in the literature. Using this concrete example, this article emphasizes the advantages of hierarchical Bayesian models in converting model selection problems to parameter estimation problems, and providing one way of specifying theoretically based priors for competing models.

 In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making. (C) 2010 Elsevier B.V. All rights reserved.

-----------------------
 This article reviews the rationale for using accumulative one-step-ahead prediction error (APE) as a data-driven method for model selection. Theoretically, APE is closely related to Bayesian model selection and the method of minimum description length (MDL). The sole requirement for using APE is that the models under consideration are capable of generating a prediction for the next, unseen data point. This means that APE may be readily applied to selection problems involving very complex models. APE automatically takes the functional form of parameters into account, and the 'plug-in' version of APE does not require the specification of priors. APE is particularly easy to compute for data that have a natural ordering, such as time series. Here, we explore the possibility of using APE to discriminate the short-range ARMA(1, 1) model from the long-range ARFIMA(0,d, 0) model. We also illustrate how APE may be used for model meta-selection, allowing one to choose between different model selection methods. (c) 2006 Elsevier Inc. All rights reserved.

 Most decision-making research has focused on choices between two alternatives. For choices between many alternatives, the primary result is Hick's Law-that mean response time increases logarithmically with the number of alternatives. Various models for this result exist within specific paradigms, and there are some more general theoretical results, but none of those have been tested stringently against data. We present an experimental paradigm that supports detailed examination of multi-choice data, and analyze predictions from a Bayesian ideal observer model for this paradigm. Data from the experiment deviate from the predictions of the Bayesian model in interesting ways. A simple heuristic model based on evidence accumulation provides a good account for the data, and has attractive properties as a limit case of the Bayesian model. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 Recent algorithmic and theoretical advances in reinforcement learning (RL) have attracted widespread interest. RL algorithms have appeared that approximate dynamic programming on an incremental basis. They can be trained on the basis of real or simulated experiences, focusing their computation on areas of state space that are actually visited during control, making them computationally tractable on very large problems. If each member of a team of agents employs one of these algorithms, a new collective learning algorithm emerges for the team as a whole. In this paper we demonstrate that such collective RL algorithms can be powerful heuristic methods for addressing large-scale control problems.
 We describe CST, an online algorithm for constructing skill trees from demonstration trajectories. CST segments a demonstration trajectory into a chain of component skills, where each skill has a goal and is assigned a suitable abstraction from an abstraction library. These properties permit skills to be improved efficiently using a policy learning algorithm. Chains from multiple demonstration trajectories are merged into a skill tree. We show that CST can be used to acquire skills from human demonstration in a dynamic continuous domain, and from both expert demonstration and learned control sequences on the uBot-5 mobile manipulator.

-----------------------
 Lejeune (1998) (Switching or gating? The attentional challenge in cognitive models of psychological time. Behav. Process. 44, 127-45) analyzed and compared two models of prospective timing: the classical switching model and the attentional-gate model. Lejeune argued that a modified switch notion, which can be opened and closed in a frequency which reflects the amount of attentional resources allocated for timing can provide a satisfactory explanation for the impact of attention on prospective timing, and therefore the notion of an 'attentional switch' is favored over adding an 'attentional gate.' In the present analysis, the two competing models are compared in terms of correspondence with the nature of attentional processes, as well as in terms of logical analysis and explanatory power. Based on this comparison, it is argued that gating is a better model of prospective timing than switching. (C) 2000 Published by Elsevier Science B.V. All rights reserved.

 Lejeune (1998) (Switching or gating? The attentional challenge in cognitive models of psychological time. Behav. Process. 34, 127-45) analyzed and compared two models of prospective timing: the classical switching model and the attentional-gate model. Lejeune argued that a modified switch notion, which can he opened and closed in a frequency which reflects the amount of attentional resources allocated for timing can provide a satisfactory explanation for the impact of attention on prospective timing, and therefore the notion of an 'attentional switch' is favored over adding an 'attentional gate.' In the present analysis, the two competing models are compared in terms of correspondence with the nature of attentional processes, as well as in terms of logical analysis and explanatory power. Based on this comparison, it is argued that gating is a better model of prospective timing than switching. (C) 2000 Elsevier Science B.V. All rights reserved.

-----------------------
 This work describes a cognitively realistic approach to social simulation. It begins with a model created by Gilbert [4] for capturing the growth of academic science. Gilbert's model, which was equation-based, is replaced here by an agent-based (neural network) model, with the (neural network based) cognitive architecture CLARION providing greater cognitive realism. Using this agent model, results comparable to previous simulations and to human data are obtained. It is found that while different cognitive settings may affect the aggregate number of scientific articles produced by the model, they do not generally lead to different distributions of number of articles per author. It is argued that using more cognitively realistic models in simulations may lead to novel insights.

 This paper explores how mental disorders of certain types might be explained based on mechanisms and processes of human motivation (including drives and goals) and action selection (as well as other related mechanisms and processes), within a generic, comprehensive computational cognitive architecture model. Several simulation tests have been conducted that demonstrate that the model is reasonable, and captures some characteristics of certain mental disorders. The work is a first step in showing the feasibility of integrating mental disorders modeling/simulation into a cognitive architecture model.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 Participants were required to detect spot stimuli briefly presented to the upper, central, or lower visual fields. The stimuli were presented either on a green or a red background. Results showed that reaction time (RT) was shorter for the lower visual field (LVF) compared to the upper visual field (UVF). Furthermore, this LVF advantage was significantly reduced in the red background condition compared to the green one. A red light is known to suppress activity of the magno-dominated stream. Therefore, the LVF advantage in RT can be explained as resulting from the biased representation of the magno-dominated stream in the LVF. (C) 2004 Elsevier Inc. All rights reserved.

 Right-handed participants performed simple visual judgments on nonverbal stimuli presented either to the left visual field-right hemisphere (LVF-RH) or to the right visual field-left hemisphere (RVF-LH). The stimuli were exposed for 40-120 msec, followed by a backward mask. When the stimuli were presented against a green background, an RVF-LH advantage was observed for the shortest exposure duration. This result supports the notion that the LH has finer temporal resolution than the RH. Imposition of a red background disrupted performance and eliminated the RVF-LH advantage for the shortest exposure duration. Because the red background attenuates functions of the magnocellular pathway, these results suggest that the magnocellular pathway contributes to the LH advantage for fine temporal resolution.

-----------------------
 Clark &amp; Thornton's conception finds an echo in implicit learning research, which shows that subjects may perform adaptively in complex structured situations through the use of simple statistical learning mechanisms. However, the authors fail to draw a distinction between, on the one hand, subjects' representations which emerge from type-1 learning mechanisms, and, on the other, their knowledge of the genuine abstract ''recoding function'' which defines a type-2 problem.

 The domain-general learning mechanisms elicited in incidental learning situations are of potential interest in many research fields, including language acquisition, object knowledge formation and motor learning. They have been the focus of studies on implicit learning for nearly 40 years. Stemming from a different research tradition, studies on statistical learning carried out in the past 10 years after the seminal studies by Saffran and collaborators, appear to be closely related, and the similarity between the two approaches is strengthened further by their recent evolution. However, implicit learning and statistical learning research favor different interpretations, focusing on the formation of chunks and statistical computations, respectively. We examine these differing approaches and suggest that this divergence opens up a major theoretical challenge for future studies.

-----------------------
 We have developed an experimental platform that allows a large number of human participants to interact in real time within a common virtual world. Within this environment, human participants foraged for resources distributed in two spatially separated pools. In addition to varying the relative replenishment rate for the two pools (50-50, 65-35, or 80-20), we manipulated whether the participants could see each other and the entire resource distribution or had their vision restricted to resources at their own location. Two empirical deviations from an optimal distribution of the participants were found. First, the participants were more scattered within a resource pool than the resources were themselves. Second, there was systematic underutilization of the richer pool. For example, the participants distributed themselves 73% and 27% to resource pools that had replenishment rates of 80% and 20%, respectively. In addition, there were oscillations in the harvesting rate of the pools across time, revealed by a Fourier analysis with prominent power near 50 sec per cycle. The suboptimalities and oscillations were more apparent when the locations of the participants and the food were not visible. Individual participant knowledge thus affects the efficiency with which a population of participants harvests resources.

 The allocation of human participants to resources was studied by observing the population dynamics of people interacting in real time within a common virtual world. Resources were distributed in two spatially separated pools with varying relative reinforcement rates (50-50, 65-35, or 80-20). We manipulated whether the participants could see each other and the distribution of the resources. When the participants could see each other but not the resources, the richer pool was underutilized. When the participants could see the resources but not each other, the richer pool was overutilized. In conjunction with prior experiments that correlated the visibility of agents and resources (Goldstone &amp; Ashpole, 2004), these results indicate that participants' foraging decisions are influenced by both forager and resource information. The results suggest that the presence of a crowd at a resource is a deterring, rather than an attractive, factor. Both fast and slow oscillations in the harvesting rates of the pools across time were revealed by Fourier analyses. The slow waves of crowd migration were most prevalent when the resources were invisible, whereas the fast cycles were most prevalent when the resources were visible and the participants were invisible.

-----------------------
 Human cognition requires coping with a complex and uncertain world. This suggests that dealing with uncertainty may be the central challenge for human reasoning. In Bayesian Rationality we argue that probability theory, the calculus of uncertainty, is the right framework in which to understand everyday reasoning. We also argue that probability theory explains behavior, even on experimental tasks that have been designed to probe people's logical reasoning abilities. Most commentators agree on the centrality of uncertainty; some suggest that there is a residual role for logic in understanding reasoning; and others put forward alternative formalisms for uncertain reasoning, or raise specific technical, methodological, or empirical challenges. In responding to these points, we aim to clarify the scope and limits of probability and logic in cognitive science; explore the meaning of the "rational" explanation of cognition; and re-evaluate the empirical case for Bayesian rationality.

 According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford &amp; Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of "deductive" reasoning is tackled head-on: It is argued that purportedly "logical" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed. The book considers the implications of this work, and the wider "probabilistic turn" in cognitive science and artificial intelligence, for understanding human rationality.

-----------------------
 We have developed an experimental platform that allows a large number of human participants to interact in real time within a common virtual world. Within this environment, human participants foraged for resources distributed in two spatially separated pools. In addition to varying the relative replenishment rate for the two pools (50-50, 65-35, or 80-20), we manipulated whether the participants could see each other and the entire resource distribution or had their vision restricted to resources at their own location. Two empirical deviations from an optimal distribution of the participants were found. First, the participants were more scattered within a resource pool than the resources were themselves. Second, there was systematic underutilization of the richer pool. For example, the participants distributed themselves 73% and 27% to resource pools that had replenishment rates of 80% and 20%, respectively. In addition, there were oscillations in the harvesting rate of the pools across time, revealed by a Fourier analysis with prominent power near 50 sec per cycle. The suboptimalities and oscillations were more apparent when the locations of the participants and the food were not visible. Individual participant knowledge thus affects the efficiency with which a population of participants harvests resources.

 The allocation of human participants to resources was studied by observing the population dynamics of people interacting in real time within a common virtual world. Resources were distributed in two spatially separated pools with varying relative reinforcement rates (50-50, 65-35, or 80-20). We manipulated whether the participants could see each other and the distribution of the resources. When the participants could see each other but not the resources, the richer pool was underutilized. When the participants could see the resources but not each other, the richer pool was overutilized. In conjunction with prior experiments that correlated the visibility of agents and resources (Goldstone &amp; Ashpole, 2004), these results indicate that participants' foraging decisions are influenced by both forager and resource information. The results suggest that the presence of a crowd at a resource is a deterring, rather than an attractive, factor. Both fast and slow oscillations in the harvesting rates of the pools across time were revealed by Fourier analyses. The slow waves of crowd migration were most prevalent when the resources were invisible, whereas the fast cycles were most prevalent when the resources were visible and the participants were invisible.

-----------------------
 Individual differences in how mothers structure reminiscing about shared past experiences with their preschool children are related to children's developing autobiographical memory skills and understanding of self and emotion. More specifically, mothers who engage in highly elaborative reminiscing have children who come to tell more coherent and emotionally expressive autobiographical narratives, and these children also show better understanding of self and are better able to regulate emotion than children of less elaborative mothers. This body of research is reviewed and relations between maternal reminiscing style and children's developing self and emotional understanding are explicated.

 Objective The primary purpose of our study was to examine the relationship between parental coping and children with asthmas psychological well-being and asthma-related quality of life (ArQL). Methods Eighty-nine motherchild dyads with a child with asthma ranging in age from 8 to 12-years old participated. During baseline and 6 month follow-up visits, children completed questionnaires assessing anxiety and ArQL; mothers completed questionnaires assessing coping, ArQL, an index of recent stressors, and demographic/medical history forms. Results Mothers who relied more on active coping strategies at baseline had children with better ArQL 6 months later, and those who relied on more avoidance coping strategies at baseline had children with poorer ArQL of life 6 months later. Conclusions These results reveal that maternal coping plays an important role in the ArQL of children with asthma. Implications for interventions aimed at improving the physical and mental health of children with asthma are discussed.

-----------------------
 Objective, To develop a simple score to help assess the presence or absence of infection in critically ill patients using routinely available variables.
 Objective. To determine the incidence of body temperature (BT) alterations in critically ill patients, and their relationship with infection and outcome. Design. Prospective, observational study. Setting. Thirty-one bed, medico-surgical department of intensive care. Patients. Adult patients admitted consecutively to the ICU for at least 24 h, during 6 summer months. Interventions. None. esults. Fever (BTgreater than or equal to38.3degreesC) occurred in 139 (28.2%) patients and hypothermia (BTless than or equal to36degreesC) in 45 (9.1%) patients, at some time during the ICU stay. Fever was present in 52 of 100 (52.0%) infected patients without septic shock, and in 24 of 38 (63.2%) patients with septic shock. Hypothermia occurred in 5 of 100 (5.0%) infected patients without septic shock and in 5 of 38 (13.1%) patients with septic shock. Patients with hypothermia and fever had higher Sequential Organ Failure Assessment (SOFA) scores on admission (6.3+/-3.7 and 6.4+/-3.3 vs 4.6+/-3.2; p&lt;0.01), maximum SOFA scores during ICU stay (7.6+/-5.2 and 8.2+/-4.7 vs 5.4+/-3.8; p&lt;0.01) and mortality rates (33.3 and 35.3% vs 10.3%; p&lt;0.01). The length of stay (LOS) was longer in febrile patients than in hypothermic and normothermic (36degreesC&lt;BT&lt;38.3degreesC) patients [median 6 (1-57) vs 5 (2-28) and 3 (1-33) days, p=0.02 and p=0.01, respectively). Among the septic patients hypothermic patients were older than febrile patients (69+/-9 vs 54+/-7 years, p=0.01). Patients with septic shock had a higher mortality if they were hypothermic than if they were febrile (80 vs 50%, p&lt;0.01). onclusions. Both hypothermia and fever are associated with increased morbidity and mortality rates. Patients with hypothermia have a worse prognosis than those with fever.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 Prior research demonstrates that understanding theory of mind (ToM) is seriously and similarly delayed in late-signing deaf children and children with autism. Are these children simply delayed in timing relative to typical children, or do they demonstrate different patterns of development? The current research addressed this question by testing 145 children (ranging from 3 to 13 years) with deafness, autism, or typical development using a ToM scale. Results indicate that all groups followed the same sequence of steps, up to a point, but that children with autism showed an importantly different sequence of understandings (in the later steps of the progression) relative to all other groups.

 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

-----------------------
 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

 Utterances expressing generic kinds ("birds fly") highlight qualities of a category that are stable and enduring, and thus provide insight into conceptual organization. To explore the role that linguistic input plays in children's production of generic nouns, we observed American and Chinese deaf children whose hearing losses prevented them from learning speech and whose hearing parents had not exposed them to sign. These children develop gesture systems that have language-like structure at many different levels. The specific question we addressed in this study was whether the gesture systems, developed without input from a conventional language model, would contain generics. We found that the deaf children used generics in the gestures they invented, and did so at about the same rate as hearing children growing up in the same cultures and learning English or Mandarin. Moreover, the deaf children produced more generics for animals than for artifacts, a bias found previously in adult English- and Mandarin-speakers and also found in both groups of hearing children in our current study. This bias has been hypothesized to reflect the different conceptual organizations underlying animal and artifact categories. Our results suggest that not only is a language model not necessary for young children to produce generic utterances, but the bias to produce more generics for animals than artifacts also does not require linguistic input to develop. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Background: This study evaluated the effectiveness of an intervention for reading-delayed children in Year-1 classes. Methods: A sample (N = 77) of children drawn from 14 schools representing those with the weakest reading skills were randomly allocated to one of two groups. A 20-week intervention group received the intervention for two consecutive 10-week periods, while a 10-week intervention group only received the intervention for the second 10 weeks of the study. The programme was delivered in daily 20-minute sessions that alternated between small group (N = 3) and individual teaching. The programme combined phoneme awareness training, word and text reading, and phonological linkage exercises. Results: The children receiving the intervention during the first 10-week period made significantly more progress on measures of letter knowledge, single word reading, and phoneme awareness than children not receiving the intervention. However, the children who only received the intervention during the second 10-week period made rapid progress and appeared to catch up with the children who had been given the more prolonged intervention. Failure to respond to the intervention was predicted by poor initial literacy skills and being in receipt of free school meals. Conclusion: A reading intervention programme delivered on a daily basis by trained teaching assistants is an effective intervention for children who show reading delays at the end of their first year in school. However, around one-quarter of the children did not respond to this intervention and these children would appear to need more intensive or more prolonged help to improve their reading skills.

 The authors examine the reading profile in children with Down syndrome by comparing the nonword decoding skills in children with Down syndrome and typically developing children matched for word recognition level. Journal articles published before 04.05.2010 were identified by using the keyword Down* cross-referenced to 'reading', 'literacy', 'decoding', and 'reading comprehension' were selected. A total of eight papers met the criteria for inclusion. Each study was reviewed and coded on both inclusion criteria and coding protocol before the analysis was performed. Children with Down syndrome had equivalent nonword decoding skills to typically developing children matched for word recognition level, but showed deficits on measures of two important underlying skills, vocabulary and phonological awareness. Differences in vocabulary, but not phonological awareness, were predictive of differences in nonword decoding skills. The practical and theoretical implications of these findings are discussed. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Background: This study evaluated the effectiveness of an intervention for reading-delayed children in Year-1 classes. Methods: A sample (N = 77) of children drawn from 14 schools representing those with the weakest reading skills were randomly allocated to one of two groups. A 20-week intervention group received the intervention for two consecutive 10-week periods, while a 10-week intervention group only received the intervention for the second 10 weeks of the study. The programme was delivered in daily 20-minute sessions that alternated between small group (N = 3) and individual teaching. The programme combined phoneme awareness training, word and text reading, and phonological linkage exercises. Results: The children receiving the intervention during the first 10-week period made significantly more progress on measures of letter knowledge, single word reading, and phoneme awareness than children not receiving the intervention. However, the children who only received the intervention during the second 10-week period made rapid progress and appeared to catch up with the children who had been given the more prolonged intervention. Failure to respond to the intervention was predicted by poor initial literacy skills and being in receipt of free school meals. Conclusion: A reading intervention programme delivered on a daily basis by trained teaching assistants is an effective intervention for children who show reading delays at the end of their first year in school. However, around one-quarter of the children did not respond to this intervention and these children would appear to need more intensive or more prolonged help to improve their reading skills.

 The authors examine the reading profile in children with Down syndrome by comparing the nonword decoding skills in children with Down syndrome and typically developing children matched for word recognition level. Journal articles published before 04.05.2010 were identified by using the keyword Down* cross-referenced to 'reading', 'literacy', 'decoding', and 'reading comprehension' were selected. A total of eight papers met the criteria for inclusion. Each study was reviewed and coded on both inclusion criteria and coding protocol before the analysis was performed. Children with Down syndrome had equivalent nonword decoding skills to typically developing children matched for word recognition level, but showed deficits on measures of two important underlying skills, vocabulary and phonological awareness. Differences in vocabulary, but not phonological awareness, were predictive of differences in nonword decoding skills. The practical and theoretical implications of these findings are discussed. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Background: This study evaluated the effectiveness of an intervention for reading-delayed children in Year-1 classes. Methods: A sample (N = 77) of children drawn from 14 schools representing those with the weakest reading skills were randomly allocated to one of two groups. A 20-week intervention group received the intervention for two consecutive 10-week periods, while a 10-week intervention group only received the intervention for the second 10 weeks of the study. The programme was delivered in daily 20-minute sessions that alternated between small group (N = 3) and individual teaching. The programme combined phoneme awareness training, word and text reading, and phonological linkage exercises. Results: The children receiving the intervention during the first 10-week period made significantly more progress on measures of letter knowledge, single word reading, and phoneme awareness than children not receiving the intervention. However, the children who only received the intervention during the second 10-week period made rapid progress and appeared to catch up with the children who had been given the more prolonged intervention. Failure to respond to the intervention was predicted by poor initial literacy skills and being in receipt of free school meals. Conclusion: A reading intervention programme delivered on a daily basis by trained teaching assistants is an effective intervention for children who show reading delays at the end of their first year in school. However, around one-quarter of the children did not respond to this intervention and these children would appear to need more intensive or more prolonged help to improve their reading skills.

 The authors examine the reading profile in children with Down syndrome by comparing the nonword decoding skills in children with Down syndrome and typically developing children matched for word recognition level. Journal articles published before 04.05.2010 were identified by using the keyword Down* cross-referenced to 'reading', 'literacy', 'decoding', and 'reading comprehension' were selected. A total of eight papers met the criteria for inclusion. Each study was reviewed and coded on both inclusion criteria and coding protocol before the analysis was performed. Children with Down syndrome had equivalent nonword decoding skills to typically developing children matched for word recognition level, but showed deficits on measures of two important underlying skills, vocabulary and phonological awareness. Differences in vocabulary, but not phonological awareness, were predictive of differences in nonword decoding skills. The practical and theoretical implications of these findings are discussed. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Generalizing knowledge about nonobvious object properties often involves inductive inference. For example, having discovered that a particular object can float, we may infer that other objects of similar appearance likewise float. In this research, exploratory play served as a window on early inductive capability. In the first study, 48 infants between 9 and 16 months explored pairs of novel toys in 2 test conditions: violated expectation (two similar toys were presented in sequence, the first toy produced an interesting nonobvious property, such as a distinctive sound or movement, while the second toy was invisibly altered such that it failed to produce the nonobvious property available in the first toy), and interest control (two similar-looking toys were presented in sequence, neither of which produced the interesting property). Infants quickly and persistently attempted to reproduce the interesting property when exploring the second toy of the violated expectation condition relative to the first toy of the interest control condition (a baseline estimate) or the second toy of the interest control condition (an estimate of simple disinterest). The second study, with 40 9-16-month-olds, confirmed these results and also indicated a degree of discrimination on infants' part: Infants seldom expected toys of radically different appearance to possess the same nonobvious property. The findings indicate that infants as young as 9 months can draw simple inferences about nonobvious object properties after only brief experience with just 1 exemplar.

 Prior studies (Gergely et al., 1995; Woodward, 1998) have found that infants focus on the goals of an action over other details. The current studies tested whether infants would distinguish between a behavior that seemed to be goal-directed and one that seemed not to be. Infants in one condition saw an actor grasp one of two toys that sat side by side on a stage, infants in the other condition saw the actor drop her hand onto one-of the toys in a manner that looked unintentional. Once infants had been habituated to these events, they were shown test events in which either the path of motion or the object that was touched had changed. Nine-month-olds differentiated between these two actions. When they saw the actor grasp the toy, they looked longer on trials with a change in goal object than on trials with a change in path. When they saw the actor drop her hand onto the toy, they looked equally at the two test events. These findings did not result from infants being more interested in grasping as compared to inert hands. In a second study, 5-month-old infants showed patterns similar to those seen in 9-month-olds. These findings have implications for theories of the development of the concept of intention. They argue against the claim that infants are innately predisposed to interpret any motion of an animate agent as intentional.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 Working memory plays a crucial role in supporting learning, with poor progress in reading and mathematics characterizing children with low memory skills. This study investigated whether these problems can be overcome by a training program designed to boost working memory. Children with low working memory skills were assessed on measures of working memory, IQ and academic attainment before and after training on either adaptive or non-adaptive versions of the program. Adaptive training that taxed working memory to its limits was associated with substantial and sustained gains in working memory, with age-appropriate levels achieved by the majority of children. Mathematical ability also improved significantly 6 months following adaptive training. These findings indicate that common impairments in working memory and associated learning difficulties may be overcome with this behavioral treatment.

 The present study investigates how working memory and fluid intelligence are related in young children and how these links develop over time. The major aim is to determine which aspect of the working memory system-short-term storage or cognitive control drives the relationship with fluid intelligence. A sample of 119 children was followed from kindergarten to second grade and completed multiple assessments of working memory, short-term memory, and fluid intelligence. The data showed that working memory, short-term memory, and fluid intelligence were highly related but separate constructs in young children. The results further showed that when the common variance between working memory and short-term memory was controlled, the residual working memory factor manifested significant links with fluid intelligence whereas the residual short-term memory factor did not. These findings suggest that in young children cognitive control mechanisms rather than the storage component of working memory span tasks are the source of their link with fluid intelligence. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 Human performance defines the standard that machine learning systems aspire to in many areas, including learning language. This suggests that studying human cognition may be a good way to develop better learning algorithms, as well as providing basic insights into how the human mind works. However, in order for ideas to flow easily from cognitive science to computer science and vice versa, we need a common framework for describing human and machine learning. I will summarize recent work exploring the hypothesis that probabilistic models of cognition, which view learning as a form of statistical inference, provide such a framework, including results that illustrate how novel ideas from statistics can inform cognitive science. Specifically, I will talk about how probabilistic models can be used to identify the assumptions of learners, learn at different levels of abstraction, and link the inductive biases of individuals to cultural universals.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 4 studies investigated the broad claim that preschoolers understand biological inheritance. In Study 1, 4-7-year-old children were told a story in which a boy was born to one man and adopted by another. The biological father was described as having one set of features (e.g., green eyes) and the adoptive father as having another (e.g., brown eyes). Subjects were asked which man the boy would resemble when he grew up. Preschoolers showed little understanding that selective chains of processes mediate resemblance to parents. It was not until age 7 that children substantially associated the boy with his biological father on physical features and his adoptive father on beliefs. That is, it was not until age 7 that children demonstrated that they understood birth as part of a process selectively mediating the acquisition of physical traits and learning or nurturance as mediating the acquisition of beliefs. In Study 2, subjects were asked whether, as a boy grew up, various of his features could change. Children generally shared our adult intuitions, indicating that their failure in Study 1 was not due to their having a different sense of what features can change. Studies 3 and 4 replicated Study 1, with stories involving mothers instead of fathers and with lessened task demands. Taken together, the results of the 4 studies refute the claim that preschoolers understand biological inheritance. The findings are discussed in terms of whether children understand biology as an autonomous cognitive domain.

 This is a study of the feasibility of teaching a causal theoretical understanding of biological inheritance to young children. The phenomenon of biological inheritance is a promising one in which to study children's learning of a system of knowledge, for it engages concerns over how understanding individual facts is related to having more broadly coherent frameworks of understanding. For those preschoolers who do not already understand biological inheritance, the construct ion of such an understanding could well entail a reorganization of which facts are at the causal core of their explanations of how and why offspring tend to resemble their parents. The present study was of a pre-test-intervention-post-test design involving tasks based on Johnson and Solomon (1997) and on Springer and Keil (1989) and building on Springer (1995). The intervention focused on three factors: first, children were motivated to seek change in their understanding (i.e. they were made aware that they lacked an explanation of why offspring resemble their parents); secondly, they were supplied with relevant facts (e.g. that babies come from their mothers' bellies); and thirdly, they were presented with a conceptual peg (i.e. a rudimentary notion of genes) about which they could organize and better recruit those facts. Children who had taken parr in the training were subsequently more likely to make adult-like judgments than those who had not.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 Animals of many species use the geometric shape of an enclosed rectangular environment to reorient, even in the presence of a more informative featural cue. Manipulating the rearing environment affects performance on spatial tasks, but its effect on the use of geometric versus featural navigational cues is unknown. Our study varied the geometric information available in the rearing environment (circular vs. rectangular rearing tanks) of convict cichlids (Archocentrus nigrofasciatus) and tested their use of navigational cues. All the fish used geometric information to navigate when no features were present. When features were present, the fish used geometric and featural information separately. If cues were in conflict, fish raised in a circular tank showed significantly less use of geometric information than fish raised in a rectangular tank. Thus, the ability to use geometry to navigate does not require exposure to angular geometric cues during rearing, though rearing environment affects the dominance of featural and geometric cues.

 Since Cheng (Cognition 23:149-178, 1986) first proposed the "geometric module" in rats, a great deal of research has focused on how other species use geometric information and how geometric encoding may differ across species. Here, hand-reared and wild-caught black-capped chickadees and wild-caught mountain chickadees searched for food hidden in one corner in a rectangular environment. Previous research has shown that mountain chickadees do not spontaneously encode geometric information when a salient feature is present near the goal location. Using a slightly different training and testing procedure, we found that both hand-reared and wild-caught black-capped chickadees encoded geometric information, even in the presence of a salient landmark. Some, but not all, mountain chickadees also encoded geometric information. Overall, our results suggest that use of geometric information may be a less preferred strategy for mountain chickadees than for either wild-caught or hand-reared black-capped chickadees. To our knowledge, this is the first direct interspecies comparison of use of geometric information in a spatial orientation task.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 An experiment with a multiple-cue judgment task tested the hypothesis that humans can only abstract explicit representations of cue-criterion relations when the cues are related to the criterion by an additive function. It is proposed that the sequential and capacity-constrained nature of controlled, explicit thought can only induce and execute linear additive cue integration; non-additive environments require exemplar memory. The results showed that an additive task induced processes of cue abstraction and cue integration, while a multiplicative task induced exemplar processes. The results suggest flexible interplay between distinct representation-levels, a preference to abstract explicit "rules" whenever possible, although this capacity is constrained to additive cue-criterion relations.

 The majority of previous studies on multiple-cue judgment with continuous cues have involved comparisons between judgments and multiple linear regression models that integrated cues into a judgment. The authors present an experiment indicating that in a judgment task with additive combination of multiple continuous cues, people indeed displayed abstract knowledge of the cue criterion relations that was mentally integrated into a judgment, but in a task with multiplicative combination of continuous cues, people instead relied on retrieval of memory traces of similar judgment cases (exemplars). These results suggest that people may adopt qualitatively distinct forms of knowledge, depending on the structure of a multiple-cue judgment task. The authors discuss implications for theories of multiple-cue judgment.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 In this paper we examine the way disjunctive choices work in conversational context. We focus on disjunctive deontic rules, such as 'you must either submit an essay or attend an exam'. According to the Gricean maxim of orderliness, a derivative of the maxim of manner, people should interpret the first-mentioned option as the one preferred by the speaker. Two experiments demonstrated order bias, a non-logical preference for the first-mentioned option in the disjunctive situations of obligatory choice and free choice permissions, but not in the non-disjunctive situation of obligatory consequences. Order bias influences interpretations of the speaker's preferences, but not of the recipient's preferences; it was mainly a function of the nature of the situation, while linguistic form had only limited effect. We offer a general theoretical framework for order bias as a communicative device, constraining both interpretation and generation of utterances.

 The ability to entertain possibilities and draw inferences about them is essential to human intelligence. We examine the hypothesis that conditional if-then statements trigger a mental simulation process in which people suppose the antecedent (if statement) to be true and evaluate the consequent (then statement) in that context. On the assumption that supposing an event to be true increases belief that the event has occurred or will occur, this hypothesis is consistent with the claim that evaluating a conditional will heighten belief in its antecedent more than in its consequent. Two experiments, employing conditionals of the form If animal A has property X, then animal B will have property X, in which X was a property that people could not readily relate to the animals, supported this claim. The effect was stronger following the evaluation of conditionals with dissimilar animal categories.

-----------------------
 This paper presents a cognitive account of the process of evaluating scientific data. Our account assumes that when individuals evaluate data, they construct a mental model of a data-interpretation package, in which the data and theoretical interpretations of the data are integrated. We propose that individuals attempt to discount data by seeking alternative explanations for events within the mental model; data-interpretation packages are accepted when the individual cannot find alternative accounts for these events. Our analysis indicates that there are many levels at which data-interpretation packages can be accepted or denied.

 This article reports the results of a study investigating how undergraduates evaluate realistic scientific data in the domains of geology and paleontology. The results are used to test several predictions of a theory of data evaluation, which we call models-of-data theory. Models-of-data theory assumes that when evaluating data, the individual constructs a particular kind of cognitive model that integrates many features of the data with a theoretical interpretation of the data, The individual evaluates the model by attempting to generate alternative causal explanations for the events in the model. We contrast models-of-data theory with other proposals for how data are cognitively represented and show that models-of-data theory gives a good account of the pattern of written evaluations of data produced by the undergraduates in the study, We discuss theoretical and instructional implications of the theory.

-----------------------
 Facial composites are typically constructed by witnesses to crime by describing a suspect's face and then selecting facial features from a kit of parts. Unfortunately, when produced in this way, composites are very poorly identified. In contrast, there is mounting evidence that other, more recognition-based approaches can produce a much better likeness of a suspect. With the EvoFIT system, for example, witnesses are presented with sets of complete faces and a composite is 'evolved' through a process of selection and breeding. The current work serves to augment EvoFIT by developing a set of psychologically useful 'knobs' that allow faces to be manipulated along dimensions such as facial weight, masculinity, and age. These holistic dimensions were implemented by increasing the size and variability of the underlying face model and obtaining perceptual ratings so that the space could be suitably vectorised. Two evaluations suggested that the new dimensions were operating appropriately.

 Face construction by selecting individual facial features rarely produces recognisable images. We have been developing a system called EvoFIT that works by the repeated selection and breeding of complete faces. Here, we explored two techniques. The first blurred the external parts of the face, to help users focus on the important central facial region. The second, manipulated an evolved face using psychologically-useful 'holistic' scales: age, masculinity, honesty, etc. Using face construction procedures that mirrored police work, a large benefit emerged for the holistic scales; the benefit of blurring accumulated over the construction process. Performance was best using both techniques: EvoFITs were correctly named 24.5% on average compared to 4.2% for faces constructed using a typical 'feature' system. It is now possible, therefore, to evolve a fairly recognisable composite from a 2 day memory of a face, the norm for real witnesses. A plausible model to account for the findings is introduced. Copyright (C) 2010 John Wiley &amp; Sons, Ltd.

-----------------------
 The mental model theory of conditional reasoning presented by P. N. Johnson-Laird and R. M. J. Byrne (2002) has recently been the subject of criticisms (e.g., J. St. B. T. Evans, D. E. Over, &amp; S. J. Handley, 2005). The authors argue that the theoretical conflict can be resolved by differentiating 2 kinds of reasoning, reasoning about possibilities given the truth of assertions and reasoning about the truth of assertions given possibilities. The standard mental model theory accounts for the former kind of reasoning but does not adequately account for the latter, contrary to the suppositional approach favored by J. St. B. T. Evans et al. (2005). The authors thus propose a modified mental model theory of conditionals that reconciles the 2 theoretical approaches. It is demonstrated that this theory is able to explain the key findings that have been opposed to the standard theory by J. St. B. T. Evans et al. and makes new predictions that are empirically verified.

 This article presents a developmental dual-process theory of the understanding of conditionals that integrates Evans' heuristic-analytic theory within the revised mental model theory of conditional proposed by Barrouillet, Gauffroy, and Lecas (2008). According to this theory, the interpretation of a conditional sentence is driven by unconscious and implicit heuristic processes that provide individuals with an initial representation that captures its meaning by representing the cases that make it true. This initial model can be enriched with additional models (a process named fleshing out within the mental model theory) through the intervention of conscious and demanding analytic processes. Being optional, these processes construct representations of cases that are only compatible with the conditional, leaving its truth-value indeterminate when they occur. Because heuristic processes are relatively immune to developmental changes, while analytic processes strongly develop with age, the initial model remains stable through development whereas the number of additional models that can be constructed increases steadily. Thus, the dual-process mental model theory predicts in which cases conditionals will be deemed true, indeterminate, or false and how these cases evolve with age. These predictions were verified in children, adolescents and adults who were asked to evaluate the truth value and the probability of several types of conditionals. The results reveal a variety of developmental trajectories in the way different conditionals are interpreted, which can all be accounted for by our revised mental model theory. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 Implicit motor sequence learning refers to an important human ability to acquire new motor skills through the repeated performance of a motor sequence. This learning process is characterized by slow, incremental gains of motor performance. The present fMRI study was developed to better delineate the areas supporting these temporal dynamics of learning. By using the serial color matching paradigm, our study focused on the motor level of sequence learning and tracked the time course of learning-related neural changes. Imaging results showed a significant contribution of the left anterior hippocampus in an early sequence acquisition stage (first scanning session) as well as during a later stage with stabilized learning effects (second scanning session). Hippocampal activation significantly correlated with the behavioral learning process and was affected by a change of the motor sequence. These results suggest a strong involvement of the hippocampus in implicit motor sequence learning. On the other hand, a very extensive and bilateral neural network of parietal, temporal and frontal cortical areas (including SMA, pre-SMA) together with parts of the cerebellum and striatum were found to play a role during random visuo-motor task performance.

 The dynamics of the neural network that underlies learning transitive structures of an ordered sequence remains poorly understood. To address this, in the present study we used fMRI to track the time course of transitive inference learning. The hippocampus and the angular gyrus were each shown to be closely related to the learning trajectory, but differentially so. Hippocampal activity was shown to consistently increase with learning but no correlation was found between performance and hippocampal activation, suggesting a general role for the hippocampus. Left angular gyrus activity was also found to consistently increase with training, but, in addition, correlated significantly with behavioral performance. This suggests an involvement of the angular gyros in learning the ordinal associations between the stimuli. (c) 2007 Elsevier Inc. All rights reserved.

-----------------------
 The home-field disadvantage refers to the disadvantage inherent in research that takes a particular cultural group as the starting point or standard for research, including cross-cultural research. We argue that home-field status is a serious handicap that often pushes researchers toward deficit thinking, however good the researchers' intentions may be. In this article, we aim to make this home-field bias more explicit and, in doing so, more avoidable. We discuss three often-overlooked disadvantages that result from this home-field status: the problem of marked versus unmarked culture, the problem of homogenous versus heterogeneous culture, and the problem of regression toward the mean. We also recommend four interventions researchers can apply to avoid the home-field disadvantage or, at the least, attenuate its deleterious effects.

 Using a variation on an experimental approach from biology, we distinguish the influence of sociocultural factors from that of economic, demographic, and ecological factors in environmental management and maintenance. This is important to issues of global environmental change, where there is little empirical research into cultural effects on deforestation and land use. Findings with three groups who live in the same rain-forest habitat and manifest strikingly distinct behaviors, cognitions, and social relations relative to the forest indicate that rational self-interest and institutional constraints may not by themselves account for commons behavior and cultural patternings of cognition are significant. Only the area's last native Itza' Maya (who have few cooperative institutions) show systematic awareness of ecological complexity involving animals, plants, and people and practices clearly favoring forest regeneration. Spanish-speaking immigrants prove closer to native Maya in thought, action, and social networking than immigrant Q'eqchi' Maya (who have highly cooperative institutions). The role of spiritual values and the limitations of rational, utility-based decision theories are explored. Emergent cultural patterns derived statistically from measurements of individual cognitions and behaviors suggest that cultural transmission and formation consist not primarily of shared rules or norms but of complex distributions of causally connected representations across minds.

-----------------------
 Time-accuracy functions for tasks involving single-digit mental addition and subtraction were derived in a sample of 18 younger (mean age = 21.7 years) and 16 older adults (mean age = 68.8 years). Sequential complexity was manipulated by varying the number of operations (5 vs. 10); coordinative complexity was induced by bracketing. Age differences were apparent in the coordinative conditions, even though no age difference was present in the sequential conditions. This indicates that the age difference under conditions of high coordinative demands could not be attributed solely to a decline in basic speed of processing. The Age x Complexity interaction was due to larger onset times and lower asymptotic performance by the older adults in the coordinative conditions but not due to to rate of approach to the asymptote. This implies that coordinative demands do not differentially hurt access from semantic memory in older adults; however, coordinative demands do have disproportionately negative consequences for computation speed and self-monitoring in elder adults.

 This study investigated whether older adults could acquire the ability to perform 2 cognitive operations in parallel in a paradigm in which young adults had been shown to be able to do so (K. Oberauer &amp; R. Kliegl, 2004). Twelve young and 12 older adults practiced a numerical and a visuospatial continuous memory updating task in single-task and dual-task conditions for 16 to 24 sessions. After practice, 9 young adults were able to process the 2 tasks without dual-task costs, but none of the older adults had reached the criterion of parallel processing. The results suggest a qualitative difference between young and older adults in how they approach dual-task situations.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 Based on a prototype approach to emotion concepts, two studies were conducted: (1) to identify the mental state words that Indonesian speakers are most certain name emotions (perasaan hati) and (2) to map the hierarchical and family-resemblance structure of the top 124 emotion concepts. As in an earlier study of emotion terms in American English (Shaver, Schwartz, Kirson, &amp; O'Connor, 1987), cluster analysis of sorting data collected in Indonesia revealed five basic-level emotion categories: cinta (love), senang (happiness), marah (anger), kawatir/takut (anxiety/fear), and sedih (sadness). Also in line with the American results, the five basic-level categories formed two large categories at the superordinate level: positive emotions and negative emotions. Each of the five basic-level categories contained several subordinate-level categories, totaling 31 in all. The results suggest that the emotion lexicons, and corresponding conceptualizations of the emotion domain, in Indonesia and the U.S.A. are similar at the superordinate and basic levels but somewhat variable at the subordinate level. This outcome - like other kinds of psychological research on emotions and emotion concepts - suggests that the gross structure of representations of the emotion domain are similar worldwide, perhaps for biological reasons, but that different cultures make different fine-grained distinctions and emphasize different subordinate-level emotion concepts.

 Using a prototype approach to emotion concepts, two studies were conducted in the Basque Country, where an ancient non-Indo-European language is still spoken, to identify the mental state words that Basque speakers are most certain name emotions (emozioak) and to map the hierarchical and family resemblance structure of the most prototypical 124 emotion concepts. Cluster analysis of sorting data collected in the Basque Country revealed five basic level emotion categories similar to those found in American English and Indonesian (love, happiness, anger, sadness, and fear) as well as five other small positive emotion categories. All major categories found at the basic level contained several terms that are not traceable to Romance languages. Also in line with the American and Indonesian results, the basic level categories in Basque fell within two large superordinate categories: positive and negative emotions. Each of the five large basic level categories contained several subordinate level categories. The results suggest that the emotion lexicons, and corresponding conceptualisations of the emotion domain, in the Basque Country, Indonesia, and the US are similar, although there are some important differences.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 We investigated working memory updating performance in younger and older adults before, during. and after 100-day practice. Performance to presentation time (PT) relation was fitted to a negatively accelerated logistic function. Relative to younger adults, older adults showed lower asymptotes at pretest and posttest, and shallower slopes at pretest. Older adults practicing the task with fast PT gained less than older adults practicing the task with slow PT, probably reflecting the persistent use of a selective strategy throughout the 100-day practice period in the fast FT group. These results have implications for designing and evaluating age-comparative working memory training programs.

 The authors examined life-span differences in the maintenance of skilled episodic memory performance by assessing 100 individuals (10-11, 12-13, 21-26, and 66-79 years old) 11 months after termination of an intensive multisession mnemonic training program (Y. Brehmer, S.-C. Li, V. Muller, T. von Oertzen, &amp; U. Lindenberger, 2007). Skill maintenance was tested in 2 follow-up sessions, the first without and the second with mnemonic reinstruction. Younger and older adults' average performance levels were stable across time. In contrast, both younger and older children's memory performance improved beyond originally attained levels. Older adults' performance improved from the first to the second follow-up session, presumably profiting from instruction-induced skill reactivation. Results suggest that (a) skill maintenance is largely intact in healthy older adults, (b) older adults need environmental support to fully reactivate their former skill levels (cf. F. I. M. Craik, 1983), and (c) children adapt a skill learned 11 months ago to their increasing cognitive capabilities.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 Much of young children's symbolic play is heavily scaffolded by adult symbolic action models, which children may imitate, and by adult verbal scripts. The current studies attempted to evaluate 18-35-month-old children's symbolic skills in the absence of such scaffolding. In a study of symbol comprehension, children were rested for their ability to comprehend an adult's use of either a replica object or an associated gesture to communicate which object in an array she want ed. In a study of symbol production, children were given some objects that afforded symbolic manipulations, but without adult symbolic action models or verbal scripts. The results of the two studies converged to suggest chat children below 2 years of age have symbolic skills with gestures, but: nor with objects. It was also found that while children at 26 months were able to use an object as a symbol for another object, they had difficulties when the symbol had another conventional use (e.g. a drinking cup used as a hat). The findings are discussed in terms of DeLoache's dual representation model, and a modification of chat: model is proposed.

 In this study we sought to determine the degree to which 2- to 3-year-old children use objects symbolically in the relative absence of adult symbolic actions or linguistic descriptions, and how the nature of objects influences symbolic play. Results revealed a dramatic increase in children's creative symbolic productions between 2 and 3 years of age, with the tendency to produce symbolic actions influenced to an equal degree by adult symbolic action models and verbal directions. Children of all ages were heavily influenced by the nature of the object to be used as a symbol, with the youngest children using only replica objects as symbols. In a second study, we examined childrens looks to an adult as they engaged in different kinds of activities with objects. The main finding was that children looked to the adult immediately after performing a symbolic action more often than if they performed an instrumental action. We argue for the essentially social nature of symbolic play, both in terms of how children learn to use objects as symbols and in terms of the reasons they do so.

-----------------------
 Thirty-six 2-, 4- and 6-month-old infants were videotaped while interacting with a female adult stranger engaging in either organized or disorganized 1-min peekaboo games. Two-month-old infants gazed and smiled equally at the stranger, regardless of the relative organization of the peekaboo game. In contrast, 4- and 6-month-old infants smiled significantly more and gazed significantly less;in the organized peekaboo condition than in the disorganized peekaboo condition. These results suggest that from a diffuse sensitivity to the presence of a social partner, infants by 4 months develop a new sensitivity to the narrative envelope of protoconversation, in particular the timing and the structure of social exchanges scaffolded by adults. These observations are interpreted as evidence of developing social expectations in the first 6 months of life. This early development is viewed as announcing and preparing the communicative competence that blossoms by the end of the 1st year.

 Two-month-old infants (N = 29) participated in face-to-face interactions with their mothers and with strangers. The contingent responsiveness for smiles and vocalizations, while attending to the partner, was assessed for each partner in both interactions. For smiles and for vocalizations, infants were less responsive to the stranger relative to the mother when the stranger's contingent responsiveness was either more contingent or less contingent than that of the mother. Results are supportive of the hypothesis that young infants develop sensitivities to levels of social contingency present in their maternal interactions, which influence their responsiveness to others.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 Previous research has used cross-linguistic priming methodology with bilingual adults to explore the nature of their syntactic representations. The present paper extends the use of this methodology to bilingual children to investigate the relation between the syntactic structures of their two languages. Specifically, we examined whether the use of passives by the experimenter in one language primed the subsequent use of passives by the child in the other language. Results showed evidence of syntactic priming from Spanish to English: hearing a Spanish sentence containing a passive led to the increase in children's production of the parallel structure in English. However, there was no priming in the other direction : hearing an English sentence containing a passive did not increase children's use of the parallel structure in Spanish. These results provide evidence for both the integration of syntactic representations in bilingual children and the asymmetry of the relation between their two languages.

 Priming methodology was previously used to investigate children's ability to represent abstract syntactic forms. Existing evidence indicates that following exposure to a particular syntactic structure (such as the passive voice), English-speaking children increase their production of that structure with new lexical items. In the present work, we utilize priming methodology to explore whether exposure to passive primes may increase children's production of sentences that have a different structure but share a similar purpose in discourse. We report three studies, two involving English- and Russian-speaking children, and a third involving Russian-speaking adults. Unlike English, Russian offers a variety of syntactic forms that emphasize the patient of a transitive action, thus fulfilling the discourse function of the passive. We found that English speakers increased the use of the particular syntactic form presented in the prime, whereas Russian speakers increased their production of several different syntactic forms used to emphasize the patient of the action.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 Children with unilateral pre- or perinatal brain injury (BI) show remarkable plasticity for language learning. Previous work highlights the important role that lesion characteristics play in explaining individual variation in plasticity in the language development of children with BI. The current study examines whether the linguistic input that children with BI receive from their caregivers also contributes to this early plasticity, and whether linguistic input plays a similar role in children with BI as it does in typically developing (TD) children. Growth in vocabulary and syntactic production is modeled for 80 children (53 TD, 27 BI) between 14 and 46 months. Findings indicate that caregiver input is an equally potent predictor of vocabulary growth in children with BI and in TD children. In contrast, input is a more potent predictor of syntactic growth for children with BI than for TD children. Controlling for input, lesion characteristics (lesion size, type, seizure history) also affect the language trajectories of children with BI. Thus, findings illustrate how both variability in the environment (linguistic input) and variability in the organism (lesion characteristics) work together to contribute to plasticity in language learning.

 Children with pre- or perinatal brain injury (PL) exhibit marked plasticity for language learning. Previous work has focused mostly on the emergence of earlier-developing skills, such as vocabulary and syntax. Here we ask whether this plasticity for earlier-developing aspects of language extends to more complex, later-developing language functions by examining the narrative production of children with PL. Using an elicitation technique that involves asking children to create stories de novo in response to a story stem, we collected narratives from 11 children with PL and 20 typically developing (TD) children. Narratives were analysed for length, diversity of the vocabulary used, use of complex syntax, complexity of the macro-level narrative structure and use of narrative evaluation. Children's language performance on vocabulary and syntax tasks outside the narrative context was also measured. Findings show that children with PL produced shorter stories, used less diverse vocabulary, produced structurally less complex stories at the macro-level, and made fewer inferences regarding the cognitive states of the story characters. These differences in the narrative task emerged even though children with PL did not differ from TD children on vocabulary and syntax tasks outside the narrative context. Thus, findings suggest that there may be limitations to the plasticity for language functions displayed by children with PL, and that these limitations may be most apparent in complex, decontextualized language tasks such as narrative production.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 Context Tobacco smoking is a recognized behavioral risk factor for periodontal disease ( through its systemic effects), and cannabis smoking may contribute in a similar way.

 Three very large. nationally representative samples of married couples were used to examine the relative importance of 3 types of personality effects on relationship and life satisfaction: actor effects, partner effects, and similarity effects. Using data sets from Australia (N = 5,278). the United Kingdom (N = 6,554), and Germany (N = 11.418) provided an opportunity to test whether effects replicated across samples. Actor effects accounted for approximately 6% of the variance in relationship satisfaction and between 10% and 15% of the variance in life satisfaction. Partner effects (which were largest for Agreeableness, Conscientiousness, and Emotional Stability) accounted for between 1% and 3% of the variance in relationship satisfaction and between 1% and 2% of the variance in life satisfaction. Couple similarity consistently explained less than.5% of the variance in life and relationship satisfaction after controlling for actor and partner effects.

-----------------------
 Clinical and epidemiologic studies have established that posttraumatic stress disorder (PTSD) is highly comorbid with other mental disorders. However, such studies have largely relied on adults' retrospective reports to ascertain comorbidity. The authors examined the developmental mental health histories of adults with PTSD using data on mental disorders assessed across the first 3 decades of life among members of the longitudinal Dunedin Multidisciplinary Health and Development Study; 100% of those diagnosed with past-year PTSD and 93.5% of those with lifetime PTSD at age 26 had met criteria for another mental disorder between ages 11 and 21. Most other mental disorders had first onsets by age 15. Of new cases of PTSD arising between ages 26 and 32, 96% had a prior mental disorder and 77% had been diagnosed by age 15. These data suggest PTSD almost always develops in the context of other mental disorders. Research on the etiology of PTSD may benefit from taking lifetime developmental patterns of comorbidity into consideration. Juvenile mental-disorder histories may help indicate which individuals are most likely to develop PTSD in populations at high risk of trauma exposure.

 The present study examined the influence of stable personality traits on romantic relationships using longitudinal data on a large, representative sample of young adults. Relationship experiences (quality, conflict, and abuse) showed relatively small mean-level changes over time and significant levels of rank-order stability, even across different relationship partners. Antecedent personality traits (assessed at age 18) predicted relationship experiences at age 26 and change in relationship experiences from age 21 to 26. Conversely, relationship experiences also predicted change in personality. Importantly, these findings generally held across relationship partners, suggesting that some people tend to be generally happy (or unhappy) across relationships, and this is due, in part, to stable individual differences in personality. Discussion focuses on the broader implications of the findings, in particular the need for relationship researchers to consider the importance of personality for why relationships thrive or fail and the need for personality researchers to consider the impact of relationship experiences on intraindividual personality development.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Two experiments examined the impact of attention on sensorimotor skills. In Experiment 1, experienced golfers putted under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step putting performance. Dual-task condition putting was more accurate. In Experiment 2, right-footed novice and experienced soccer players dribbled through a slalom course under dual-task or skill-focused conditions. When using their dominant right foot, experts again performed better in the dual-task condition. However, when using their less proficient left foot, experts performed better in the skill-focused condition. Novices performed better under skill-focus regardless of foot. Whereas novices and the less-proficient performances of experts benefit from online attentional monitoring of step-by-step performance, high-level skill execution is harmed.

 In two experiments, we examined the attentional mechanisms governing sensorimotor skill execution across levels of expertise, In Experiment 1, novice and expert golfers took a series of putts under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step performance. Novices performed better under skill-focused than under dual-task conditions. Experts showed the opposite pattern. In Experiment 2, novice and expert golfers putted under instructions that emphasized either putting accuracy or speed-the latter intended to reduce the time available to monitor and explicitly adjust execution parameters. Novices putted better under accuracy instructions. Experts were more accurate under speed instructions. In agreement with theories of skill acquisition and automaticity, novice performance is enhanced by conditions that allow for on-line attentional monitoring (e.g., skill-focused or accuracy instructions) in comparison with conditions that prevent explicit attentional control of skill execution (e.g., dud-task or speed constraints). In contrast, the proceduralized skill of experts benefits from environments that limit, rather than encourage, attention to execution.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 The current study examined age differences in maltreated and nonmaltreated children's knowledge of juvenile dependency court vocabulary and proceedings. One hundred and sixty-seven children aged 4-14 years were questioned about their understanding of legal vocabulary and about the content of a story depicting a child involved in dependency court. Age-related increases emerged across all measures of children's legal understanding. Direct experience with the dependency system was also related to the accuracy of children's legal knowledge. Children with greater experience in the dependency system were more knowledgeable than children with no such experience, although even the oldest maltreated children with considerable dependency system experience evidenced some deficits in legal knowledge. Overall, findings suggest children and adolescents involved in dependency proceedings need help understanding some aspects of the dependency process, and this need exists regardless of whether children have been involved in cases ongoing for some time.

 Despite widespread use of self-report measures of adult attachment, relatively little research has explored the predictive utility of these measures in the domain of parent-child relationships. The present study examined the association between self-reported attachment style and parental responsiveness during a stressful event. Children and their parents were observed while children received an inoculation at a county immunization clinic. Children's reactions to the inoculation were rated and parents' responsiveness was assessed with the Emotional Availability Scales (EAS). Results revealed that children of parents scoring high oil self-reported attachment avoidance were more distressed during the inoculation than children of parents scoring low on avoidance. Moreover, parents high on avoidance were less responsive when children were highly distressed, whereas this pattern was reversed among parents scoring low on avoidance. Finally, the influence of adult attachment on parental behavior and children's distress was found to be independent of children's temperament and parental personality. These findings suggest that self-report adult attachment measures may be useful in the domain of parent-child relationships.

-----------------------
 Objective: The current study examined children's eyewitness memory nearly 4 years after an event and the ability of adults to evaluate such memory.

 We examined adults' long-term autobiographical memory for a dramatic life event participating as a child victim in a criminal prosecution because of alleged sexual abuse. The study is unique in several ways, including that we had extensive documentation concerning the sexual abuse allegations, the children's involvement in their legal case, and other factors known to affect long-term memory. Approximately 14 years after their legal involvement, 94 adolescents and young adults, aged 16-30 years, were interviewed about their childhood legal experiences. A subset of the victims had taken the stand in criminal court, and memory for testifying was examined. Results revealed that greater severity of the abuse, a closer relationship to the perpetrator, testifying more frequently, and greater distress were associated with more accurate memory. Findings suggest that both individual and sociocontextual factors must be considered in pursuing a scientific understanding of autobiographical memory for stressful childhood experiences. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 The current study examined age differences in maltreated and nonmaltreated children's knowledge of juvenile dependency court vocabulary and proceedings. One hundred and sixty-seven children aged 4-14 years were questioned about their understanding of legal vocabulary and about the content of a story depicting a child involved in dependency court. Age-related increases emerged across all measures of children's legal understanding. Direct experience with the dependency system was also related to the accuracy of children's legal knowledge. Children with greater experience in the dependency system were more knowledgeable than children with no such experience, although even the oldest maltreated children with considerable dependency system experience evidenced some deficits in legal knowledge. Overall, findings suggest children and adolescents involved in dependency proceedings need help understanding some aspects of the dependency process, and this need exists regardless of whether children have been involved in cases ongoing for some time.

 Despite widespread use of self-report measures of adult attachment, relatively little research has explored the predictive utility of these measures in the domain of parent-child relationships. The present study examined the association between self-reported attachment style and parental responsiveness during a stressful event. Children and their parents were observed while children received an inoculation at a county immunization clinic. Children's reactions to the inoculation were rated and parents' responsiveness was assessed with the Emotional Availability Scales (EAS). Results revealed that children of parents scoring high oil self-reported attachment avoidance were more distressed during the inoculation than children of parents scoring low on avoidance. Moreover, parents high on avoidance were less responsive when children were highly distressed, whereas this pattern was reversed among parents scoring low on avoidance. Finally, the influence of adult attachment on parental behavior and children's distress was found to be independent of children's temperament and parental personality. These findings suggest that self-report adult attachment measures may be useful in the domain of parent-child relationships.

-----------------------
 Objective: The current study examined children's eyewitness memory nearly 4 years after an event and the ability of adults to evaluate such memory.

 We examined adults' long-term autobiographical memory for a dramatic life event participating as a child victim in a criminal prosecution because of alleged sexual abuse. The study is unique in several ways, including that we had extensive documentation concerning the sexual abuse allegations, the children's involvement in their legal case, and other factors known to affect long-term memory. Approximately 14 years after their legal involvement, 94 adolescents and young adults, aged 16-30 years, were interviewed about their childhood legal experiences. A subset of the victims had taken the stand in criminal court, and memory for testifying was examined. Results revealed that greater severity of the abuse, a closer relationship to the perpetrator, testifying more frequently, and greater distress were associated with more accurate memory. Findings suggest that both individual and sociocontextual factors must be considered in pursuing a scientific understanding of autobiographical memory for stressful childhood experiences. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Much of young children's symbolic play is heavily scaffolded by adult symbolic action models, which children may imitate, and by adult verbal scripts. The current studies attempted to evaluate 18-35-month-old children's symbolic skills in the absence of such scaffolding. In a study of symbol comprehension, children were rested for their ability to comprehend an adult's use of either a replica object or an associated gesture to communicate which object in an array she want ed. In a study of symbol production, children were given some objects that afforded symbolic manipulations, but without adult symbolic action models or verbal scripts. The results of the two studies converged to suggest chat children below 2 years of age have symbolic skills with gestures, but: nor with objects. It was also found that while children at 26 months were able to use an object as a symbol for another object, they had difficulties when the symbol had another conventional use (e.g. a drinking cup used as a hat). The findings are discussed in terms of DeLoache's dual representation model, and a modification of chat: model is proposed.

 In this study we sought to determine the degree to which 2- to 3-year-old children use objects symbolically in the relative absence of adult symbolic actions or linguistic descriptions, and how the nature of objects influences symbolic play. Results revealed a dramatic increase in children's creative symbolic productions between 2 and 3 years of age, with the tendency to produce symbolic actions influenced to an equal degree by adult symbolic action models and verbal directions. Children of all ages were heavily influenced by the nature of the object to be used as a symbol, with the youngest children using only replica objects as symbols. In a second study, we examined childrens looks to an adult as they engaged in different kinds of activities with objects. The main finding was that children looked to the adult immediately after performing a symbolic action more often than if they performed an instrumental action. We argue for the essentially social nature of symbolic play, both in terms of how children learn to use objects as symbols and in terms of the reasons they do so.

-----------------------
 The present study examined English-speaking children's tendency to make argument structure overgeneralization errors (e.g., I disappeared it). Children were exposed to several English verbs of fixed transitivity (exclusively intransitive or exclusively transitive) and then asked questions that encouraged them to overgeneralize usage of the verbs. Seventy-two children (24 in each of three age groups: 3, 4/5, and 8 years of age) experienced four actions performed by puppets. Each action had two verbs of similar meaning associated with it in the context of the experimental action: one more familiar to young children and one less familiar. Children at all ages were more likely to overgeneralize usage of verbs that were less familiar to them, supporting the hypothesis that children's usage of verbs in particular construction types becomes entrenched over time. As children solidly learn the transitivity status of particular verbs, they become more reluctant to use those verbs in other argument structure constructions.

 We tested two hypotheses about how English-speaking children learn to avoid making argument structure errors such as Don't giggle me. The first is that children base their usage of verbs on membership in narrow-range semantic classes (Pinker 1989). The second is that children make use of indirect negative evidence in the form of alternative expressions that preempt tendencies to overgeneralize. Ninety-fix children (32 each at 2.5, 4.5, and 6/7 years of age) were introduced to two nonce verbs, one as a transitive verb and one as an intransitive verb. One verb was from a semantic class that can be used both transitively and intransitively while the other was from a fixed transitivity class. Half of the children were given preempting alternatives with bath verbs; for example, they heard a verb in a simple transitive construction (as in Ernie's meeking the car) and then they also heard it in a passive construction-which enabled them to answer the question 'What's happening with the car?' with It's getting meeked (rather than! generalizing to the intransitive construction with It's meeking). We found empirical support for the constraining role of verb classes and of preemption, but only for children 4.5 years of age and older. Results are discussed in terms of a model of syntactic development in which children begin with lexically specific linguistic constructions and only gradually learn to differentiate verbs as lexical items from argument structure constructions as abstract linguistic entities.

-----------------------
 Individual differences in how mothers structure reminiscing about shared past experiences with their preschool children are related to children's developing autobiographical memory skills and understanding of self and emotion. More specifically, mothers who engage in highly elaborative reminiscing have children who come to tell more coherent and emotionally expressive autobiographical narratives, and these children also show better understanding of self and are better able to regulate emotion than children of less elaborative mothers. This body of research is reviewed and relations between maternal reminiscing style and children's developing self and emotional understanding are explicated.

 Objective The primary purpose of our study was to examine the relationship between parental coping and children with asthmas psychological well-being and asthma-related quality of life (ArQL). Methods Eighty-nine motherchild dyads with a child with asthma ranging in age from 8 to 12-years old participated. During baseline and 6 month follow-up visits, children completed questionnaires assessing anxiety and ArQL; mothers completed questionnaires assessing coping, ArQL, an index of recent stressors, and demographic/medical history forms. Results Mothers who relied more on active coping strategies at baseline had children with better ArQL 6 months later, and those who relied on more avoidance coping strategies at baseline had children with poorer ArQL of life 6 months later. Conclusions These results reveal that maternal coping plays an important role in the ArQL of children with asthma. Implications for interventions aimed at improving the physical and mental health of children with asthma are discussed.

-----------------------
 Individual differences in how mothers structure reminiscing about shared past experiences with their preschool children are related to children's developing autobiographical memory skills and understanding of self and emotion. More specifically, mothers who engage in highly elaborative reminiscing have children who come to tell more coherent and emotionally expressive autobiographical narratives, and these children also show better understanding of self and are better able to regulate emotion than children of less elaborative mothers. This body of research is reviewed and relations between maternal reminiscing style and children's developing self and emotional understanding are explicated.

 Objective The primary purpose of our study was to examine the relationship between parental coping and children with asthmas psychological well-being and asthma-related quality of life (ArQL). Methods Eighty-nine motherchild dyads with a child with asthma ranging in age from 8 to 12-years old participated. During baseline and 6 month follow-up visits, children completed questionnaires assessing anxiety and ArQL; mothers completed questionnaires assessing coping, ArQL, an index of recent stressors, and demographic/medical history forms. Results Mothers who relied more on active coping strategies at baseline had children with better ArQL 6 months later, and those who relied on more avoidance coping strategies at baseline had children with poorer ArQL of life 6 months later. Conclusions These results reveal that maternal coping plays an important role in the ArQL of children with asthma. Implications for interventions aimed at improving the physical and mental health of children with asthma are discussed.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

 Utterances expressing generic kinds ("birds fly") highlight qualities of a category that are stable and enduring, and thus provide insight into conceptual organization. To explore the role that linguistic input plays in children's production of generic nouns, we observed American and Chinese deaf children whose hearing losses prevented them from learning speech and whose hearing parents had not exposed them to sign. These children develop gesture systems that have language-like structure at many different levels. The specific question we addressed in this study was whether the gesture systems, developed without input from a conventional language model, would contain generics. We found that the deaf children used generics in the gestures they invented, and did so at about the same rate as hearing children growing up in the same cultures and learning English or Mandarin. Moreover, the deaf children produced more generics for animals than for artifacts, a bias found previously in adult English- and Mandarin-speakers and also found in both groups of hearing children in our current study. This bias has been hypothesized to reflect the different conceptual organizations underlying animal and artifact categories. Our results suggest that not only is a language model not necessary for young children to produce generic utterances, but the bias to produce more generics for animals than artifacts also does not require linguistic input to develop. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

 Utterances expressing generic kinds ("birds fly") highlight qualities of a category that are stable and enduring, and thus provide insight into conceptual organization. To explore the role that linguistic input plays in children's production of generic nouns, we observed American and Chinese deaf children whose hearing losses prevented them from learning speech and whose hearing parents had not exposed them to sign. These children develop gesture systems that have language-like structure at many different levels. The specific question we addressed in this study was whether the gesture systems, developed without input from a conventional language model, would contain generics. We found that the deaf children used generics in the gestures they invented, and did so at about the same rate as hearing children growing up in the same cultures and learning English or Mandarin. Moreover, the deaf children produced more generics for animals than for artifacts, a bias found previously in adult English- and Mandarin-speakers and also found in both groups of hearing children in our current study. This bias has been hypothesized to reflect the different conceptual organizations underlying animal and artifact categories. Our results suggest that not only is a language model not necessary for young children to produce generic utterances, but the bias to produce more generics for animals than artifacts also does not require linguistic input to develop. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

 Utterances expressing generic kinds ("birds fly") highlight qualities of a category that are stable and enduring, and thus provide insight into conceptual organization. To explore the role that linguistic input plays in children's production of generic nouns, we observed American and Chinese deaf children whose hearing losses prevented them from learning speech and whose hearing parents had not exposed them to sign. These children develop gesture systems that have language-like structure at many different levels. The specific question we addressed in this study was whether the gesture systems, developed without input from a conventional language model, would contain generics. We found that the deaf children used generics in the gestures they invented, and did so at about the same rate as hearing children growing up in the same cultures and learning English or Mandarin. Moreover, the deaf children produced more generics for animals than for artifacts, a bias found previously in adult English- and Mandarin-speakers and also found in both groups of hearing children in our current study. This bias has been hypothesized to reflect the different conceptual organizations underlying animal and artifact categories. Our results suggest that not only is a language model not necessary for young children to produce generic utterances, but the bias to produce more generics for animals than artifacts also does not require linguistic input to develop. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

 This research examined children's questions and the reactions to the answers they receive in conversations with adults. If children actively seek explanatory knowledge, they should react differently depending on whether they receive a causal explanation. Study 1 examined conversations following 6 preschoolers' (ages 2-4 years) causal questions in naturalistic situations (using the Child Language Data Exchange System [CHILDES] database). Children more often agreed and asked follow-up questions following adult explanations and, conversely, more often reasked their original question and provided their own explanation following nonexplanations. Study 2 replicated these patterns within an experimental task in 42 children ages 3-5 years. Children's reactions following explanatory versus nonexplanatory information confirm that young children are motivated to seek causal information actively and use specific conversational strategies to obtain it.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Research indicates that young children, unlike adults, have a generalized tendency to view not only artifacts but also living and nonliving natural phenomena as existing for a purpose. To further understand this tendency's origin, the authors explored parents' propensity to invoke teleological explanation during explanatory conversations with their children. Over 2 weeks. Mexican-descent mothers were interviewed about question-answer exchanges with their preschool children. Analyses revealed that children asked more about biological and social phenomena than about artifacts or nonliving natural phenomena. with most questions ambiguous as to whether they were requests for causal or teleological explanations. In responding to these ambiguous questions, parents generally invoked causal rather than teleological explanations. The tendency to favor causal explanation was confirmed by analyses of transcripts from a longitudinal study of spontaneous speech in a father-son dyad. These results suggest that children's bias toward teleological explanation does not straightforwardly derive from parent explanation.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 Working memory plays a crucial role in supporting learning, with poor progress in reading and mathematics characterizing children with low memory skills. This study investigated whether these problems can be overcome by a training program designed to boost working memory. Children with low working memory skills were assessed on measures of working memory, IQ and academic attainment before and after training on either adaptive or non-adaptive versions of the program. Adaptive training that taxed working memory to its limits was associated with substantial and sustained gains in working memory, with age-appropriate levels achieved by the majority of children. Mathematical ability also improved significantly 6 months following adaptive training. These findings indicate that common impairments in working memory and associated learning difficulties may be overcome with this behavioral treatment.

 The present study investigates how working memory and fluid intelligence are related in young children and how these links develop over time. The major aim is to determine which aspect of the working memory system-short-term storage or cognitive control drives the relationship with fluid intelligence. A sample of 119 children was followed from kindergarten to second grade and completed multiple assessments of working memory, short-term memory, and fluid intelligence. The data showed that working memory, short-term memory, and fluid intelligence were highly related but separate constructs in young children. The results further showed that when the common variance between working memory and short-term memory was controlled, the residual working memory factor manifested significant links with fluid intelligence whereas the residual short-term memory factor did not. These findings suggest that in young children cognitive control mechanisms rather than the storage component of working memory span tasks are the source of their link with fluid intelligence. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 Working memory plays a crucial role in supporting learning, with poor progress in reading and mathematics characterizing children with low memory skills. This study investigated whether these problems can be overcome by a training program designed to boost working memory. Children with low working memory skills were assessed on measures of working memory, IQ and academic attainment before and after training on either adaptive or non-adaptive versions of the program. Adaptive training that taxed working memory to its limits was associated with substantial and sustained gains in working memory, with age-appropriate levels achieved by the majority of children. Mathematical ability also improved significantly 6 months following adaptive training. These findings indicate that common impairments in working memory and associated learning difficulties may be overcome with this behavioral treatment.

 The present study investigates how working memory and fluid intelligence are related in young children and how these links develop over time. The major aim is to determine which aspect of the working memory system-short-term storage or cognitive control drives the relationship with fluid intelligence. A sample of 119 children was followed from kindergarten to second grade and completed multiple assessments of working memory, short-term memory, and fluid intelligence. The data showed that working memory, short-term memory, and fluid intelligence were highly related but separate constructs in young children. The results further showed that when the common variance between working memory and short-term memory was controlled, the residual working memory factor manifested significant links with fluid intelligence whereas the residual short-term memory factor did not. These findings suggest that in young children cognitive control mechanisms rather than the storage component of working memory span tasks are the source of their link with fluid intelligence. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 We conducted three experiments to investigate how opportunities to view objects together in time influence memory for location. Children and adults learned the locations of 20 objects marked by dots on the floor of an open, square box. During learning, participants viewed the objects either simultaneously or in isolation. At test, participants replaced the objects without the aid of the dots. Experiment I showed that when the box was divided into quadrants and the objects in each quadrant were categorically related, 7-, 9-, and 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only 11-year-olds and adults in the isolated viewing condition exhibited categorical bias. Experiment 2 showed that when the objects were categorically related but no boundaries were present, 11-year-olds and adults in the simultaneous viewing condition exhibited categorical bias, but only adults showed bias in the isolated viewing condition. Experiment 3 revealed that adults exhibited bias in both simultaneous and isolated viewing conditions when boundaries were present but the objects were not related. These findings suggest that opportunities to see objects together in time interact with cues available for grouping objects to help children form spatial groups.

 The authors investigated how 3- and 4-year-old children and adults use relative distance to judge nearbyness. Participants judged whether several blocks were by a landmark. The absolute and relative distance of the blocks from the landmark varied. In Experiment 1, judgments of nearbyness decreased as the distance from the landmark increased, particularly for 4-year-olds and adults. In Experiment 2, 4-year-olds and adults were more likely to judge objects at an intermediate distance as by the landmark when intervening objects were absent than when intervening objects were present. In Experiment 3, participants of all ages were more likely to judge objects at a short distance as by the landmark when intervening objects were absent. Reliance on relative distance to judge nearbyness becomes more systematic and applicable to larger spatial extents across development.

-----------------------
 4 studies investigated the broad claim that preschoolers understand biological inheritance. In Study 1, 4-7-year-old children were told a story in which a boy was born to one man and adopted by another. The biological father was described as having one set of features (e.g., green eyes) and the adoptive father as having another (e.g., brown eyes). Subjects were asked which man the boy would resemble when he grew up. Preschoolers showed little understanding that selective chains of processes mediate resemblance to parents. It was not until age 7 that children substantially associated the boy with his biological father on physical features and his adoptive father on beliefs. That is, it was not until age 7 that children demonstrated that they understood birth as part of a process selectively mediating the acquisition of physical traits and learning or nurturance as mediating the acquisition of beliefs. In Study 2, subjects were asked whether, as a boy grew up, various of his features could change. Children generally shared our adult intuitions, indicating that their failure in Study 1 was not due to their having a different sense of what features can change. Studies 3 and 4 replicated Study 1, with stories involving mothers instead of fathers and with lessened task demands. Taken together, the results of the 4 studies refute the claim that preschoolers understand biological inheritance. The findings are discussed in terms of whether children understand biology as an autonomous cognitive domain.

 This is a study of the feasibility of teaching a causal theoretical understanding of biological inheritance to young children. The phenomenon of biological inheritance is a promising one in which to study children's learning of a system of knowledge, for it engages concerns over how understanding individual facts is related to having more broadly coherent frameworks of understanding. For those preschoolers who do not already understand biological inheritance, the construct ion of such an understanding could well entail a reorganization of which facts are at the causal core of their explanations of how and why offspring tend to resemble their parents. The present study was of a pre-test-intervention-post-test design involving tasks based on Johnson and Solomon (1997) and on Springer and Keil (1989) and building on Springer (1995). The intervention focused on three factors: first, children were motivated to seek change in their understanding (i.e. they were made aware that they lacked an explanation of why offspring resemble their parents); secondly, they were supplied with relevant facts (e.g. that babies come from their mothers' bellies); and thirdly, they were presented with a conceptual peg (i.e. a rudimentary notion of genes) about which they could organize and better recruit those facts. Children who had taken parr in the training were subsequently more likely to make adult-like judgments than those who had not.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 The authors evaluated 4 sequential sampling models for 2-choice decisions-the Wiener diffusion, Ornstein-Uhlenbeck (OU) diffusion, accumulator, and Poisson counter models-by fitting them to the response time (RT) distributions and accuracy data from 3 experiments. Each of the models was augmented with assumptions of variability across trials in the rate of accumulation of evidence from stimuli, the values of response criteria, and the value of base RT across trials. Although there was substantial model mimicry, empirical conditions were identified under which the models make discriminably different predictions. The best accounts of the data were provided by the Wiener diffusion model, the OU model with small-to-moderate decay, and the accumulator model with long-tailed (exponential) distributions of criteria, although the last was unable to produce error RTs shorter than correct RTs. The relationship between these models and 3 recent, neurally inspired models was also examined.

 In this letter, we examine the computational mechanisms of reinforcement-based decision making. We bridge the gap across multiple levels of analysis, from neural models of corticostriatal circuits-the basal ganglia (BG) model (Frank, 2005, 2006) to simpler but mathematically tractable diffusion models of two-choice decision making. Specifically, we generated simulated data from the BG model and fit the diffusion model (Ratcliff, 1978) to it. The standard diffusion model fits underestimated response times under conditions of high response and reinforcement conflict. Follow-up fits showed good fits to the data both by increasing nondecision time and by raising decision thresholds as a function of conflict and by allowing this threshold to collapse with time. This profile captures the role and dynamics of the subthalamic nucleus in BG circuitry, and as such, parametric modulations of projection strengths from this nucleus were associated with parametric increases in decision boundary and its modulation by conflict. We then present data from a human reinforcement learning experiment involving decisions with low-and high-reinforcement conflict. Again, the standard model failed to fit the data, but we found that two variants similar to those that fit the BG model data fit the experimental data, thereby providing a convergence of theoretical accounts of complex interactive decision-making mechanisms consistent with available data. This work also demonstrates how to make modest modifications to diffusion models to summarize core computations of the BG model. The result is a better fit and understanding of reinforcement-based choice data than that which would have occurred with either model alone.

-----------------------
 The commentators expressed concerns regarding the relevance and value of non-computational non-symbolic explanations of cognitive performance. But what counts as an explanation depends on the pre-theoretical assumptions behind the scenes of empirical science regarding the kinds of variables and relationships that are sought out in the first place, and some of the present disagreements stem from incommensurate assumptions. Traditional cognitive science presumes cognition to be a decomposable system of components interacting according to computational rules to generate cognitive performances (i.e., component-dominant dynamics). We assign primacy to interaction-dominant dynamics among components. Though either choice can be a good guess before the fact, the primacy of interactions is now supported by much recent empirical work in cognitive science. Consequently, in the main, the commentators have failed so far to address the growing evidence corroborating the theory-driven predictions of complexity science.

 Readers of TopiCS are invited to join a debate about the utility of ideas and methods of complexity science. The topics of debate include empirical instances of qualitative change in cognitive activity and whether this empirical work demonstrates sufficiently the empirical flags of complexity. In addition, new phenomena discovered by complexity scientists, and motivated by complexity theory, call into question some basic assumptions of conventional cognitive science such as stable equilibria and homogeneous variance. The articles and commentaries that appear in this issue also illustrate a new debate style format for topiCS.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 Recent research shows that similarity comparisons involve an alignment process in which features are placed into correspondence. In 6 studies, the authors showed that alignment is involved in category learning as well. Within a category, aligned matches (feature matches occurring on the same dimension) facilitate learning more than nonaligned matches do (matches on different dimensions), although nonaligned matches still facilitate learning relative to nonmatches. Analogously, feature matches that cross category boundaries hurt learning more if they occur on the same versus a different dimension, and cross-category feature matches on different dimensions hurt learning relative to nonmatching features. Representational assumptions of category learning models must be modified to account for the differences between aligned and nonaligned feature matches.

 Three experiments compared the learning of lower-dimensional family resemblance categories (4 dimensions) with the learning of higher-dimensional ones (8 dimensions). Category-learning models incorporating error-driven learning, hypothesis testing, or limited capacity attention predict that additional dimensions should either increase learning difficulty or decrease learning of individual features. Contrary to these predictions, the experiments showed no slower learning of high-dimensional categories; instead, subjects learned more features from high-dimensional categories than from low-dimensional categories. This result obtained both in standard learning with feedback and in noncontingent, observational learning. These results show that rather than interfering with learning, categories with more dimensions cause individuals to learn more. The authors contrast the learning of family resemblance categories with learning in classical conditioning and probability learning paradigms, in which competition among features is well documented.

-----------------------
 Recent research shows that similarity comparisons involve an alignment process in which features are placed into correspondence. In 6 studies, the authors showed that alignment is involved in category learning as well. Within a category, aligned matches (feature matches occurring on the same dimension) facilitate learning more than nonaligned matches do (matches on different dimensions), although nonaligned matches still facilitate learning relative to nonmatches. Analogously, feature matches that cross category boundaries hurt learning more if they occur on the same versus a different dimension, and cross-category feature matches on different dimensions hurt learning relative to nonmatching features. Representational assumptions of category learning models must be modified to account for the differences between aligned and nonaligned feature matches.

 Three experiments compared the learning of lower-dimensional family resemblance categories (4 dimensions) with the learning of higher-dimensional ones (8 dimensions). Category-learning models incorporating error-driven learning, hypothesis testing, or limited capacity attention predict that additional dimensions should either increase learning difficulty or decrease learning of individual features. Contrary to these predictions, the experiments showed no slower learning of high-dimensional categories; instead, subjects learned more features from high-dimensional categories than from low-dimensional categories. This result obtained both in standard learning with feedback and in noncontingent, observational learning. These results show that rather than interfering with learning, categories with more dimensions cause individuals to learn more. The authors contrast the learning of family resemblance categories with learning in classical conditioning and probability learning paradigms, in which competition among features is well documented.

-----------------------
 The study of recollective and nonrecollective retrieval has become controversial, owing to several critiques of traditional recognition-based measurement (e.g., remember/know, ROC, process dissociation). We present a new methodology in which subjects merely study and recall lists, using any standard paradigm (associative, cued, free, or serial recall), the data are analyzed with a Markov model whose parameters measure recollective and nonrecollective retrieval, and the model's fit is compared to that of one-process models. The power of this approach is illustrated in some experiments that dealt with two classic questions: (a) What are the process-level differences between associative and free recall, and (b) why does taxonomic organization improve free recall but impair associative recall? Fit results showed that a dual-retrieval model is both necessary and sufficient to account for associative and free recall data, in contrast to the sufficient-but-not-necessary pattern that prevails in the recognition literature. Key substantive findings were that associative recall is more reliant on recollective retrieval and less reliant on nonrecollective retrieval than free recall, that taxonomic organization impairs recollective retrieval in both paradigms, and that taxonomic organization enhances the reconstruction component of nonrecollective retrieval in free recall. (C) 2010 Elsevier Inc. All rights reserved.

 Although high levels of phantom recollection (illusory vivid experience of the prior "presentation" of unpresented items) have been found for false recognition, little is known about phantom recollection in recall. We examined this issue with Deese/Roediger-McDermott lists using two paradigms: repeated recall and conjoint recall. High levels of phantom recollection were observed with both standard behavioral measures and the parameters of fuzzy-trace theory's dual-recall model. In addition, phantom recollection and the true recollection that accompanies presented items appear to involve different retrieval processes, because they were dissociated by manipulations such as number of recall tests and list strength.

-----------------------
 This paper presents a cognitive account of the process of evaluating scientific data. Our account assumes that when individuals evaluate data, they construct a mental model of a data-interpretation package, in which the data and theoretical interpretations of the data are integrated. We propose that individuals attempt to discount data by seeking alternative explanations for events within the mental model; data-interpretation packages are accepted when the individual cannot find alternative accounts for these events. Our analysis indicates that there are many levels at which data-interpretation packages can be accepted or denied.

 This article reports the results of a study investigating how undergraduates evaluate realistic scientific data in the domains of geology and paleontology. The results are used to test several predictions of a theory of data evaluation, which we call models-of-data theory. Models-of-data theory assumes that when evaluating data, the individual constructs a particular kind of cognitive model that integrates many features of the data with a theoretical interpretation of the data, The individual evaluates the model by attempting to generate alternative causal explanations for the events in the model. We contrast models-of-data theory with other proposals for how data are cognitively represented and show that models-of-data theory gives a good account of the pattern of written evaluations of data produced by the undergraduates in the study, We discuss theoretical and instructional implications of the theory.

-----------------------
 Clark &amp; Thornton's conception finds an echo in implicit learning research, which shows that subjects may perform adaptively in complex structured situations through the use of simple statistical learning mechanisms. However, the authors fail to draw a distinction between, on the one hand, subjects' representations which emerge from type-1 learning mechanisms, and, on the other, their knowledge of the genuine abstract ''recoding function'' which defines a type-2 problem.

 The domain-general learning mechanisms elicited in incidental learning situations are of potential interest in many research fields, including language acquisition, object knowledge formation and motor learning. They have been the focus of studies on implicit learning for nearly 40 years. Stemming from a different research tradition, studies on statistical learning carried out in the past 10 years after the seminal studies by Saffran and collaborators, appear to be closely related, and the similarity between the two approaches is strengthened further by their recent evolution. However, implicit learning and statistical learning research favor different interpretations, focusing on the formation of chunks and statistical computations, respectively. We examine these differing approaches and suggest that this divergence opens up a major theoretical challenge for future studies.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 The authors examined the influence of cochlear implants on joint attention and social competence in severe to profoundly congenitally deaf toddlers. Twenty-seven hearing mothers and hearing toddlers (HH dyads), and 26 hearing mothers and deaf toddlers, 9 with cochlear implantation (HD-cochlear dyads), and 17 with no cochlear implantation (HD-no cochlear dyads) were observed engaging in joint attention. Mothers provided ratings of children's social competence. HH and HD-cochlear dyads displayed more joint attention than HD-no cochlear dyads. Children who were in dyads who engaged in more joint attention were rated by their mothers as higher on expressive and compliance behaviors and lower on disruptive behaviors compared to children who were in dyads who engaged in lower levels of joint attention. Findings suggest that cochlear implants may aid in the early socio-emotional development of some deaf children.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Socially withdrawn children frequently refrain from social activities in the presence of peers. The lack of social interaction in childhood may result from a variety of causes, including social fear and anxiety or a preference for solitude. From early childhood through to adolescence, socially withdrawn children are concurrently and predictively at risk for a wide range of negative adjustment outcomes, including socio-emotional difficulties (e.g., anxiety, low self-esteem, depressive symptoms, and internalizing problems), peer difficulties (e.g., rejection, victimization, poor friendship quality), and school difficulties (e.g., poor-quality teacher-child relationships, academic difficulties, school avoidance). The goals of the current review are to (a) provide some definitional, theoretical, and methodological clarity to the complex array of terms and constructs previously employed in the study of social withdrawal; (b) examine the predictors, correlates, and consequences of child and early-adolescent social withdrawal; and (c) present a developmental framework describing pathways to and from social withdrawal in childhood.

 Children with behavioral inhibition, a temperamental style characterized by infant distress to novelty and childhood social reticence, exhibit both continuity and discontinuity of this behavioral trait over the course of development. However, few researchers have identified factors that might be responsible for these different patterns. In the current study, childcare history, maternal personality, and maternal behavior were examined as moderators of the relations between infant temperament, preschool social reticence, and childhood social wariness. Seventy-seven children participated in this longitudinal study that began in infancy and continued into middle childhood. Maternal negative personality moderated the relation between infant temperament and childhood social wariness. In addition, maternal behavior moderated the relation between preschool social reticence and childhood social wariness. The findings suggest that a complex interplay of within-child and maternal factors affect the development of internalizing behavior in the early school years.

-----------------------
 Recent studies have noted a relation between the pattern of resting frontal EEG activity and individual differences in affective style in typically developing infants, children, and adults. The authors conducted a pilot study to investigate the pattern of frontal EEG activity during a resting condition (eyes-open, eyes-closed) in a group of children who had one parent clinically diagnosed with social phobia (SP; n = 6) and in a group of typically developing children of similar age with healthy parents (n = 7). Patients with a primary DSM-IV diagnosis of SP with at least one biological child were recruited from the Anxiety Disorders Clinic at McMaster University Medical Centre. We found that children of parents clinically diagnosed with SP tended to exhibit higher overall resting frontal EEG activity compared with the children of healthy parents. This pattern of overall high EEG activity that is specific to the frontal region is similar to that observed in socially anxious profiles. Preliminary findings are discussed in terms of how overall resting frontal brain activation may be an early psychophysiological marker for placing children of parents with social phobia at risk for socioemotional problems before such problems emerge.

 Gene-environment interactions involving exogenous environmental factors are known to shape behavior and personality development. Although gene-environment interactions involving endogenous environmental factors are hypothesized to play an equally important role, this conceptual approach has not been empirically applied in the study of early-developing temperament in humans. Here we report evidence for a gene-endoenvironment (i.e., resting frontal brain electroencephalogram, EEG, asymmetry) interaction in predicting child temperament. The dopamine D4 receptor (DRD4) gene (long allele vs. short allele) moderated the relation between resting frontal EEG asymmetry (left vs. right) at 9 months and temperament at 48 months. Children who exhibited left frontal EEG asymmetry at 9 months and who possessed the DRD4 long allele were significantly more soothable at 48 months than other children. Among children with right frontal EEG asymmetry at 9 months, those with the DRD4 long allele had significantly more difficulties focusing and sustaining attention at 48 months than those with the DRD4 short allele. Resting frontal EEG asymmetry did not influence temperament in the absence of the DRD4 long allele. We discuss how the interaction of genetic and endoenvironmental factors may confer risk and protection for different behavioral styles in children.

-----------------------
 Despite the potency of confession evidence in criminal law, recent DNA exonerations indicate that false confessions are a contributing factor in numerous wrongful convictions. After distinguishing between voluntary, compliant, and internalized false confessions, this article reviews research implicating a sequence of three processes responsible for false confessions and the adverse consequences of these confessions. First, police often target innocent people for interrogation because of erroneous judgments of truth and deception made during preinterrogation interviews. Second, innocent people are sometimes induced to confess as a function of certain police interrogation tactics, dispositional suspect vulnerabilities, and naive mental state that accompanies innocence. Third, people cannot readily distinguish between true and false confessions and often fail to discount those confessions they Perceive to be coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

 Despite the commonsense belief that people do not confess to crimes they did not commit, 20 to 25% of all DNA exonerations involve innocent prisoners who confessed. After distinguishing between voluntary, compliant, and internalized false confessions, this article suggests that a sequence of three processes is responsible for false confessions and their adverse consequences. First, police sometimes target innocent people for interrogation because of erroneous judgments of truth and deception. Second, innocent people sometimes confess as a function of certain interrogation tactics, dispositional suspect vulnerabilities, and the phenomenology of innocence. Third, jurors fail to discount even those confessions they see as coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

-----------------------
 Despite the potency of confession evidence in criminal law, recent DNA exonerations indicate that false confessions are a contributing factor in numerous wrongful convictions. After distinguishing between voluntary, compliant, and internalized false confessions, this article reviews research implicating a sequence of three processes responsible for false confessions and the adverse consequences of these confessions. First, police often target innocent people for interrogation because of erroneous judgments of truth and deception made during preinterrogation interviews. Second, innocent people are sometimes induced to confess as a function of certain police interrogation tactics, dispositional suspect vulnerabilities, and naive mental state that accompanies innocence. Third, people cannot readily distinguish between true and false confessions and often fail to discount those confessions they Perceive to be coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

 Despite the commonsense belief that people do not confess to crimes they did not commit, 20 to 25% of all DNA exonerations involve innocent prisoners who confessed. After distinguishing between voluntary, compliant, and internalized false confessions, this article suggests that a sequence of three processes is responsible for false confessions and their adverse consequences. First, police sometimes target innocent people for interrogation because of erroneous judgments of truth and deception. Second, innocent people sometimes confess as a function of certain interrogation tactics, dispositional suspect vulnerabilities, and the phenomenology of innocence. Third, jurors fail to discount even those confessions they see as coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

-----------------------
 Broca's area, a cerebral cortical area located in the inferior frontal gyrus (IFG) of the human brain, has been identified as one of several critical regions associated with the motor planning and execution of language. Anatomically, Broca's area is most often larger in the left hemisphere, and functional imaging studies in humans indicate significant left-lateralized patterns of activation during language-related tasks [1-3]. If, and to what extent, nonhuman primates, particularly chimpanzees, possess a homologous region that is involved in the production of their own communicative signals remains unknown. Here, we show that portions of the IFG as well as other cortical and subcortical regions in chimpanzees are active during the production of communicative signals. These findings are the first to provide direct evidence of the neuroanatomical structures associated with the production of communicative behaviors in chimpanzees. Significant activation in the left IFG in conjunction with other cortical and subcortical brain areas during the production of communicative signals in chimpanzees suggests that the neurological substrates underlying language production in the human brain may have been present in the common ancestor of humans and chimpanzees.

 Brain asymmetries, particularly asymmetries within regions associated with language, have been suggested as a key difference between humans and our nearest ancestors. These regions include the planum temporale (PT) - the bank of tissue that lies posterior to Heschl's gyrus and encompasses Wemicke's area, an important brain region involved in language and speech in the human brain. In the human brain, both the surface area and the grey matter volume of the PT are larger in the left compared to right hemisphere, particularly among right-handed individuals. Here we compared the grey matter volume and asymmetry of the PT in chimpanzees and three other species of nonhuman primate in two Genera including velvet monkeys (Chlorocebus aethiops sabaeus), rhesus macaques (Macaca mulatta) and bonnet macaques (Macaca radiata). We show that the three monkey species do not show population-level asymmetries in this region whereas the chimpanzees do, suggesting that the evolutionary brain development that gave rise to PT asymmetry occurred after our split with the monkey species, but before our split with the chimpanzees. (C) 2011 Elsevier Ltd. All rights reserved.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 In development, children often use gesture to communicate before they use words. The question is whether these gestures merely precede language development or are fundamentally tied to it. We examined 10 children making the transition from single words to two-word combinations and found that gesture had a tight relation to the children's lexical and syntactic development. First, a great many of the lexical items that each child produced initially in gesture later moved to that child's verbal lexicon. Second, children who were first to produce gesture-plus-word combinations conveying two elements in a proposition (point at bird and say "nap") were also first to produce two-word combinations ("bird nap"). Changes in gesture thus not only predate but also predict changes in language, suggesting that early gesture may be paving the way for future developments in language.

 Children produce their first gestures before their first words, and their first gesture+word sentences before their first word+word sentences. These gestural accomplishments have been found not only to predate linguistic milestones, but also to predict them. Findings of this sort suggest that gesture itself might be playing a role in the language-learning process. But what role does it play? Children's gestures could elicit from their mothers the kinds of words and sentences that the children need to hear in order to take their next linguistic step. We examined maternal responses to the gestures and speech that 10 children produced during the one-word period. We found that all 10 mothers 'translated' their children's gestures into words, providing timely models for how one- and two-word ideas can be expressed in English. Gesture thus offers a mechanism by which children can point out their thoughts to mothers, who then calibrate their speech to those thoughts, and potentially facilitate language-learning.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 Thirty-six 2-, 4- and 6-month-old infants were videotaped while interacting with a female adult stranger engaging in either organized or disorganized 1-min peekaboo games. Two-month-old infants gazed and smiled equally at the stranger, regardless of the relative organization of the peekaboo game. In contrast, 4- and 6-month-old infants smiled significantly more and gazed significantly less;in the organized peekaboo condition than in the disorganized peekaboo condition. These results suggest that from a diffuse sensitivity to the presence of a social partner, infants by 4 months develop a new sensitivity to the narrative envelope of protoconversation, in particular the timing and the structure of social exchanges scaffolded by adults. These observations are interpreted as evidence of developing social expectations in the first 6 months of life. This early development is viewed as announcing and preparing the communicative competence that blossoms by the end of the 1st year.

 Two-month-old infants (N = 29) participated in face-to-face interactions with their mothers and with strangers. The contingent responsiveness for smiles and vocalizations, while attending to the partner, was assessed for each partner in both interactions. For smiles and for vocalizations, infants were less responsive to the stranger relative to the mother when the stranger's contingent responsiveness was either more contingent or less contingent than that of the mother. Results are supportive of the hypothesis that young infants develop sensitivities to levels of social contingency present in their maternal interactions, which influence their responsiveness to others.

-----------------------
 Thirty-six 2-, 4- and 6-month-old infants were videotaped while interacting with a female adult stranger engaging in either organized or disorganized 1-min peekaboo games. Two-month-old infants gazed and smiled equally at the stranger, regardless of the relative organization of the peekaboo game. In contrast, 4- and 6-month-old infants smiled significantly more and gazed significantly less;in the organized peekaboo condition than in the disorganized peekaboo condition. These results suggest that from a diffuse sensitivity to the presence of a social partner, infants by 4 months develop a new sensitivity to the narrative envelope of protoconversation, in particular the timing and the structure of social exchanges scaffolded by adults. These observations are interpreted as evidence of developing social expectations in the first 6 months of life. This early development is viewed as announcing and preparing the communicative competence that blossoms by the end of the 1st year.

 Two-month-old infants (N = 29) participated in face-to-face interactions with their mothers and with strangers. The contingent responsiveness for smiles and vocalizations, while attending to the partner, was assessed for each partner in both interactions. For smiles and for vocalizations, infants were less responsive to the stranger relative to the mother when the stranger's contingent responsiveness was either more contingent or less contingent than that of the mother. Results are supportive of the hypothesis that young infants develop sensitivities to levels of social contingency present in their maternal interactions, which influence their responsiveness to others.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 Using diffusion tensor imaging and tractography, we found that a disruption in structural connectivity in ventral occipitotemporal cortex may be the neurobiological basis for the lifelong impairment in face recognition that is experienced by individuals who suffer from congenital prosopagnosia. Our findings suggest that white-matter fibers in ventral occipitotemporal cortex support the integrated function of a distributed cortical network that subserves normal face processing.

 The summed activity of multiple nodes of a distributed cortical network supports face recognition in humans, including "core" ventral occipitotemporal cortex (VOTC) regions [1-3], and "extended" regions outside VOTC [4, 5]. Many individuals with congenital prosopagnosia-an impairment in face processing [6-9]-exhibit normal blood oxygenation level-dependent (BOLD) activation in the core VOTC regions [10, 11]. These individuals evince a reduction in the structural integrity of the white matter tracts connecting VOTC to anterior temporal and frontal cortices [12], part of the "extended" face network. The impairment in congenital prosopagnosia may arise, therefore, not from a dysfunction of the core VOTC areas but from a failure to propagate signals between the intact VOTC and the extended nodes of the network. Using the fMR adaptation paradigm with famous and unknown faces, we show that individuals with congenital prosopagnosia evince normal adaptation effects in VOTC, indicating sensitivity to facial identity, but show no differential activation for familiar versus unknown faces outside VOTC, particularly in the precuneus/posterior cingulate cortex and the anterior paracingulate cortex. Normal BOLD activation in VOTC is thus insufficient to subserve intact face recognition, and disrupted information propagation between VOTC and the extended face processing network may explain the functional impairment in congenital prosopagnosia.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 The present study examined the influence of stable personality traits on romantic relationships using longitudinal data on a large, representative sample of young adults. Relationship experiences (quality, conflict, and abuse) showed relatively small mean-level changes over time and significant levels of rank-order stability, even across different relationship partners. Antecedent personality traits (assessed at age 18) predicted relationship experiences at age 26 and change in relationship experiences from age 21 to 26. Conversely, relationship experiences also predicted change in personality. Importantly, these findings generally held across relationship partners, suggesting that some people tend to be generally happy (or unhappy) across relationships, and this is due, in part, to stable individual differences in personality. Discussion focuses on the broader implications of the findings, in particular the need for relationship researchers to consider the importance of personality for why relationships thrive or fail and the need for personality researchers to consider the impact of relationship experiences on intraindividual personality development.

 This study provides a comprehensive picture of age differences in self-esteem from age 9 to 90 years using cross-sectional data collected from 326,641 individuals over the Internet. Self-esteem levels were high in childhood, dropped during adolescence, rose gradually throughout adulthood, and declined sharply in old age. This trajectory generally held across gender, socioeconomic status, ethnicity, and nationality (U.S. citizens vs. non-U.S. citizens). Overall, these findings support previous research, help clarify inconsistencies in the literature, and document new trends that require further investigation.

-----------------------
 The current study examined age differences in maltreated and nonmaltreated children's knowledge of juvenile dependency court vocabulary and proceedings. One hundred and sixty-seven children aged 4-14 years were questioned about their understanding of legal vocabulary and about the content of a story depicting a child involved in dependency court. Age-related increases emerged across all measures of children's legal understanding. Direct experience with the dependency system was also related to the accuracy of children's legal knowledge. Children with greater experience in the dependency system were more knowledgeable than children with no such experience, although even the oldest maltreated children with considerable dependency system experience evidenced some deficits in legal knowledge. Overall, findings suggest children and adolescents involved in dependency proceedings need help understanding some aspects of the dependency process, and this need exists regardless of whether children have been involved in cases ongoing for some time.

 Despite widespread use of self-report measures of adult attachment, relatively little research has explored the predictive utility of these measures in the domain of parent-child relationships. The present study examined the association between self-reported attachment style and parental responsiveness during a stressful event. Children and their parents were observed while children received an inoculation at a county immunization clinic. Children's reactions to the inoculation were rated and parents' responsiveness was assessed with the Emotional Availability Scales (EAS). Results revealed that children of parents scoring high oil self-reported attachment avoidance were more distressed during the inoculation than children of parents scoring low on avoidance. Moreover, parents high on avoidance were less responsive when children were highly distressed, whereas this pattern was reversed among parents scoring low on avoidance. Finally, the influence of adult attachment on parental behavior and children's distress was found to be independent of children's temperament and parental personality. These findings suggest that self-report adult attachment measures may be useful in the domain of parent-child relationships.

-----------------------
 The current study examined age differences in maltreated and nonmaltreated children's knowledge of juvenile dependency court vocabulary and proceedings. One hundred and sixty-seven children aged 4-14 years were questioned about their understanding of legal vocabulary and about the content of a story depicting a child involved in dependency court. Age-related increases emerged across all measures of children's legal understanding. Direct experience with the dependency system was also related to the accuracy of children's legal knowledge. Children with greater experience in the dependency system were more knowledgeable than children with no such experience, although even the oldest maltreated children with considerable dependency system experience evidenced some deficits in legal knowledge. Overall, findings suggest children and adolescents involved in dependency proceedings need help understanding some aspects of the dependency process, and this need exists regardless of whether children have been involved in cases ongoing for some time.

 Despite widespread use of self-report measures of adult attachment, relatively little research has explored the predictive utility of these measures in the domain of parent-child relationships. The present study examined the association between self-reported attachment style and parental responsiveness during a stressful event. Children and their parents were observed while children received an inoculation at a county immunization clinic. Children's reactions to the inoculation were rated and parents' responsiveness was assessed with the Emotional Availability Scales (EAS). Results revealed that children of parents scoring high oil self-reported attachment avoidance were more distressed during the inoculation than children of parents scoring low on avoidance. Moreover, parents high on avoidance were less responsive when children were highly distressed, whereas this pattern was reversed among parents scoring low on avoidance. Finally, the influence of adult attachment on parental behavior and children's distress was found to be independent of children's temperament and parental personality. These findings suggest that self-report adult attachment measures may be useful in the domain of parent-child relationships.

-----------------------
 The current study examined age differences in maltreated and nonmaltreated children's knowledge of juvenile dependency court vocabulary and proceedings. One hundred and sixty-seven children aged 4-14 years were questioned about their understanding of legal vocabulary and about the content of a story depicting a child involved in dependency court. Age-related increases emerged across all measures of children's legal understanding. Direct experience with the dependency system was also related to the accuracy of children's legal knowledge. Children with greater experience in the dependency system were more knowledgeable than children with no such experience, although even the oldest maltreated children with considerable dependency system experience evidenced some deficits in legal knowledge. Overall, findings suggest children and adolescents involved in dependency proceedings need help understanding some aspects of the dependency process, and this need exists regardless of whether children have been involved in cases ongoing for some time.

 Despite widespread use of self-report measures of adult attachment, relatively little research has explored the predictive utility of these measures in the domain of parent-child relationships. The present study examined the association between self-reported attachment style and parental responsiveness during a stressful event. Children and their parents were observed while children received an inoculation at a county immunization clinic. Children's reactions to the inoculation were rated and parents' responsiveness was assessed with the Emotional Availability Scales (EAS). Results revealed that children of parents scoring high oil self-reported attachment avoidance were more distressed during the inoculation than children of parents scoring low on avoidance. Moreover, parents high on avoidance were less responsive when children were highly distressed, whereas this pattern was reversed among parents scoring low on avoidance. Finally, the influence of adult attachment on parental behavior and children's distress was found to be independent of children's temperament and parental personality. These findings suggest that self-report adult attachment measures may be useful in the domain of parent-child relationships.

-----------------------
 Clark &amp; Thornton's conception finds an echo in implicit learning research, which shows that subjects may perform adaptively in complex structured situations through the use of simple statistical learning mechanisms. However, the authors fail to draw a distinction between, on the one hand, subjects' representations which emerge from type-1 learning mechanisms, and, on the other, their knowledge of the genuine abstract ''recoding function'' which defines a type-2 problem.

 The domain-general learning mechanisms elicited in incidental learning situations are of potential interest in many research fields, including language acquisition, object knowledge formation and motor learning. They have been the focus of studies on implicit learning for nearly 40 years. Stemming from a different research tradition, studies on statistical learning carried out in the past 10 years after the seminal studies by Saffran and collaborators, appear to be closely related, and the similarity between the two approaches is strengthened further by their recent evolution. However, implicit learning and statistical learning research favor different interpretations, focusing on the formation of chunks and statistical computations, respectively. We examine these differing approaches and suggest that this divergence opens up a major theoretical challenge for future studies.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 Much of young children's symbolic play is heavily scaffolded by adult symbolic action models, which children may imitate, and by adult verbal scripts. The current studies attempted to evaluate 18-35-month-old children's symbolic skills in the absence of such scaffolding. In a study of symbol comprehension, children were rested for their ability to comprehend an adult's use of either a replica object or an associated gesture to communicate which object in an array she want ed. In a study of symbol production, children were given some objects that afforded symbolic manipulations, but without adult symbolic action models or verbal scripts. The results of the two studies converged to suggest chat children below 2 years of age have symbolic skills with gestures, but: nor with objects. It was also found that while children at 26 months were able to use an object as a symbol for another object, they had difficulties when the symbol had another conventional use (e.g. a drinking cup used as a hat). The findings are discussed in terms of DeLoache's dual representation model, and a modification of chat: model is proposed.

 In this study we sought to determine the degree to which 2- to 3-year-old children use objects symbolically in the relative absence of adult symbolic actions or linguistic descriptions, and how the nature of objects influences symbolic play. Results revealed a dramatic increase in children's creative symbolic productions between 2 and 3 years of age, with the tendency to produce symbolic actions influenced to an equal degree by adult symbolic action models and verbal directions. Children of all ages were heavily influenced by the nature of the object to be used as a symbol, with the youngest children using only replica objects as symbols. In a second study, we examined childrens looks to an adult as they engaged in different kinds of activities with objects. The main finding was that children looked to the adult immediately after performing a symbolic action more often than if they performed an instrumental action. We argue for the essentially social nature of symbolic play, both in terms of how children learn to use objects as symbols and in terms of the reasons they do so.

-----------------------
 Objectives. Errors in eyewitness accounts can occur when a witness comes into contact with post-event 'misinformation'. A common way to encounter misinformation is through face-to-face interaction, in particular, via conversation with other individuals who also witnessed the crime. The current research compares this kind of misinformation with the non-social post-event narrative method typically employed in laboratory studies.

 The forgetting and remembering phenomena that Erdelyi outlines here have little to do with the concept of repression. None of the research that he describes shows that it is possible for people to repress (and then recover) memories for entire, significant, and potentiatly emotion-laden events. In the absence of scientific evidence, we continue to challenge the validity of the concept of repression.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 Implicit motor sequence learning refers to an important human ability to acquire new motor skills through the repeated performance of a motor sequence. This learning process is characterized by slow, incremental gains of motor performance. The present fMRI study was developed to better delineate the areas supporting these temporal dynamics of learning. By using the serial color matching paradigm, our study focused on the motor level of sequence learning and tracked the time course of learning-related neural changes. Imaging results showed a significant contribution of the left anterior hippocampus in an early sequence acquisition stage (first scanning session) as well as during a later stage with stabilized learning effects (second scanning session). Hippocampal activation significantly correlated with the behavioral learning process and was affected by a change of the motor sequence. These results suggest a strong involvement of the hippocampus in implicit motor sequence learning. On the other hand, a very extensive and bilateral neural network of parietal, temporal and frontal cortical areas (including SMA, pre-SMA) together with parts of the cerebellum and striatum were found to play a role during random visuo-motor task performance.

 The dynamics of the neural network that underlies learning transitive structures of an ordered sequence remains poorly understood. To address this, in the present study we used fMRI to track the time course of transitive inference learning. The hippocampus and the angular gyrus were each shown to be closely related to the learning trajectory, but differentially so. Hippocampal activity was shown to consistently increase with learning but no correlation was found between performance and hippocampal activation, suggesting a general role for the hippocampus. Left angular gyrus activity was also found to consistently increase with training, but, in addition, correlated significantly with behavioral performance. This suggests an involvement of the angular gyros in learning the ordinal associations between the stimuli. (c) 2007 Elsevier Inc. All rights reserved.

-----------------------
 Implicit motor sequence learning refers to an important human ability to acquire new motor skills through the repeated performance of a motor sequence. This learning process is characterized by slow, incremental gains of motor performance. The present fMRI study was developed to better delineate the areas supporting these temporal dynamics of learning. By using the serial color matching paradigm, our study focused on the motor level of sequence learning and tracked the time course of learning-related neural changes. Imaging results showed a significant contribution of the left anterior hippocampus in an early sequence acquisition stage (first scanning session) as well as during a later stage with stabilized learning effects (second scanning session). Hippocampal activation significantly correlated with the behavioral learning process and was affected by a change of the motor sequence. These results suggest a strong involvement of the hippocampus in implicit motor sequence learning. On the other hand, a very extensive and bilateral neural network of parietal, temporal and frontal cortical areas (including SMA, pre-SMA) together with parts of the cerebellum and striatum were found to play a role during random visuo-motor task performance.

 The dynamics of the neural network that underlies learning transitive structures of an ordered sequence remains poorly understood. To address this, in the present study we used fMRI to track the time course of transitive inference learning. The hippocampus and the angular gyrus were each shown to be closely related to the learning trajectory, but differentially so. Hippocampal activity was shown to consistently increase with learning but no correlation was found between performance and hippocampal activation, suggesting a general role for the hippocampus. Left angular gyrus activity was also found to consistently increase with training, but, in addition, correlated significantly with behavioral performance. This suggests an involvement of the angular gyros in learning the ordinal associations between the stimuli. (c) 2007 Elsevier Inc. All rights reserved.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 This study investigated the impact of duration-varying response effects on the generation and execution of duration-varying responses. Participants performed short or long keypresses which produced auditory effects of corresponding duration (short response --&gt; short tone, long response --&gt; long tone) or of noncorresponding duration (short response --&gt; long tone, long response --&gt; short tone). Experiment I revealed faster responding with a corresponding than with a noncorresponding Response-Effect (R-E) mapping; that is, a temporal R-E compatibility effect. Additionally, increasing effect duration increased response latencies, whereas it decreased keypress duration. Experiment 2 showed that the influence of temporal R-E compatibility persists even when responses are cued in advance, suggesting that at least part of it originates from response generation processes occurring later than a traditional response selection stage. These findings corroborate and complement effect-based theories of action control which assume that the selection, initiation, and execution of movements is mediated by anticipation of their sensory effects.

 This study investigated the impact of contingent action effects on response production. In Experiment 1 responses of varying intensity were initiated faster when contingently followed by auditory effects of corresponding rather than of noncorresponding intensity. This response-effect (R-E) compatibility influence was robust with respect to practice, and it was not due to persisting influences of preceding R-E episodes. These results support the conclusion that R-E compatibility reflects the impact of anticipatory effect representations in response production. Experiment 2 showed that anticipatory effect codes have an impact on early processes of response production (response selection) as well as on processes that immediately precede overt responding (response initiation). Finally, they also influence the way the actions are physically performed (response execution). The results support and specify ideo-motor theories of action control that assume movements to be controlled by anticipations of their sensorial effects.

-----------------------
 Hand allograft is a method in the stage of clinical experimentation, which is reserved in France for the treatment of bilateral traumatic amputees. This study reports the Lyon team experience, which is pioneer in this domain. Four patients Q mates and 1 female) underwent seven (one unilateral and three bilateral) hand transplantations from September 1998 to February 2007. The Level of amputation was at the wrist or at the mid-forearm. Delay since hand loss ranged from 2.5 to 9 years. The surgical protocol was elaborated and planned case by case. All. recipients received the same immunosuppressive treatment. Episodes of acute rejection were observed in the first 3 months after transplantation, which were easily managed after a few days increasing oral prednisone doses and applying topical immunosuppressants. Currently the patients receive the doses of immunosuppressants comparable to those in kidney-grafted patients. We have not registered any severe complication of immunosuppressive treatment up tilt now (7 years follow-up for the earliest graft). We performed analytical and functional clinical, as well as questionnaire evaluation of patients. The first case (unilateral graft) resulted in graft failure at 2 years due to non-compliance of the patient. The three bilateral graftees demonstrate a favorable evolution despite. some immunological (hyperglycemia, serum sickness) and surgical (thrombosis, osteomyelitis, skin toss) complications, which could be managed. The middle and long-term follow-up evaluation revealed good to excellent sensorimotor recovery of 4 hands in both mate recipients (4 and 7 years) with satisfactory social adaptation, higher or equal to those expected after post-traumatic replantations at the equivalent level and higher to those obtained with currently available myoelectric prosthesis. The last patient, a young female who has been grafted in February 2007, receives ongoing reeducation course and shows normal progress of functional restoration of both hands. The encouraging results of this clinical experimentation make us currently consider hand allografting as reasonable and useful both for the patients and for evolution of research in composite tissues allotransplantation (CTA). Further long-term careful research and worldwide monitoring of all patients with hand allografts is required to, on the one part, state on the authorization of this surgery, and, on the other part, to better elucidate the mechanisms of successful CTA. (c) 2007 Publie par Elsevier Masson SAS.

 Schizoaffective disorder was first described by Kasanin in 1932. This disorder associates schizophrenic symptoms and mood swings. To diagnose a schizoaffective disorder is difficult since this trouble is so vaguely defined. Psychiatrists are confronted to this difficulty daily as there are many patients suffering from mood disorder and schizophrenic symptoms and they are difficult to take care of. Patients suffering from schizoaffective disorder are more often hospitalized and their social and professional insertion is poorer than that of patients suffering from bipolar disorder. On the contrary, these characteristics are more accentuated in schizophrenic patients than in patients with schizoaffective disorder. Epidemiologic research in mental health currently tends to considerate psychosis in a dimensional point of view. This conception is inherited from Griesinger's unitary psychosis notion. The present article considers historical aspects favouring a concept of continuum of psychosis. (C) 2008 Elsevier Masson SAS.

-----------------------
 Four experiments were conducted to investigate whether variations in orientation that profoundly affect the ability to imagine rotations also affect the ability to imagine projective transformations. For a basic rectilinear object and the three simpler Platonic Solids, imagining projective transformations (e.g., the casting of a shadow) was quite successful when the objects were aligned with the direction of projection. For the solids, this alignment occurred when the objects were generalized cylinders about axes aligned with the projection. As the objects were made more oblique to the projection, performance deteriorated markedly. When the objects were moderately aligned with the projection, performance depended on the orientation of the object and the orientation of the projection to the environment. We suggest that the imagination of projection and of rotation is a type of problem solving in which spatial structures are organized in relation to initially given properties of the objects and transformations. When there is alignment among the various structural components, this process of imagination works efficiently. Without such alignment, nonexperts often fail. We suggest that aligned (i.e., parallel and perpendicular) orientations are effective in spatial imagination because they are categorically distinct and singular, and they provide a critical form of redundancy. (C) 1996 Academic Press, Inc.

 The regular polyhedra, commonly known as the ''Platonic solids'', are fundamental three-dimensional structures. It is known that the ease of imagining one of these solids, the cube, varies radically with its orientation to the vertical. We demonstrate the same variation for perception and imagination of all three of the simpler Platonic solids: The cube, octahedron, and tetrahedron. In orientations of the objects that are relatively easy to comprehend, the objects are generalized cylinders about the vertical. In the difficult orientations, the objects are antiprismatic about the vertical. The critical difference between these structures is that generalized cylinders have uniform orientations of edges and surfaces about an object axis while antiprisms have nonuniform orientations. These results support strongly the view that the orientations of object features are important in spatial organization, that humans are highly sensitive to objective forms of regularity in spatial organization, and that the generalized cylinder is a form of spatial regularity that people find simple.

-----------------------
 The masked affective priming task was used as an unobtrusive measure of intergroup prejudices in a sample of German adolescents (aged 13-15). Pictures of Turks and Germans were used as masked primes that preceded positive and negative target adjectives conveying either other-relevant valence (e.g., honest, evil) or possessor-relevant valence (e.g., talented, dull). Affective priming indices (denoting relative negativity of Turkish primes) were positively correlated with the open expression of prejudices towards Turks and foreigners in general in questionnaires as well as with discriminative interaction behavior in a virtual ball-tossing game. As expected, these correlations were found only for priming indices based on other-relevant targets, thereby emphasizing the differentiation of automatic prejudice into (imputed) hostility and depreciation.

 In this paper, we yield evidence for the dependence of affective priming on the congruency of the previous trial. Affective priming refers to the finding that valence categorizations of targets are facilitated when the preceding prime is of the same valence. In two experiments, affective priming was diminished after incongruent trials (i.e., prime and target were of different valence), whereas, significant affective priming was observed after congruent trials (i.e., prime and target were of same valence). We compare this pattern to the known sequential dependencies in Stroop- and Eriksen-type tasks. Furthermore, our results can help to improve the statistical power of studies in which the affective priming task is used as a measure for automatic evaluations of attitude-objects. (c) 2008 Elsevier B.V. All rights reserved.

-----------------------
 The masked affective priming task was used as an unobtrusive measure of intergroup prejudices in a sample of German adolescents (aged 13-15). Pictures of Turks and Germans were used as masked primes that preceded positive and negative target adjectives conveying either other-relevant valence (e.g., honest, evil) or possessor-relevant valence (e.g., talented, dull). Affective priming indices (denoting relative negativity of Turkish primes) were positively correlated with the open expression of prejudices towards Turks and foreigners in general in questionnaires as well as with discriminative interaction behavior in a virtual ball-tossing game. As expected, these correlations were found only for priming indices based on other-relevant targets, thereby emphasizing the differentiation of automatic prejudice into (imputed) hostility and depreciation.

 In this paper, we yield evidence for the dependence of affective priming on the congruency of the previous trial. Affective priming refers to the finding that valence categorizations of targets are facilitated when the preceding prime is of the same valence. In two experiments, affective priming was diminished after incongruent trials (i.e., prime and target were of different valence), whereas, significant affective priming was observed after congruent trials (i.e., prime and target were of same valence). We compare this pattern to the known sequential dependencies in Stroop- and Eriksen-type tasks. Furthermore, our results can help to improve the statistical power of studies in which the affective priming task is used as a measure for automatic evaluations of attitude-objects. (c) 2008 Elsevier B.V. All rights reserved.

-----------------------
 The authors of this reply article note that B. Gawronski, E. P. LeBel, K. R. Peters, and R. Banse (2009) (a) expressed agreement in their comment with the analysis put forward in the target article (J. De Houwer, S. Teige-Mocigemba, A. Spruyt, &amp; A. Moors, 2009) and (b) pointed to a further implication for the way in which the implicitness of a measure should be examined. The current authors note that B. A. Nosek and A. G. Greenwald (2009), on the other hand, raised questions in their comment about the definition of the concept "implicit" in the target article, arguing for a fundamentally different approach to measurement that emphasizes not theoretical understanding but usefulness for predicting behavior. In this reply, the current authors respond to these comments and argue that when theoretical claims are made about measures, these claims should be backed up with appropriate evidence. In the absence of basic research, measures and their relation to behavior can only be described.

 The main goal of our target article was to provide concrete recommendations for improving the replicability of research findings. Most of the comments focus on this point. In addition, a few comments were concerned with the distinction between replicability and generalizability and the role of theory in replication. We address all comments within the conceptual structure of the target article and hope to convince readers that replication in psychological science amounts to much more than hitting the lottery twice. Copyright (C) 2013 John Wiley &amp; Sons, Ltd.

-----------------------
 The authors of this reply article note that B. Gawronski, E. P. LeBel, K. R. Peters, and R. Banse (2009) (a) expressed agreement in their comment with the analysis put forward in the target article (J. De Houwer, S. Teige-Mocigemba, A. Spruyt, &amp; A. Moors, 2009) and (b) pointed to a further implication for the way in which the implicitness of a measure should be examined. The current authors note that B. A. Nosek and A. G. Greenwald (2009), on the other hand, raised questions in their comment about the definition of the concept "implicit" in the target article, arguing for a fundamentally different approach to measurement that emphasizes not theoretical understanding but usefulness for predicting behavior. In this reply, the current authors respond to these comments and argue that when theoretical claims are made about measures, these claims should be backed up with appropriate evidence. In the absence of basic research, measures and their relation to behavior can only be described.

 The main goal of our target article was to provide concrete recommendations for improving the replicability of research findings. Most of the comments focus on this point. In addition, a few comments were concerned with the distinction between replicability and generalizability and the role of theory in replication. We address all comments within the conceptual structure of the target article and hope to convince readers that replication in psychological science amounts to much more than hitting the lottery twice. Copyright (C) 2013 John Wiley &amp; Sons, Ltd.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 This Letter presents a search for high-mass resonances decaying into tau(+)tau(-) final states using proton-proton collisions at root s = 7 TeV produced by the Large Hadron Collider. The data were recorded with the ATLAS detector and correspond to an integrated luminosity of 4.6 fb(-1). No statistically significant excess above the Standard Model expectation is observed; 95% credibility upper limits are set on the cross section times branching fraction of Z' resonances decaying into tau(+)tau(-) pairs as a function of the resonance mass. As a result, Z' bosons of the Sequential Standard Model with masses less than 1.40 TeV are excluded at 95% credibility. (c) 2013 CERN. Published by Elsevier B.V. All rights reserved.

 Determining the relationship between conscious and unconscious influences is essential for obtaining valid estimates of the 2 types of influence. S. Joordens and P. M. Merikle (1993) recently argued that a redundancy relationship provides a plausible alternative to the independence model proposed by L. L. Jacoby, J. P. Toth, and A. P. Yonelinas (1993). In this article, the authors address Joordens and Merikle's concerns and still find the independence model preferable: First, the redundancy model requires the questionable assumption that a direct test (inclusion) is process pure. Second, results obtained with the independence model, but not with the redundancy model, converge with results from indirect tests. Finally, conclusions drawn from the independence model are in accordance with the theorizing that surrounds the concept of automaticity.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 The characteristics and organization of memories from World War II (WWII) were examined in relation to posttraumatic stress reactions. In Study 1, 145 Danes recalled and rated four memories from WWII. They rated the WWII period for posttraumatic stress reactions and importance to identity and life story. Memory clarity, rehearsal and consequences correlated positively with posttraumatic stress reactions and with WWII importance to identity and life story. In Study 2 it Subgroup of 58 participants nominated five life story memories, divided their life story into chapters and rated WWII for posttraumatic stress reactions and importance to identity and life story. Posttraumatic stress reactions cot-related positively with percentage of life story chapters about WWII, the tendency to connect non-WWII memories with the WWII period and subjective clarity and rehearsal of WWII memories. The results contradict the idea that posttraumatic stress reactions;Are associated with vague and poorly integrated trauma memories. Copyright (C) 2008 John Wiley &amp; Sons, Ltd.

 Differences between positive and negative autobiographical memories are often explained with reference to hypothesized evolutionary functions. Generally, it has been proposed that autobiographical memory serves directive, self-, and social functions. However, the relationship between emotional valence and the three functions has never been studied. In Study 1, participants generated memories that mapped onto each of the three functions. Directive memories were dominated by negative emotion, whereas self- and social memories were dominated by positive emotion. In Study 2, participants generated their most positive and most negative memories, as well as their most frequent involuntary and most vivid flashbulb memories, and the three functions were measured through rating-scale questions. The directive function had the lowest ratings across all memory classes, but, consistent with the results of Study 1, positive memories were rated higher on the self- and social functions, whereas negative memories were rated higher on the directive function.

-----------------------
 Previous studies have shown that EEG activity in the gamma range can be modulated by attention. Here, we compared this activity for voluntary and involuntary spatial attention in a spatial-cueing paradigm with faces as targets. The stimuli and trial timing were kept constant across attention conditions with only the predictive value of the cue changing. Gamma-band response was linked to voluntary shifts of attention, but not to the involuntary capture of attention. The presence of increased gamma responses for the voluntary allocation of attention, and its absence in cases of involuntary capture suggests that the neural mechanisms governing these two types of attention are different. Moreover, these data allow a description of the temporal dynamics contributing to the dissociation between voluntary and involuntary attention. The distribution of this correlate of voluntary attention is consistent with a top-down process involving contralateral anterior and posterior regions.

 We propose that voluntary and involuntary attention affect different mechanisms and have different consequences for performance measured in reaction time. Voluntary attention enhances the perceptual representation whereas involuntary attention affects the tendency to respond to stimuli in one location or another. In a spatial-cueing paradigm, we manipulated perceptual difficulty and compared voluntary and involuntary attention. For the voluntary-attention condition, the spatial cue was predictive of the target location, whereas in the involuntary-attention condition it was not. Increasing perceptual difficulty increased the attention effect with Voluntary attention, but decreased it with involuntary attention. Thus voluntary and involuntary attention have different consequences when perceptual difficulty is manipulated and hence are probably caused by different mechanisms.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 In this reply to Neal and Hesketh and to the commentators, we argue that implicit knowledge is partly abstract and can be usefully defined by the criteria of both metaknowledge and intentional control. We suggest that the pattern of dissociations supports a claim of separate implicit and explicit learning modes. According to our characterization, implicit learning leads to knowledge that is not automatically represented as knowledge by the learning process; instead, the presence of knowledge has to be inferred by the subject (partial explicitation) if metaknowledge is gained at all. During explicit learning, knowledge is automatically labeled as knowledge by the learning process, so that metaknowledge comes immediately and to the fullest extent. Finally, we suggest that implicit knowledge may to some degree apply regardless of intention.

 In this review, we consider three possible criteria by which knowledge might be regarded as implicit or inaccessible: It might be implicit only in the sense that it is difficult to articulate freely, or it might be implicit according to either an objective threshold or a subjective threshold. We evaluate evidence for these criteria in relation to artificial grammar learning, the control of complex systems, and sequence learning, respectively. We argue that the convincing evidence is not yet in, but construing the implicit nature of implicit learning in terms of a subjective threshold is most likely to prove fruitful for future research. Furthermore, the subjective threshold criterion may demarcate qualitatively different types of knowledge. We argue that (1) implicit, rather than explicit, knowledge is often relatively inflexible in transfer to different domains, (2) implicit, rather than explicit, learning occurs when attention is focused on specific items and not underlying rules, and (3) implicit learning and the resulting knowledge are often relatively robust.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 The influence of prior familiarity with components on the implicit learning of relations was examined using artificial grammar learning Prior to training on grammar strings, participants were familiarised with either the novel Symbols used to construct the strings or with irrelevant geometric shapes Participants familiarised with the relevant symbols showed greater accuracy when judging the correctness of new grammar strings Familiarity with elemental components did not increase conscious awareness of the basis for discriminations (structural knowledge) but increased accuracy even in its absence The subjective familiarity of test strings predicted grammaticality judgments However, prior exposure to relevant symbols did not increase overall test string familiarity or reliance oil familiarity when making grammaticality judgments Familiarity with the symbols increased the learning of relations between them (bigrams and trigrams) thus resulting ill greater familiarity for grammatical versus ungrammatical strings The results have important implications for models of implicit learning. (C) 2009 Elsevier Inc All rights reserved

 Dominant theories of implicit learning assume that implicit learning merely involves the learning of chunks of adjacent elements in a sequence. In the experiments presented here, participants implicitly learned a nonlocal rule, thus suggesting that implicit learning can go beyond the learning of chunks. Participants were exposed to a set of musical tunes that were all generated using a diatonic inversion. In the subsequent test phase, participants either classified test tunes as obeying a rule (direct test) or rated their liking for the tunes (indirect test). Both the direct and indirect tests were sensitive to knowledge of chunks. However, only the indirect test was sensitive to knowledge of the inversion rule. Furthermore., the indirect test was overall significantly more sensitive than the direct test, thus suggesting that knowledge of the inversion rule was below an objective threshold of awareness.

-----------------------
 Does a concurrent cognitive task affect the dynamics of bimanual rhythmic coordination? In-phase coordination was performed under manipulations of phase detuning and movement frequency and either singly or in combination with an arithmetic task. Predicted direction-specific shifts in stable relative phase from 0 degrees due to detuning and movement frequency were amplified by the cognitive task. Nonlinear cross-recurrence analysis suggested that this cognitive influence on the locations of the stable points or attractors of coordination entailed a magnification of attractor noise without a reduction in attractor strength. An approximation to these findings was achieved through parameter changes in a motion equation in relative phase. Results are discussed in terms of dual-task performance as limited resources, dynamics rather than chronometrics, and reparameterization rather than degradation.

 Cognitive performance exhibits patterns of trial-to-trial variation that can be described as 1/f or pink noise, as do repeated measures of locomotor performance. Although cognitive and locomotor performances are known to interact when performed concurrently, it is not known whether concurrent performance affects the tasks' pink noise dynamical structure. In this study, participants performed a cognitive task (repeatedly producing a temporal interval) and a motor task (walking on a treadmill) in single- and dual-task conditions. In single-task conditions both tasks exhibited pink noise structure. For concurrent performance the dynamical structure of the cognitive task changed reliably in the direction of white (random) noise. The dynamical structure of locomotion remained pink noise. The change in cognitive dynamics occurred despite no reliable changes in mean or standard deviation measures for either task. The results suggest a functional reorganization of cognitive dynamics supporting successful task performance in dual-task conditions.

-----------------------
 The extent to which human learning should be thought of in terms of elementary, automatic versus controlled, cognitive processes is unresolved after nearly a century of often fierce debate. Mitchell et al. provide a persuasive review of evidence against automatic, unconscious links. Indeed, unconscious processes seem to play a negligible role in any form of learning, not just in Pavlovian conditioning. But a modern connectionist framework, in which "cognitive" phenomena are emergent properties, is likely to offer a fuller account of human learning than the propositional framework Mitchell et al. propose.

 Since the very earliest experimental investigations of learning, tension has existed between association-based and cognitive theories. Associationism accounts for the phenomena of both conditioning and "higher" forms of learning via concepts such as excitation, inhibition, and reinforcement, whereas cognitive theories assume that learning depends on hypothesis testing, cognitive models, and propositional reasoning. Cognitive theories have received considerable impetus in regard to both human and animal learning from recent research suggesting that the key illustration of cue selection in learning, blocking, often arises from inferential reasoning. At the same time, a dichotomous view that separates noncognitive, unconscious (implicit) learning from cognitive, conscious (explicit) learning has gained favor. This review selectively describes key findings from this research, evaluates evidence for and against associative and cognitive explanatory constructs, and critically examines both the dichotomous view of learning as well as the claim that learning can occur unconsciously.

-----------------------
 The study of memory is witnessing a spirited clash between proponents of traditional laboratory research and those advocating a more naturalistic approach to the study of ''real-life'' or ''everyday'' memory. The debate has generally centered on the ''what'' (content), ''where'' (context), and ''how'' (methods) of memory research. In this target article, we argue that the controversy discloses a further, more fundamental breach between two underlying memory metaphors, each having distinct implications for memory theory and assessment: Whereas traditional memory research has been dominated by the storehouse metaphor, leading to a focus on the number of items remaining in store and accessible to memory, the recent wave of everyday memory research has shifted toward a correspondence metaphor, focusing on the accuracy of memory in representing past events. The correspondence metaphor calls for a research approach that differs from the traditional one in important respects: in emphasizing the intentional-representational function of memory, in addressing the wholistic and graded aspects of memory correspondence, in taking an output-bound assessment perspective, and in allowing more room for the operation of subject-controlled metamemory processes and motivational factors. This analysis can help tie together some of the what, where, and how aspects of the ''real-life/laboratory'' controversy. More important, however, by explicating the unique metatheoretical foundation of the accuracy-oriented approach to memory we aim to promote a more effective exploitation of the correspondence metaphor in both naturalistic and laboratory research contexts.

 There has been unprecedented interest in recent years in questions pertaining to accuracy and distortion in memory. This interest, catalyzed in part by real-life problems, marks a significant departure from the quantity-oriented approach that has characterized much of traditional memory research. We outline a correspondence metaphor of memory underlying accuracy-oriented research, and show how the features of this metaphor are manifested across the disparate bodies of research reviewed here. These include work in the Gestalt tradition, spatial memory, memory for gist, schema theory, source monitoring, fluency misattributions, false recall and recognition, postevent misinformation, false memories, eyewitness research, and autobiographical memory. In examining the dynamics of memory accuracy, we highlight the importance of metacognitive monitoring and control processes. We end by discussing some of the methodological, theoretical, and metatheoretical issues inherent in accuracy-oriented research, attempting to prepare the groundwork for a more coherent psychology of memory accuracy.

-----------------------
 The study of memory is witnessing a spirited clash between proponents of traditional laboratory research and those advocating a more naturalistic approach to the study of ''real-life'' or ''everyday'' memory. The debate has generally centered on the ''what'' (content), ''where'' (context), and ''how'' (methods) of memory research. In this target article, we argue that the controversy discloses a further, more fundamental breach between two underlying memory metaphors, each having distinct implications for memory theory and assessment: Whereas traditional memory research has been dominated by the storehouse metaphor, leading to a focus on the number of items remaining in store and accessible to memory, the recent wave of everyday memory research has shifted toward a correspondence metaphor, focusing on the accuracy of memory in representing past events. The correspondence metaphor calls for a research approach that differs from the traditional one in important respects: in emphasizing the intentional-representational function of memory, in addressing the wholistic and graded aspects of memory correspondence, in taking an output-bound assessment perspective, and in allowing more room for the operation of subject-controlled metamemory processes and motivational factors. This analysis can help tie together some of the what, where, and how aspects of the ''real-life/laboratory'' controversy. More important, however, by explicating the unique metatheoretical foundation of the accuracy-oriented approach to memory we aim to promote a more effective exploitation of the correspondence metaphor in both naturalistic and laboratory research contexts.

 There has been unprecedented interest in recent years in questions pertaining to accuracy and distortion in memory. This interest, catalyzed in part by real-life problems, marks a significant departure from the quantity-oriented approach that has characterized much of traditional memory research. We outline a correspondence metaphor of memory underlying accuracy-oriented research, and show how the features of this metaphor are manifested across the disparate bodies of research reviewed here. These include work in the Gestalt tradition, spatial memory, memory for gist, schema theory, source monitoring, fluency misattributions, false recall and recognition, postevent misinformation, false memories, eyewitness research, and autobiographical memory. In examining the dynamics of memory accuracy, we highlight the importance of metacognitive monitoring and control processes. We end by discussing some of the methodological, theoretical, and metatheoretical issues inherent in accuracy-oriented research, attempting to prepare the groundwork for a more coherent psychology of memory accuracy.

-----------------------
 Two lexical decision experiments investigated priming for a critical item (CI, sleep) and its related yoked associate (YA, blanket) when one had been studied in a related Deese/Roediger-McDermott (DRM) list (Experiments 1 &amp; 2) or a list of totally unrelated words (Experiment 2) and the other had been nonstudied. Semantic priming from the related DRM list occurred for nonstudied CIs (but not YAs) regardless of whether the Cl received within-test priming from its studied related YA during the lexical decision task, though the effect in the absence of within-test priming averaged across experiments was only significant by a one-tailed test. Also averaged across experiments, repetition priming occurred for both studied Os and YAs when they had been studied in related DRM lists whether or not there was also within-test priming from a nonstudied related yoked pairmate, though individual effects within the two experiments were sometimes not significant. Repetition priming boosted semantic priming from related DRM lists less for CIs than for YAs, similar to the finding that memory discriminability is poorer for CIs than for YAs in episodic recognition. This smaller repetition priming boost for CIs than for YAs occurred to the same degree when the CIs or YAs were studied in an unrelated list. When nonstudied CIs and YAs were totally unrelated to all previously studied items and separated by 3-7 items in the lexical decision task, a YA produced a small 16 msec priming effect for its CI, averaged across both experiments. The implications of these results for the activation account of the DRM false-memory effect and for single-prime versus multiple-prime long-term semantic priming effects are discussed.

 Four experiments examined whether studying a single Deese/Roediger-McDermott (DRM) list produces semantic priming for nonstudied critical items (Cls) and semantic + repetition priming for studied associates. After 30 s of mental arithmetic that followed the study of a DRM list, priming was assessed in a lexical decision task when the nonwords were either pronounceable (Experiment 1) or pseudohomophones (Experiments 2 - 4). Priming was measured relative to a baseline containing exactly the same Cls and associates that had not been primed by their related DRM lists. Significant CI semantic priming effects occurred in all four experiments, whether or not there was within-test priming from a related associate preceding the CI by 3 - 7 items. To our knowledge, these are the first experiments using standard DRM study procedures to provide a convincing demonstration of a genuine CI semantic priming effect in a delayed indirect memory test that should be free of intentional retrieval strategies. Discussion focuses on measuring long-term semantic activation effects without the influence of source monitoring in a lexical decision task. (c) 2005 Elsevier Inc. All rights reserved.

-----------------------
 The study addressed the hypothesis that the content of confabulation is emotionally biased. Confabulating amnesic patients were compared with amnesic non-confabulating patients in a memory recognition experiment that manipulated the valence (pleasant, unpleasant), temporal source (past, present, future) and selection agent (self, other) of the to-be-recognised memories. The results revealed that confabulating patients were more likely than amnesic non-confabulating patients to incorrectly recognise past autobiographical events or thoughts as currently relevant memories, and this was more pronounced for pleasant compared to unpleasant events. These findings suggest that motivational factors, along with defective reality and temporality monitoring, contribute to confabulation. (c) 2007 Elsevier Ltd. All rights reserved.

 There has been little experimental work investigating the emotional content of confabulation, despite clinical descriptions of self-serving and affectively positive biases. False memories were elicited in 10 amnesic confabulating patients, 10 healthy controls and four amnesic control patients without confabulation. Memory protocols of the interviews with these groups were presented to naive raters who were asked to rate the emotional valence of the listed confabulations. The false memories of the confabulating patients were found to distort previous experiences in ways significantly more pleasant and self-enhancing than those of controls. It was also found paradoxically that the more depressed the patients' mood the more positive the content of their confabulations. These findings suggest that the content of confabulation is mostly positive. The results have implications for the role of emotion and motivation in confabulation, as well as for the clinical management of confabulating patients. (C) 2007 Elsevier Masson Srl. All rights reserved.

-----------------------
 There has been little experimental work investigating the emotional content of confabulation, despite clinical descriptions of self-serving and affectively positive biases. False memories were elicited in 10 amnesic confabulating patients, 10 healthy controls and four amnesic control patients without confabulation. Memory protocols of the interviews with these groups were presented to naive raters who were asked to rate the emotional valence of the listed confabulations. The false memories of the confabulating patients were found to distort previous experiences in ways significantly more pleasant and self-enhancing than those of controls. It was also found paradoxically that the more depressed the patients' mood the more positive the content of their confabulations. These findings suggest that the content of confabulation is mostly positive. The results have implications for the role of emotion and motivation in confabulation, as well as for the clinical management of confabulating patients. (C) 2007 Elsevier Masson Srl. All rights reserved.

 Schizophrenia is a severe mental illness, which affects sense of identity. While the ability to have a coherent vision of the self (i.e., self-images) relies partly on its reciprocal relationships with autobiographical memories, little is known about how memories ground "self-images" in schizophrenia. Twenty-five patients with schizophrenia and 25 controls were asked to give six autobiographical memories related to four self-statements they considered essential for defining their identity. Results showed that patients' self-images were more passive than those of controls. Autobiographical memories underlying self-images were less thematically linked to these self-images in patients. We also found evidence of a weakened sense of self and a deficient organization of autobiographical memories grounding the self in schizophrenia. These abnormalities may account for the poor cohesiveness of the self in schizophrenia. (C) 2011 Elsevier Inc. All rights reserved.

-----------------------
 The relations between automatic processing and (the absence of) consciousness are discussed in this paper. It is argued that automatic processing should not be identified with the absence of consciousness. The organism has access to representations resulting from automatic processing, but these representations, in contrast to the representations resulting from nonautomatic processing, are not propositional. Therefore monitoring of the process, the defining feature of nonautomatic processing, is not possible. (C) 1997 Academic Press.

 We apply Dienes &amp; Perner's (D&amp;P's) framework to the automatic/nonautomatic processing contrast. Our analysis leads to the conclusion that automatic and nonautomatic processing result in representations that have explicit results. We propose equating consciousness with explicitness of aspects rather than with full explicitness as defined by D&amp;P.

-----------------------
 The relations between automatic processing and (the absence of) consciousness are discussed in this paper. It is argued that automatic processing should not be identified with the absence of consciousness. The organism has access to representations resulting from automatic processing, but these representations, in contrast to the representations resulting from nonautomatic processing, are not propositional. Therefore monitoring of the process, the defining feature of nonautomatic processing, is not possible. (C) 1997 Academic Press.

 We apply Dienes &amp; Perner's (D&amp;P's) framework to the automatic/nonautomatic processing contrast. Our analysis leads to the conclusion that automatic and nonautomatic processing result in representations that have explicit results. We propose equating consciousness with explicitness of aspects rather than with full explicitness as defined by D&amp;P.

-----------------------
 Advance preparation reduces RT task-switching cost, which is thought to be evidence of preparatory control in the cuing task-switching paradigm. In the present study, we emphasize errors in relation to response speed. In two experiments, we show that (1) task switching increased the rate at which the currently irrelevant task was erroneously executed ("task errors") and (2) advance task preparation reduced the task error rate to that seen in nonswitch trials. The implications of the results to the hypothesis concerning task-specific preparation are discussed.

 A part of action preparation is deciding what the relevant task is. This task-decision process is conceptually separate from response selection. To show this, the authors manipulated task conflict in a spatial task-switching paradigm, using conflict stimuli that appeared during trials with univalent targets (affording 1 task). The conflict stimuli afforded task identity because they were used as task cues with bivalent targets (affording 2 tasks) that were intermixed with the univalent targets. Thus, for univalent targets, irrelevant stimuli either caused low task conflict or high task conflict. In three experiments, the authors found poorer performance in high task conflict trials than in low task conflict trials. Task conflict was introduced during target appearance (Experiment 1) or task preparation (Experiments 2 and 3). In the latter case, the task conflict effect decreased with increasing task preparation time showing that task preparation involves task decision.

-----------------------
 We devised criterial recollection tests to investigate why testing memory for pictures elicits lower false recognition than testing memory for words. Subjects studied unrelated black words paired either with the same word in red font, a corresponding picture, or both. They then took three memory tests, always using black words: a recognition test (say "yes" to all studied items), a red word-test, and a picture-test (say "yes" only if you recollect a red word or a picture, respectively). Regardless of whether pictures were more or less familiar than red words, false recognition was lowest on the picture test. These results cannot be explained easily by familiarity or strength-based criterion shifts. Instead, they suggest that subjects expected more detailed recollections for pictures, thereby facilitating a diagnostic monitoring process (the "distinctiveness heuristic"). This recollective difference also influenced source monitoring errors (an "it-had-to-be-a-word" effect), again suggesting that detailed recollective expectations influence monitoring processes. (C) 2004 Elsevier Inc. All rights reserved.

 According to the distinctiveness heuristic, subjects rely more on detailed recollections (and less on familiarity) when memory is tested for pictures relative to words, leading to reduced false recognition. If so, then neural regions that have been implicated in effortful postretrieval monitoring (e. g., dorsolateral prefrontal cortex) might be recruited less heavily when trying to remember pictures. We tested this prediction with the criterial recollection task. Subjects studied black words, paired with either the same word in red font or a corresponding colored picture. Red words were repeated at study to equate recognition hits for red words and pictures. During fMRI scanning, alternating red word memory tests and picture memory tests were given, using only white words as test stimuli (say "yes" only if you recollect a corresponding red word or picture, respectively). These tests were designed so that subjects had to rely on memory for the criterial information. Replicating prior behavioral work, we found enhanced rejection of lures on the picture test compared to the red word test, indicating that subjects had used a distinctiveness heuristic. Critically, dorsolateral prefrontal activity was reduced when rejecting familiar lures on the picture test, relative to the red word test. These findings indicate that reducing false recognition via the distinctiveness heuristic is not heavily dependent on frontally mediated postretrieval monitoring processes.

-----------------------
 We devised criterial recollection tests to investigate why testing memory for pictures elicits lower false recognition than testing memory for words. Subjects studied unrelated black words paired either with the same word in red font, a corresponding picture, or both. They then took three memory tests, always using black words: a recognition test (say "yes" to all studied items), a red word-test, and a picture-test (say "yes" only if you recollect a red word or a picture, respectively). Regardless of whether pictures were more or less familiar than red words, false recognition was lowest on the picture test. These results cannot be explained easily by familiarity or strength-based criterion shifts. Instead, they suggest that subjects expected more detailed recollections for pictures, thereby facilitating a diagnostic monitoring process (the "distinctiveness heuristic"). This recollective difference also influenced source monitoring errors (an "it-had-to-be-a-word" effect), again suggesting that detailed recollective expectations influence monitoring processes. (C) 2004 Elsevier Inc. All rights reserved.

 According to the distinctiveness heuristic, subjects rely more on detailed recollections (and less on familiarity) when memory is tested for pictures relative to words, leading to reduced false recognition. If so, then neural regions that have been implicated in effortful postretrieval monitoring (e. g., dorsolateral prefrontal cortex) might be recruited less heavily when trying to remember pictures. We tested this prediction with the criterial recollection task. Subjects studied black words, paired with either the same word in red font or a corresponding colored picture. Red words were repeated at study to equate recognition hits for red words and pictures. During fMRI scanning, alternating red word memory tests and picture memory tests were given, using only white words as test stimuli (say "yes" only if you recollect a corresponding red word or picture, respectively). These tests were designed so that subjects had to rely on memory for the criterial information. Replicating prior behavioral work, we found enhanced rejection of lures on the picture test compared to the red word test, indicating that subjects had used a distinctiveness heuristic. Critically, dorsolateral prefrontal activity was reduced when rejecting familiar lures on the picture test, relative to the red word test. These findings indicate that reducing false recognition via the distinctiveness heuristic is not heavily dependent on frontally mediated postretrieval monitoring processes.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 The study addressed the hypothesis that the content of confabulation is emotionally biased. Confabulating amnesic patients were compared with amnesic non-confabulating patients in a memory recognition experiment that manipulated the valence (pleasant, unpleasant), temporal source (past, present, future) and selection agent (self, other) of the to-be-recognised memories. The results revealed that confabulating patients were more likely than amnesic non-confabulating patients to incorrectly recognise past autobiographical events or thoughts as currently relevant memories, and this was more pronounced for pleasant compared to unpleasant events. These findings suggest that motivational factors, along with defective reality and temporality monitoring, contribute to confabulation. (c) 2007 Elsevier Ltd. All rights reserved.

 There has been little experimental work investigating the emotional content of confabulation, despite clinical descriptions of self-serving and affectively positive biases. False memories were elicited in 10 amnesic confabulating patients, 10 healthy controls and four amnesic control patients without confabulation. Memory protocols of the interviews with these groups were presented to naive raters who were asked to rate the emotional valence of the listed confabulations. The false memories of the confabulating patients were found to distort previous experiences in ways significantly more pleasant and self-enhancing than those of controls. It was also found paradoxically that the more depressed the patients' mood the more positive the content of their confabulations. These findings suggest that the content of confabulation is mostly positive. The results have implications for the role of emotion and motivation in confabulation, as well as for the clinical management of confabulating patients. (C) 2007 Elsevier Masson Srl. All rights reserved.

-----------------------
 The study addressed the hypothesis that the content of confabulation is emotionally biased. Confabulating amnesic patients were compared with amnesic non-confabulating patients in a memory recognition experiment that manipulated the valence (pleasant, unpleasant), temporal source (past, present, future) and selection agent (self, other) of the to-be-recognised memories. The results revealed that confabulating patients were more likely than amnesic non-confabulating patients to incorrectly recognise past autobiographical events or thoughts as currently relevant memories, and this was more pronounced for pleasant compared to unpleasant events. These findings suggest that motivational factors, along with defective reality and temporality monitoring, contribute to confabulation. (c) 2007 Elsevier Ltd. All rights reserved.

 There has been little experimental work investigating the emotional content of confabulation, despite clinical descriptions of self-serving and affectively positive biases. False memories were elicited in 10 amnesic confabulating patients, 10 healthy controls and four amnesic control patients without confabulation. Memory protocols of the interviews with these groups were presented to naive raters who were asked to rate the emotional valence of the listed confabulations. The false memories of the confabulating patients were found to distort previous experiences in ways significantly more pleasant and self-enhancing than those of controls. It was also found paradoxically that the more depressed the patients' mood the more positive the content of their confabulations. These findings suggest that the content of confabulation is mostly positive. The results have implications for the role of emotion and motivation in confabulation, as well as for the clinical management of confabulating patients. (C) 2007 Elsevier Masson Srl. All rights reserved.

-----------------------
 In the current literature about human learning, the distinction between implicit and explicit learning is broadly accepted. In the present paper we will argue that the separation into two types of learning has no solid ground - neither theoretically nor empirically. Based on recent definitions of implicit and explicit learning, the distinction between intentional and non-intentional learning is suggested as the scientifically more useful one - a distinction that has been already drawn at the beginning of the history of German psychology. Two research traditions - Wundt' s theory of apperception and Kohler's concept of,,Gestalten" - are exemplary discussed in order to demonstrate that, on a theoretical level, the concept of "intention" is not a major variable for understanding human learning. Also, the empirical distinction between two types of learning, for instance, dissociations between intentional and non-intentional sequence learning, is problematic because of limitations in the interpretation of indicators for both types of learning.

 Sequence teaming tasks rue very simple choice reaction tasks which have become surprisingly popular in cognitive psychology. We analyse possible causes for this popularity by looking at the theoretical status of sequence teaming tasks, and we provide a sketch of the relevant research activities, Two categories of research activities are distinguished. First, sequence learning tasks are used to investigate dissociations between qualitatively different explicit (often referred to as 'conscious') and implicit (often referred to as 'unconscious') learning processes. Second, learning conditions are manipulated directly in order to analyse the mechanisms underlying sequence learning. We conclude that the claimed dissociations have yet to be demonstrated convincingly. Nevertheless, the use of sequence learning tasks can result in valuable contributions to the understanding of sequence learning mechanisms per se.

-----------------------
 Memory distortion occurs in the laboratory and in everyday life. This article focuses on false recognition, a common type of memory distortion in which individuals incorrectly claim to have encountered a novel object or event. By considering evidence from neuropsychology, neuroimaging, and electrophysiology, we address three questions. (1) Are there patterns of neural activity that can distinguish between true and false recognition? (2) Which brain regions contribute to false recognition? (3) Which brain regions play a role in monitoring or reducing false recognition? Neuroimaging and electrophysiological studies suggest that sensory activity is greater for true recognition compared to false recognition. Neuropsychological and neuroimaging results indicate that the hippocampus and several cortical regions contribute to false recognition. Evidence from neuropsychology, neuroimaging, and electrophysiology implicates the prefrontal cortex in retrieval monitoring that can limit the rate of false recognition.

 False recognition, a type of memory distortion where one claims to remember something that never happened, can occur in response to items that are similar but not identical to previously seen items (i.e., related false recognition) or in response to novel items (i.e., unrelated false recognition). It is unknown whether these 2 types of memory errors arise from the same or distinct neural substrates. Using functional magnetic resonance imaging, we compared the neural activity associated with true recognition, related false recognition, and unrelated false recognition for abstract shapes. True recognition and related false recognition were associated with similar patterns of neural activity, including activity in the prefrontal cortex, the parietal cortex, and the medial temporal lobe. By contrast, unrelated false recognition was associated with activity in language-processing regions. These results indicate that false recognition is not a unitary phenomenon, but rather can reflect the operation of 2 distinct cognitive and neural processes.

-----------------------
 The functional form of performance improvements has been extensively studied in speeded cognitive and motor tasks; in such tasks, reductions in response times have been characterized by the ubiquitous power law of learning or by a simpler exponential form. Performance improvements in perceptual capacities are also important in expertise, but their functional form is unknown. This study investigated the functional form of perceptual learning. For individual observers, reductions in thresholds were best described by an exponential function, rather than a power or compound exponential and power (apex) function. Learning was specific to orientation, a result that supports the perceptual locus of the learning, and was decoupled in high and low external noise, a result that reflects separable learning mechanisms in the two conditions. The simple exponential form of learning implies a constant relative rate of learning throughout practice; there was no evidence supporting multilevel hypotheses, such as serial reverse hierarchical and parallel-learning models, that posit multiple processes of learning characterized by different rates.

 To explore the relationship between category and perceptual learning, we examined both category and perceptual learning in patients with treated Wilson's disease (WD), whose basal ganglia, known to be important in category learning, were damaged by the disease. We measured their learning rate and accuracy in rule-based and information-integration category learning, and magnitudes of perceptual learning in a wide range of external noise conditions, and compared the results with those of normal controls. The WD subjects exhibited deficits in both forms of category learning and in perceptual learning in high external noise. However, their perceptual learning in low external noise was relatively spared. There was no significant correlation between the two forms of category learning, nor between perceptual learning in low external noise and either form of category learning. Perceptual learning in high external noise was, however, significantly correlated with information-integration but not with rule-based category learning. The results suggest that there may be a strong link between information-integration category learning and perceptual learning in high external noise. Damage to brain structures that are important for information-integration category learning may lead to poor perceptual learning in high external noise, yet spare perceptual learning in low external noise. Perceptual learning in high and low external noise conditions may involve separate neural substrates.

-----------------------
 The authors evaluated 4 sequential sampling models for 2-choice decisions-the Wiener diffusion, Ornstein-Uhlenbeck (OU) diffusion, accumulator, and Poisson counter models-by fitting them to the response time (RT) distributions and accuracy data from 3 experiments. Each of the models was augmented with assumptions of variability across trials in the rate of accumulation of evidence from stimuli, the values of response criteria, and the value of base RT across trials. Although there was substantial model mimicry, empirical conditions were identified under which the models make discriminably different predictions. The best accounts of the data were provided by the Wiener diffusion model, the OU model with small-to-moderate decay, and the accumulator model with long-tailed (exponential) distributions of criteria, although the last was unable to produce error RTs shorter than correct RTs. The relationship between these models and 3 recent, neurally inspired models was also examined.

 In this letter, we examine the computational mechanisms of reinforcement-based decision making. We bridge the gap across multiple levels of analysis, from neural models of corticostriatal circuits-the basal ganglia (BG) model (Frank, 2005, 2006) to simpler but mathematically tractable diffusion models of two-choice decision making. Specifically, we generated simulated data from the BG model and fit the diffusion model (Ratcliff, 1978) to it. The standard diffusion model fits underestimated response times under conditions of high response and reinforcement conflict. Follow-up fits showed good fits to the data both by increasing nondecision time and by raising decision thresholds as a function of conflict and by allowing this threshold to collapse with time. This profile captures the role and dynamics of the subthalamic nucleus in BG circuitry, and as such, parametric modulations of projection strengths from this nucleus were associated with parametric increases in decision boundary and its modulation by conflict. We then present data from a human reinforcement learning experiment involving decisions with low-and high-reinforcement conflict. Again, the standard model failed to fit the data, but we found that two variants similar to those that fit the BG model data fit the experimental data, thereby providing a convergence of theoretical accounts of complex interactive decision-making mechanisms consistent with available data. This work also demonstrates how to make modest modifications to diffusion models to summarize core computations of the BG model. The result is a better fit and understanding of reinforcement-based choice data than that which would have occurred with either model alone.

-----------------------
 This work describes a cognitively realistic approach to social simulation. It begins with a model created by Gilbert [4] for capturing the growth of academic science. Gilbert's model, which was equation-based, is replaced here by an agent-based (neural network) model, with the (neural network based) cognitive architecture CLARION providing greater cognitive realism. Using this agent model, results comparable to previous simulations and to human data are obtained. It is found that while different cognitive settings may affect the aggregate number of scientific articles produced by the model, they do not generally lead to different distributions of number of articles per author. It is argued that using more cognitively realistic models in simulations may lead to novel insights.

 This paper explores how mental disorders of certain types might be explained based on mechanisms and processes of human motivation (including drives and goals) and action selection (as well as other related mechanisms and processes), within a generic, comprehensive computational cognitive architecture model. Several simulation tests have been conducted that demonstrate that the model is reasonable, and captures some characteristics of certain mental disorders. The work is a first step in showing the feasibility of integrating mental disorders modeling/simulation into a cognitive architecture model.

-----------------------
 Prospective memory (ProM) refers to our ability to become aware of a previously formed plan at the right time and place. After two decades of research on prospective memory and aging, narrative reviews and summaries have arrived at widely different conclusions. One view is that prospective memory shows large age declines, larger than age declines on retrospective memory (RetM). Another view is that prospective memory is an exception to age declines and remains invariant across the adult lifespan. The present meta-analysis of over twenty years of research settles this controversy. It shows that prospective memory declines with aging and that the magnitude of age decline varies by prospective memory subdomain (vigilance, prospective memory proper, habitual prospective memory) as well as test setting (laboratory, natural). Moreover, this meta-analysis demonstrates that previous claims of no age declines in prospective memory are artifacts of methodological and conceptual issues afflicting prior research including widespread ceiling effects, low statistical power, age confounds, and failure to distinguish between various subdomains of prospective memory (e. g., vigilance and prospective memory proper).

 Background: Prospective memory (ProM) is the ability to become aware of a previously-formed plan at the right time and place. For over twenty years, researchers have been debating whether prospective memory declines with aging or whether it is spared by aging and, most recently, whether aging spares prospective memory with focal vs. non-focal cues. Two recent meta-analyses examining these claims did not include all relevant studies and ignored prevalent ceiling effects, age confounds, and did not distinguish between prospective memory subdomains (e. g., ProM proper, vigilance, habitual ProM) (see Uttl, 2008, PLoS ONE). The present meta-analysis focuses on the following questions: Does prospective memory decline with aging? Does prospective memory with focal vs. non-focal cues decline with aging? Does the size of age-related declines with focal vs. non-focal cues vary across ProM subdomains? And are age-related declines in ProM smaller than age-related declines in retrospective memory?

-----------------------
 Prospective memory is required for many aspects of everyday cognition, its breakdown may be as debilitating as impairments in retrospective memory, and yet, the former has received relatively little attention by memory researchers. This article outlines a strategy for changing the fortunes of prospective memory, for guiding new research to shore up the claim that prospective memory is a distinct aspect of cognition, and to obtain evidence for clear performance dissociations between prospective memory and other memory functions. We begin by identifying the unique requirements of prospective memory tasks and by dividing memory's prospective functions into subdomains that are analogous to divisions in retrospective memory (e.g., short- versus long-term memory). We focus on one prospective function, called prospective memory proper we define this function in the spirit of James (1890) as requiring that we are aware of a plan, of which meanwhile we have not been thinking, with the additional consciousness that we made the plan earlier. We give an operational definition of prospective memory Proper and specify how ill differs from explicit and implicit retrospective memory and how it might be empirically assessed. (C) 2001 Elsevier Science.

 Are self-report measures of prospective memory (ProM) reliable and valid? To examine this question, 240 undergraduate student volunteers completed several widely used self-report measures of ProM including the Prospective Memory Questionnaire (PMQ), the Prospective and Retrospective Memory Questionnaire (PRMQ), the Comprehensive Assessment of Prospective Memory (CAPM) questionnaire, self-reports of retrospective memory (RetM), objective measures of ProM and RetM, and measures of involvement in activities and events, memory strategies and aids use, personality and verbal intelligence. The results showed that both convergent and divergent validity of ProM self-reports are poor, even though we assessed ProM using a newly developed, reliable continuous measure. Further analyses showed that a substantial proportion of variability in ProM self-report scores was due to verbal intelligence, personality (conscientiousness, neuroticism), activities and event involvement (busyness), and use of memory strategies and aids. ProM self-reports have adequate reliability, but poor validity and should not be interpreted as reflecting ProM ability.

-----------------------
 Thirty-six 2-, 4- and 6-month-old infants were videotaped while interacting with a female adult stranger engaging in either organized or disorganized 1-min peekaboo games. Two-month-old infants gazed and smiled equally at the stranger, regardless of the relative organization of the peekaboo game. In contrast, 4- and 6-month-old infants smiled significantly more and gazed significantly less;in the organized peekaboo condition than in the disorganized peekaboo condition. These results suggest that from a diffuse sensitivity to the presence of a social partner, infants by 4 months develop a new sensitivity to the narrative envelope of protoconversation, in particular the timing and the structure of social exchanges scaffolded by adults. These observations are interpreted as evidence of developing social expectations in the first 6 months of life. This early development is viewed as announcing and preparing the communicative competence that blossoms by the end of the 1st year.

 Two-month-old infants (N = 29) participated in face-to-face interactions with their mothers and with strangers. The contingent responsiveness for smiles and vocalizations, while attending to the partner, was assessed for each partner in both interactions. For smiles and for vocalizations, infants were less responsive to the stranger relative to the mother when the stranger's contingent responsiveness was either more contingent or less contingent than that of the mother. Results are supportive of the hypothesis that young infants develop sensitivities to levels of social contingency present in their maternal interactions, which influence their responsiveness to others.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hemianopic reading and visual exploration impairments are well-known clinical phenomena. Yet. it is unclear whether they are primarily caused by the hemianopic visual field defect itself or by additional brain injury preventing efficient spontaneous oculomotor adaptation. To establish the extent to which these impairments are visually elicited we simulated unilateral homonymous hemianopia in healthy participants, using a gaze-contingent display paradigm, and investigated its effect on reading and visual exploration. We demonstrate that simulated hemianopia induces the reading and visual exploration impairments of hemianopic patients. Over time, however, all participants showed efficient spontaneous oculomotor adaptation to the visual-sensory loss which improved their reading and visual exploration performance. Our results suggest that the hemianopic visual field defect is a major component of the chronic impairments of reading and visual and exploration found in hemianopic patients although it may not be their sole cause. (C) 2008 Elsevier Ltd. All rights reserved.

 Reading and visual exploration impairments in unilateral homonymous hemianopia are well-established clinical phenomena. Spontaneous adaptation of eye-movements to the visual field defect leads to improved reading and visual exploration performance. Yet, it is still unclear whether oculomotor adaptation to visual field loss is task-specific or whether there is a transfer of adaptation-related improvements between reading and visual exploration. We therefore simulated unilateral homonymous hemianopia in healthy participants and explored the specificity with which oculomotor adaptation to this pure visual-sensory dysfunction during uninstructed reading or visual exploration practice leads to improvements in both abilities. Our findings demonstrate that there is no transfer of adaptation-related changes of eye-movements and performance improvements between reading and visual exploration. Efficient oculomotor adaptation to visual field loss is highly specific and task-dependent. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Displacing prisms and galvanic stimulation were used to examine visual-vestibular interactions during target-directed gait. Participants walked towards a wall 6 m away. After taking four steps, a target on the wall, located directly in front or to the right of the participant, was illuminated. Participants continued walking towards the target. Galvanic vestibular stimulation was triggered at either gait initiation, a step before the potential turn, or at target illumination. Although the visual and vestibular perturbations significantly altered gait trajectory, the greatest interaction occurred when galvanic stimulation was triggered one step before the target appeared. This implies an increase in the weighting of vestibular inputs just before turning to prepare for the potential change in direction.

 The adaptation of reaching movements has typically been investigated by either distorting visual feedback of the reaching limb or by distorting the forces acting upon the reaching limb. Here, we investigate reach adaptation when error is created by systematically perturbing the target of the reach rather than the limb itself (Magescas and Prablanc in J Cogn Neurosci 18: 75-83, 2006). Specifically, we investigate how adaptation is affected by (1) the timing of the perturbation with respect to the movement of the eye and the hand and (2) participant awareness of the perturbation. In Experiment 1, participants looked and pointed to a target that disappeared either at the onset of their eye movement or shortly after their eye movement and then reappeared, displaced to the right, at the completion of the reach. In Experiment 2, we made the target displacement more explicit by leaving the target at its initial location until the end of the reach, at which point it was displaced to the right. In Experiment 3, we extinguished the target at the onset of the eye movement but also informed participants about the presence and magnitude of the perturbation. In the no-feedback post-test phase, participants for whom the target disappeared during the reach demonstrated much stronger aftereffects of the perturbation, misreaching to the right, whereas participants for whom the target stayed on until reach completion demonstrated rapid extinction of rightward misreaching. Furthermore, participants who were informed about the target perturbation exhibited faster de-adaptation than those who were not. Our results suggest that adaptation to a target displacement is contingent on the explicitness of the target perturbation, whether this is achieved by manipulating stimulus timing or instruction.

-----------------------
 Two experiments examined the impact of attention on sensorimotor skills. In Experiment 1, experienced golfers putted under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step putting performance. Dual-task condition putting was more accurate. In Experiment 2, right-footed novice and experienced soccer players dribbled through a slalom course under dual-task or skill-focused conditions. When using their dominant right foot, experts again performed better in the dual-task condition. However, when using their less proficient left foot, experts performed better in the skill-focused condition. Novices performed better under skill-focus regardless of foot. Whereas novices and the less-proficient performances of experts benefit from online attentional monitoring of step-by-step performance, high-level skill execution is harmed.

 In two experiments, we examined the attentional mechanisms governing sensorimotor skill execution across levels of expertise, In Experiment 1, novice and expert golfers took a series of putts under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step performance. Novices performed better under skill-focused than under dual-task conditions. Experts showed the opposite pattern. In Experiment 2, novice and expert golfers putted under instructions that emphasized either putting accuracy or speed-the latter intended to reduce the time available to monitor and explicitly adjust execution parameters. Novices putted better under accuracy instructions. Experts were more accurate under speed instructions. In agreement with theories of skill acquisition and automaticity, novice performance is enhanced by conditions that allow for on-line attentional monitoring (e.g., skill-focused or accuracy instructions) in comparison with conditions that prevent explicit attentional control of skill execution (e.g., dud-task or speed constraints). In contrast, the proceduralized skill of experts benefits from environments that limit, rather than encourage, attention to execution.

-----------------------
 Clark &amp; Thornton's conception finds an echo in implicit learning research, which shows that subjects may perform adaptively in complex structured situations through the use of simple statistical learning mechanisms. However, the authors fail to draw a distinction between, on the one hand, subjects' representations which emerge from type-1 learning mechanisms, and, on the other, their knowledge of the genuine abstract ''recoding function'' which defines a type-2 problem.

 The domain-general learning mechanisms elicited in incidental learning situations are of potential interest in many research fields, including language acquisition, object knowledge formation and motor learning. They have been the focus of studies on implicit learning for nearly 40 years. Stemming from a different research tradition, studies on statistical learning carried out in the past 10 years after the seminal studies by Saffran and collaborators, appear to be closely related, and the similarity between the two approaches is strengthened further by their recent evolution. However, implicit learning and statistical learning research favor different interpretations, focusing on the formation of chunks and statistical computations, respectively. We examine these differing approaches and suggest that this divergence opens up a major theoretical challenge for future studies.

-----------------------
 The extent to which human learning should be thought of in terms of elementary, automatic versus controlled, cognitive processes is unresolved after nearly a century of often fierce debate. Mitchell et al. provide a persuasive review of evidence against automatic, unconscious links. Indeed, unconscious processes seem to play a negligible role in any form of learning, not just in Pavlovian conditioning. But a modern connectionist framework, in which "cognitive" phenomena are emergent properties, is likely to offer a fuller account of human learning than the propositional framework Mitchell et al. propose.

 Since the very earliest experimental investigations of learning, tension has existed between association-based and cognitive theories. Associationism accounts for the phenomena of both conditioning and "higher" forms of learning via concepts such as excitation, inhibition, and reinforcement, whereas cognitive theories assume that learning depends on hypothesis testing, cognitive models, and propositional reasoning. Cognitive theories have received considerable impetus in regard to both human and animal learning from recent research suggesting that the key illustration of cue selection in learning, blocking, often arises from inferential reasoning. At the same time, a dichotomous view that separates noncognitive, unconscious (implicit) learning from cognitive, conscious (explicit) learning has gained favor. This review selectively describes key findings from this research, evaluates evidence for and against associative and cognitive explanatory constructs, and critically examines both the dichotomous view of learning as well as the claim that learning can occur unconsciously.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 The "violation of expectancy" looking-time methodology has proven a powerful tool for exploring prelinguistic mental representations in human infants as well as in nonhuman primates. Four studies applying this methodology to the question of spontaneous number representations in cotton-top tamarins (Saguinus oedipus) are reported here. Monkeys were shown 1 + 1 events in which objects were placed behind a screen, 1 by 1. The screen was removed, revealing consistent (2 objects) and inconsistent (1, 3, or 1 large object twice the mass of original object) outcomes. In all studies, monkeys looked longer at the inconsistent than at the consistent outcome. When the monkeys view a 1 + 1 operation, they expect exactly 2 objects. It is likely that these numerical representations are spontaneously available to a variety of primate species and could provide a foundation on which humans' number sense was constructed over evolution and development.

 Infants know that humans are exempt from some of the principles that govern the motion of inanimate objects: for instance, humans can be caused to move without being struck. In the current study, we report that infants nevertheless do apply some of the same principles to both humans and objects, where appropriate. Five-month-old infants expect humans, like all material objects, to be solid. (c) 2005 Published by Elsevier B.V.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 We examined deaf and hearing children's progression of steps in theory of mind (ToM) development including their understanding of social pretending. Ninety-three children (33 deaf; 60 hearing) aged 3-13 years were tested on a set of six closely matched ToM tasks. Results showed that deaf children were delayed substantially behind hearing children in understanding pretending, false belief (FB) and other ToM concepts, in line with their delayed uptake of social pretend (SP) play. By using a scaling methodology, we confirmed previous evidence of a consistent five-step developmental progression for both groups. Moreover, by including social pretence understanding, both deaf and hearing children's ToM sequences were shown to extend reliably to six sequential developmental steps. Finally and focally, even though both groups' sequences were six steps long, the placement of pretence relative to other ToM milestones varied with hearing status. Deaf children understood social pretending at an earlier step in the ToM sequence than hearing children, albeit at a later chronological age. Theoretically, the findings are relevant to questions about how universal developmental progressions come together along with culturally distinctive inputs and biological factors (such as hearing loss) to set the pace for ToM development.

 To examine cultural contrasts in the ordered sequence of conceptual developments leading to theory of mind (ToM), we compared 135 3- to 6-year-olds (77 Australians; 58 Iranians) on an established 5-step ToM scale (Wellman &amp; Liu, 2004). There was a cross-cultural difference in the sequencing of ToM steps but not in overall rates of ToM mastery. In line with our predictions, the children from Iran conformed to a distinctive sequence previously observed only in children in China. In contrast to the case with children from Australia (and the United States), knowledge access was understood earlier than opinion diversity in children from Iran, consistent with this collectivist culture's emphasis on filial respect, dispute avoidance, and acquiring knowledge. Having a sibling was linked with faster overall ToM progress in Australia only and was not related to scale sequences in either culture.

-----------------------
 When disoriented in a closed rectangular tank, fish (Xenotoca eiseni) reoriented in accord with the large-scale shape of the environment, but they were also able to conjoin geometric information with nongeometric properties such as the color of a wall or the features provided by panels located at the corners of the tank. Fish encoded geometric information even when featural information sufficed to solve the spatial task. When tested after transformations that altered the original arrangement of the panels, fish were more affected by those transformations that modified the geometric relationship between the target and the shape of the environment. Finally, fish appeared unable to use nongeometric information provided by distant panels. These findings show that a reorientation mechanism based on geometry is widespread among vertebrates, though the joint use of geometric and nongeometric cues by fish suggest that the degree of information encapsulation of the mechanism varies considerably between species.

 Disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested fish in the same task and found that they were able to conjoin geometric and non-geometric (landmark) information to reorient themselves in both the large and the small space used. Moreover, fish proved able to reorient immediately when dislocated from a large to a small experimental space and vice versa, suggesting that they encoded the relative rather than the absolute metrics of the environment. However, fish tended to make relatively more errors based on geometric information when transfer occurred from a small to a large space, and to make relatively more errors based on landmark information when transfer occurred from a large to a small space. The hypothesis is discussed that organisms are prepared to use only distant featural information as landmarks. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 It has been found that disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested domestic chicks ha the some task and found that they mere able to conjoin geometric and nongeometric (landmark) information to reorient themselves in both the large and the small space used. Moreover; chicks reoriented immediately when displaced from a large to a small experimental space and vice versa, suggesting that they used the relative metrics of the environment. However; when tested with a transformation (affine transformation) that alters the geometric relations between the target and the shape of the environment, chicks tended to make more errors based on geometric information when tested in the small than in the large space. These findings suggest that the reliance of the use of geometric information on the spatial scale of the environment is not restricted to the human species.

 Disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested fish in the same task and found that they were able to conjoin geometric and non-geometric (landmark) information to reorient themselves in both the large and the small space used. Moreover, fish proved able to reorient immediately when dislocated from a large to a small experimental space and vice versa, suggesting that they encoded the relative rather than the absolute metrics of the environment. However, fish tended to make relatively more errors based on geometric information when transfer occurred from a small to a large space, and to make relatively more errors based on landmark information when transfer occurred from a large to a small space. The hypothesis is discussed that organisms are prepared to use only distant featural information as landmarks. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 It has been found that disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested domestic chicks ha the some task and found that they mere able to conjoin geometric and nongeometric (landmark) information to reorient themselves in both the large and the small space used. Moreover; chicks reoriented immediately when displaced from a large to a small experimental space and vice versa, suggesting that they used the relative metrics of the environment. However; when tested with a transformation (affine transformation) that alters the geometric relations between the target and the shape of the environment, chicks tended to make more errors based on geometric information when tested in the small than in the large space. These findings suggest that the reliance of the use of geometric information on the spatial scale of the environment is not restricted to the human species.

 Disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested fish in the same task and found that they were able to conjoin geometric and non-geometric (landmark) information to reorient themselves in both the large and the small space used. Moreover, fish proved able to reorient immediately when dislocated from a large to a small experimental space and vice versa, suggesting that they encoded the relative rather than the absolute metrics of the environment. However, fish tended to make relatively more errors based on geometric information when transfer occurred from a small to a large space, and to make relatively more errors based on landmark information when transfer occurred from a large to a small space. The hypothesis is discussed that organisms are prepared to use only distant featural information as landmarks. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 It has been found that disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested domestic chicks ha the some task and found that they mere able to conjoin geometric and nongeometric (landmark) information to reorient themselves in both the large and the small space used. Moreover; chicks reoriented immediately when displaced from a large to a small experimental space and vice versa, suggesting that they used the relative metrics of the environment. However; when tested with a transformation (affine transformation) that alters the geometric relations between the target and the shape of the environment, chicks tended to make more errors based on geometric information when tested in the small than in the large space. These findings suggest that the reliance of the use of geometric information on the spatial scale of the environment is not restricted to the human species.

 Disoriented children could use geometric information in combination with landmark information to reorient themselves in large but not in small experimental spaces. We tested fish in the same task and found that they were able to conjoin geometric and non-geometric (landmark) information to reorient themselves in both the large and the small space used. Moreover, fish proved able to reorient immediately when dislocated from a large to a small experimental space and vice versa, suggesting that they encoded the relative rather than the absolute metrics of the environment. However, fish tended to make relatively more errors based on geometric information when transfer occurred from a small to a large space, and to make relatively more errors based on landmark information when transfer occurred from a large to a small space. The hypothesis is discussed that organisms are prepared to use only distant featural information as landmarks. (c) 2004 Elsevier B.V. All rights reserved.

-----------------------
 We compared young and healthy older adults' ability to rate photos of faces and situations (e.g., sporting activities) for the degree of threat they posed. Older adults did not distinguish between more and less dangerous faces to the same extent as younger adults did. In contrast, we found no significant age differences in young and older adults' ability to distinguish between high- and low-danger situations. The differences between young and older adults on the face task were independent of age differences in older adults' fluid IQ. We discuss results in relation to differences between young and older adults on emotion-recognition tasks; we also discuss sociocognitive and neuropsychological (e.g., amygdala) theories of aging.

 Understanding older adults' social functioning difficulties requires insight into their recognition of emotion processing in voices and bodies, not just faces, the focus of most prior research. We examined 60 young and 61 older adults' recognition of basic emotions in facial, vocal, and bodily expressions, and when matching faces and bodies to voices, using 120 emotion items. Older adults were worse than young adults in 17 of 30 comparisons, with consistent difficulties in recognizing both positive (happy) and negative (angry and sad) vocal and bodily expressions. Nearly three quarters of older adults functioned at a level similar to the lowest one fourth of young adults, suggesting that age-related changes are common. In addition, we found that older adults' difficulty in matching emotions was not explained by difficulty on the component sources (i.e., faces or voices on their own), suggesting an additional problem of integration.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 Various theories propose that count nouns are distinguished from mass nouns by their specification of individuation. We present evidence that, while 3-year-old children acquiring language extend words differentially on the basis of mass-count syntax, they quantify over individuals for both novel mass and count nouns. We suggest that children may begin acquisition with an underspecified representation of mass noun semantics, permitting quantification over both individuals and continuous quantities. Also, children may rely on ontologically based biases to guide quantification.

 How do children as young as 2 years of age know that numerals, like one, have exact interpretations, while quantifiers and words like a do not? Previous studies have argued that only numerals have exact lexical meanings. Children Could not use scalar implicature to strengthen numeral meanings, it is argued, since they fail to do so for quantifiers [Papafragou, A., &amp; Musolino,J. (2003). Scalar implicatures: Experiments at the semantics-pragmatics interface. Cognition, 86, 253-282]. Against this view, we present evidence that children's early interpretation Of numerals does rely on scalar implicature, and argue that differences between numerals and quantifiers are due to differences in the availability of the respective scales of which they are members. Evidence from previous studies establishes that (1) children can make scalar inferences when interpreting numerals, (2) children initially assign weak, non-exact interpretations to numerals when first acquiring their meanings, and (3) children can strengthen quantifier interpretations when scalar alternatives are made explicitly available. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 We argue that psycholinguistics should be concerned with both the representation and the processing of language. Recent experimental work on syntax in language comprehension has largely concentrated on the way in which language is processed and has assumed that theoretical linguistics serves to determine the representation of language. In contrast, we advocate experimental work on the mental representation of grammatical knowledge, and argue that syntactic priming is a promising way to do this. Syntactic priming is the phenomenon whereby exposure to a sentence with a particular syntactic construction can affect the subsequent processing of an otherwise unrelated sentence with the same (or, perhaps, related) structure, for reasons of that structure. We assess evidence for syntactic priming in corpora, and then consider experimental evidence for priming in production and comprehension and for bidirectional priming between comprehension and production. This in particular strongly suggests that priming is tapping into linguistic knowledge itself; and is not just facilitating particular processes. The final section discusses the importance of priming evidence for any account of language construed as the mental representation of human linguistic capacities.

 People have a tendency to repeat the types of sentences they use during language production. Recent experimental work has shown that this phenomenon is at least partly due to 'syntactic priming', whereby the act of processing an utterance with a particular form facilitates processing a subsequent utterance with the same or a related form. In this review, we first provide an overview of the evidence for syntactic priming. The review will then explore the implications of this research for three different areas of language theory: the possible functional significance of syntactic priming in coordinating speakers during dialogue, the mechanisms underlying sentence production, and the nature of linguistic representation.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Psychological essentialism is the idea that certain categories, such as 'lion' or 'female', have an underlying reality that cannot be observed directly. Where does this idea come from? This article reviews recent evidence suggesting that psychological essentialism is an early cognitive bias. Young children look beyond the obvious in many converging ways: when learning words, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. These findings argue against the standard view of children as concrete thinkers, instead claiming that children have an early tendency to search for hidden, non-obvious features.

 Essentialism is the idea that certain categories, such as "dog," "man," or "gold;" have an underlying reality or true nature that gives objects their identity. Essentialist accounts have been offered, in one form or another, for thousands of years, extending back at least to Aristotle and Plato. Where does this idea come from? 1 address this question from a psychological perspective and argue that essentialism is an early cognitive bias. Young children's concepts reflect a deep commitment to essentialism, and this commitment leads children to look beyond the obvious in many converging ways: when learning language, generalizing knowledge to new category members, reasoning about the insides of things, contemplating the role of nature versus nurture, and constructing causal explanations. I suggest that children have an early, powerful tendency to search for hidden, non-obvious features of things. Parents do not explicitly teach children to essentialize; instead, during the preschool years, children spontaneously construct concepts and beliefs that reflect an essentialist bias. I explore the broader implications of this perspective for human concepts, children's thinking, and the relation between human concepts and the biological world.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 Coherent visual perception necessitates the ability to track distinct objects as the same entities over time and motion. Calculations of such object persistence appear to be fairly automatic and constrained by specific rules. We explore the nature of object persistence here within the object-file framework; object files are mid-level visual representations that track entities over time and motion as the same persisting objects and store and update information about the objects. We present three new findings. First, objects files are constrained by the principle of boundedness; persisting entities should maintain a single closed contour. Second, object files are constrained by the principle of containment; all the parts and properties of a persisting object should reside within, and be connected to, the object itself. Third, object files are sensitive to the context in which an object appears; the very same physical entity that can instantiate object-file formation in one experimental context cannot in another. This contextual influence demonstrates for the first time that object files are sensitive to more than just the physical properties contained within any given visual display.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 Coherent visual perception necessitates the ability to track distinct objects as the same entities over time and motion. Calculations of such object persistence appear to be fairly automatic and constrained by specific rules. We explore the nature of object persistence here within the object-file framework; object files are mid-level visual representations that track entities over time and motion as the same persisting objects and store and update information about the objects. We present three new findings. First, objects files are constrained by the principle of boundedness; persisting entities should maintain a single closed contour. Second, object files are constrained by the principle of containment; all the parts and properties of a persisting object should reside within, and be connected to, the object itself. Third, object files are sensitive to the context in which an object appears; the very same physical entity that can instantiate object-file formation in one experimental context cannot in another. This contextual influence demonstrates for the first time that object files are sensitive to more than just the physical properties contained within any given visual display.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Coherent visual experience requires not only segmenting incoming visual input into a structured scene of objects, but also binding discrete views of objects into dynamic representations that persist across time and motion. However, surprisingly little work has explored the principles that guide the construction and maintenance of such persisting object representations. What causes a part of the visual field to be treated as the same object over time? In the cognitive development literature, a key principle of object persistence is cohesion: An object must always maintain a single bounded contour. Here we demonstrate for the first time that mechanisms of adult midlevel vision are affected by cohesion violations. Using the object-file framework, we tested whether object-specific preview benefits-a hallmark of persisting object representations-are obtained for dynamic objects that split into two during their motion. We found that these preview benefits do not fully persist through such cohesion violations without incurring significant performance costs. These results illustrate how cohesion is employed as a constraint that guides the maintenance of object representations in adult midlevel vision.

 A critical challenge for visual perception is to represent objects as the same persisting individuals over time and motion. Across several areas of cognitive science, researchers have identified cohesion as among the most important theoretical principles of object persistence: An object must maintain a single bounded contour over time. Drawing inspiration from recent work in adult visual cognition, the present study tested the power of cohesion as a constraint as it operates early in development. In particular, we tested whether the most minimal cohesion violation - a single object splitting into two - would destroy infants' ability to represent a quantity of objects over occlusion. In a forced-choice crawling paradigm, 10- and 12-month-old infants witnessed crackers being sequentially placed into containers, and typically crawled toward the container with the greater cracker quantity. When one of the crackers was visibly split in half, however, infants failed to represent the relative quantities, despite controls for the overall quantities and the motions involved. This result helps to characterize the fidelity and specificity of cohesion as a fundamental principle of object persistence, suggesting that even the simplest possible cohesion violation can dramatically impair infants' object representations and influence their overt behavior.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 This work addresses whether 30-month-olds appreciate that their communicative signals are being understood (or not) by another person. Infants produce a range of behaviors, such as repairing their failed signals, that have been construed as evidence that they have an implicit theory of mind. Such behavior could be interpreted as attempts to obtain some desired goal rather than as attempts to gain listener understanding. This study was designed to separate listener comprehension from obtaining a material goal. In 4 conditions, children either did or did not get what they wanted and the experimenter understood or misunderstood their request. As predicted, children clarified their signal more when the experimenter misunderstood compared to when she understood. Regardless of whether young children achieved their overt goal, they engaged in behaviors to ensure their communicative act had been understood.

 Children growing up in a dual-language environment have to constantly monitor the dynamic communicative context to determine what the speaker is trying to say and how to respond appropriately. Such self-generated efforts to monitor speakers' communicative needs may heighten children's sensitivity to, and allow them to make better use of, referential gestures to figure out a speaker's referential intent. In a series of studies, we explored monolingual and bilingual preschoolers' use of nonverbal referential gestures such as pointing and gaze direction to figure out a speaker's intent to refer. In Study 1, we found that 3- and 4-year-old bilingual children were better able than monolingual children to use referential gestures (e.g., gaze direction) to locate a hidden toy in the face of conflicting body-distal information (the experimenter was seated behind an empty box while the cue was directed at the correct box). Study 2 found that by 5 years of age, monolingual children had mastered this task. Study 3 established that the bilingual advantage can be found in children as young as 2 years old. Thus, the experience of growing up in a bilingual environment fosters the development of the understanding of referential intent.

-----------------------
 The aim of the present study was to investigate the potential of the Working Memory Rating Scale (WMRS), an observer-based rating scale that reflects behavioral difficulties of children with poor working memory. The findings indicate good internal reliability and adequate psychometric properties for use as a screening tool by teachers. Higher (i.e., more problematic) teacher ratings on the WMRS were associated with lower memory scores on direct assessments of working memory skills, as measured by the Automated Working Memory Assessment (AWMA) and the WISC-IV Working Memory Index. The use of the WMRS will allow educators to draw on their expertise in the classroom for early detection of children with working memory failures. (C) 2008 Elsevier Inc. All rights reserved.

 The present study investigates how working memory and fluid intelligence are related in young children and how these links develop over time. The major aim is to determine which aspect of the working memory system-short-term storage or cognitive control drives the relationship with fluid intelligence. A sample of 119 children was followed from kindergarten to second grade and completed multiple assessments of working memory, short-term memory, and fluid intelligence. The data showed that working memory, short-term memory, and fluid intelligence were highly related but separate constructs in young children. The results further showed that when the common variance between working memory and short-term memory was controlled, the residual working memory factor manifested significant links with fluid intelligence whereas the residual short-term memory factor did not. These findings suggest that in young children cognitive control mechanisms rather than the storage component of working memory span tasks are the source of their link with fluid intelligence. (C) 2010 Elsevier Inc. All rights reserved.

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 A number of philosophers of language have proposed that people do not have conceptual access to 'bare particulars', or attribute-free individuals (e.g. Wiggins, 1980). Individuals can only be picked out under some sortal, a concept which provides principles of individuation and identity. Many advocates of this view have argued that object is not a genuine sortal concept. I will argue in this paper that a narrow sense of 'object', namely the concept of any bounded, coherent, three-dimensional physical object that moves as a whole (Spelke, 1990) is a sortal for both infants and adults. Furthermore, object may be the infant's first sortal and more specific sortals such as cup and dog may be acquired later in the first year of life. I will discuss the implications for infant categorization studies, trying to draw a conceptual distinction between a perceptual category and a sortal, and I will speculate on how a child may construct sortal concepts such as cup and dog.

 Considerable evidence indicates that preverbal infants expect that only physical contact can cause an inanimate object to move. However, very few studies have investigated infants' expectations about the source of causal power. In three experiments, we found that (a) 10- and 12-month-old infants expect a human hand, and not an inanimate object, to be the primary cause of an inanimate object's motion; (b) infants' expectations can lead them to infer a hidden causal agent without any direct perceptual evidence; and (c) infants do not infer a hidden causal agent if the moving object was previously shown to be capable of self-generated motion.

-----------------------
 Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.

 In coming to understand the world-in learning concepts, acquiring language, and grasping causal relations-our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?

-----------------------
 We review the evidence for various kinds of limit in the capability of working memory, the small amount of information that can be held in mind at once. To distinguish between types of limit in working memory, we invoke metaphors of space (capacity), time (decay and speed), and energy (control of attention). The review focuses primarily on recent evidence on a limit in how many chunks can be held in working memory, how this kind of limit can be measured, and how it can be distinguished from other types of limits. We explore the theoretical and practical importance of different working memory limits in research that is nomothetic (referring to general laws) and ideographic (referring to individual and group differences). The appropriate measure of working memory depends on one's holistic or analytic scientific interest. (c) 2008, Elsevier Inc.

 Working memory can be described as the small amount of information held in a readily accessible state, available to help in the completion of cognitive tasks. There has been considerable confusion among researchers regarding the definition of working memory, which can be attributed to the difficulty of reconciling descriptions from working memory researchers with very different theoretical orientations. Here I review theories of working memory and some of the main issues in the field, discuss current behavioral and neuropsychological research that can address these issues, and consider the implications for cognitive development.

-----------------------
 Some argue that action comprehension is intimately connected with the observer's own motor capacities, whereas others argue that action comprehension depends on non-motor inferential mechanisms. We address this debate by reviewing comparative studies that license four conclusions: monkeys and apes extract the meaning of an action (i) by going beyond the surface properties of actions, attributing goals and intentions to the agent; (ii) by using environmental information to infer when actions are rational; (iii) by making predictions about an agent's goal, and the most probable action to obtain the goal given environmental constraints; (iv) in situations in which they are physiologically incapable of producing the actions. Motor theories are, thus, insufficient to account for primate action comprehension in the absence of inferential mechanisms.

 We synthesize the contrasting predictions of motor simulation and teleological theories of action comprehension and present evidence from a series of studies showing that monkeys and apes-like humans-extract the meaning of an event by (a) going beyond the surface appearance of actions, attributing goals and intentions to the agent; (b) using details about the environment to infer when an action is rational or irrational; (c) making predictions about an agent's goal and the most probable action to obtain the goal, within the constraints of the situation; (d) predicting the most probable outcome of actions even when they are physiologically incapable of producing the actions; and (e) combining information about means and outcomes to make decisions about social interactions, some with moral relevance. These studies reveal the limitations of motor simulation theories, especially those that rely on the notion of direct matching and mirror neuron activation. They provide support, however, for a teleological theory, rooted in an inferential process that extracts information about action means, potential goals, and the environmental constraints that limit rational action.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 Identification of the second of two targets is impaired when the second target is presented less than about 500 msec after the first. Nieuwenstein, Chun, van der Lubbe, and Hooge (2005, Experiment 4) reported that the magnitude of this attentional blink (AB) is reduced when the location of the second target is precued. Here we show how that finding resulted from an artifact brought about by a ceiling imposed by data limitation. Instead of using an accuracy measure, the present work used a dynamic threshold-tracking procedure that was not constrained by a performance ceiling. The results show that, when the ceiling is removed, spatial cuing does not affect and is not affected by the AB. These results are consistent with the hypothesis that cue localization and target identification may take place along separate (dorsal and ventral) visual pathways.

 Three experiments investigated the role of eye movements in the rapid resumption of an interrupted search. Passive monitoring of eye position in Experiment 1 showed that rapid resumption was associated with a short distance between the eye and the target on the next-to-last look before target detection. Experiments 2 and 3 used two different methods for presenting the target to the point of eye fixation on some trials. If eye position alone is predictive, rapid resumption should increase when the target is near fixation. The results showed that gaze-contingent targets increased overall search success, but that the proportion of rapid responses decreased dramatically. We conclude that rather than depending on a high-quality single look at a search target rapid resumption of search depends on two glances; a first glance in which a hypothesis is formed, and a second glance in which the hypothesis is confirmed.

-----------------------
 This paper presents electrophysiological data on the on-line processing of open- and closed-class words in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials were recorded from the scalp when Broca patients and non-aphasic control subjects were visually presented with a story in which the words appeared one at a time on the screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed clear differences between the processing of open- and closed-class words in an early (210-375 ms) and a late (400-700 ms) time-window. The early electrophysiological differences reflect the first manifestation of the availability of word-category information from the mental lexicon. The late differences presumably relate to post-lexical semantic and syntactic processing. In contrast to the control subjects, the Broca patients showed no early vocabulary class effect and only a limited late effect. The results suggest that an important factor in the agrammatic comprehension deficit of Broca's aphasics is a delayed and/or incomplete availability of word-class information.

 This paper presents electrophysiological evidence of an impairment in the on-line processing of word class information in patients with Broca's aphasia with agrammatic comprehension. Event-related brain potentials (ERPs) were recorded from the scalp while Broca patients and non-aphasic control subjects read open- and closed-class words that appeared one at a time oil a PC screen. Separate waveforms were computed for open- and closed-class words. The non-aphasic control subjects showed a modulation of an early left anterior negativity in the 210-325 ms as a function of vocabulary class (VC), and a late left anterior negative shift to closed-class words in the 400-700 ms epoch. An N400 effect was present in both control subjects and Broca patients. We have taken the early electrophysiological differences to reflect the first availability of word-category information from the mental lexicon. The late differences can be related to post-lexical processing. In contrast to the control subjects, the Broca patients showed no early VC effect and no late anterior shift to closed-class words. The results support the view that an incomplete and/or delayed availability of word-class information might be an important factor in Broca's agrammatic comprehension. (C) 2002 Elsevier Science Ltd. All rights reserved.

-----------------------
 The study of memory is witnessing a spirited clash between proponents of traditional laboratory research and those advocating a more naturalistic approach to the study of ''real-life'' or ''everyday'' memory. The debate has generally centered on the ''what'' (content), ''where'' (context), and ''how'' (methods) of memory research. In this target article, we argue that the controversy discloses a further, more fundamental breach between two underlying memory metaphors, each having distinct implications for memory theory and assessment: Whereas traditional memory research has been dominated by the storehouse metaphor, leading to a focus on the number of items remaining in store and accessible to memory, the recent wave of everyday memory research has shifted toward a correspondence metaphor, focusing on the accuracy of memory in representing past events. The correspondence metaphor calls for a research approach that differs from the traditional one in important respects: in emphasizing the intentional-representational function of memory, in addressing the wholistic and graded aspects of memory correspondence, in taking an output-bound assessment perspective, and in allowing more room for the operation of subject-controlled metamemory processes and motivational factors. This analysis can help tie together some of the what, where, and how aspects of the ''real-life/laboratory'' controversy. More important, however, by explicating the unique metatheoretical foundation of the accuracy-oriented approach to memory we aim to promote a more effective exploitation of the correspondence metaphor in both naturalistic and laboratory research contexts.

 There has been unprecedented interest in recent years in questions pertaining to accuracy and distortion in memory. This interest, catalyzed in part by real-life problems, marks a significant departure from the quantity-oriented approach that has characterized much of traditional memory research. We outline a correspondence metaphor of memory underlying accuracy-oriented research, and show how the features of this metaphor are manifested across the disparate bodies of research reviewed here. These include work in the Gestalt tradition, spatial memory, memory for gist, schema theory, source monitoring, fluency misattributions, false recall and recognition, postevent misinformation, false memories, eyewitness research, and autobiographical memory. In examining the dynamics of memory accuracy, we highlight the importance of metacognitive monitoring and control processes. We end by discussing some of the methodological, theoretical, and metatheoretical issues inherent in accuracy-oriented research, attempting to prepare the groundwork for a more coherent psychology of memory accuracy.

-----------------------
 The authors investigated the extent to which touch, vision, and audition mediate the processing of statistical regularities within sequential input. Few researchers have conducted rigorous comparisons across sensory modalities; in particular, the sense of touch has been virtually ignored. The current data reveal not only commonalities but also modality constraints affecting statistical learning across the senses. To be specific, the authors found that the auditory modality displayed a quantitative learning advantage compared with vision and touch. In addition, they discovered qualitative learning biases among the senses: Primarily, audition afforded better learning for the final part of input sequences. These findings are discussed in terms of whether statistical learning is likely to consist of a single, unitary mechanism or multiple, modality-constrained ones.

 When learners encode sequential patterns and generalize their knowledge to novel instances, are they relying on abstract or stimulus-specific representations? Research on artificial grammar learning (AGL) has shown transfer of learning from one stimulus set to another, and such findings have encouraged the view that statistical learning is mediated by abstract representations that are independent of the sense modality or perceptual features of the stimuli. Using a novel modification of the standard AGL paradigm, we obtained data to the contrary. These experiments pitted abstract processing against stimulus-specific learning. The findings show that statistical learning results in knowledge that is stimulus-specific rather than abstract. They show furthermore that learning can proceed in parallel for multiple input streams along separate perceptual dimensions or sense modalities. We conclude that learning sequential structure and generalizing to novel stimuli inherently involve learning mechanisms that are closely tied to the perceptual characteristics of the input.

-----------------------
 Background: Neuroimaging offers unique opportunities for understanding the acquisition of reading by children and for unravelling the mystery of developmental dyslexia. Here, I provide a selective overview of recent neuroimaging studies, drawing out implications for education and the teaching of reading.
 Purpose: To explore the phonological awareness skills of deaf children with cochlear implants (CIs) and relationships with vocabulary and reading development.
-----------------------
 At least 3 different types of computational model have been shown to account for various facets of both normal and impaired single word reading: (a) the connectionist triangle model, (b) the dual-route cascaded model, and (c) the connectionist dual process model. Major strengths and weaknesses of these models are identified. In the spirit of nested incremental modeling, a new connectionist dual process model (the CDP+ model) is presented. This model builds on the strengths of 2 of the previous models while eliminating their weaknesses. Contrary to the dual-route cascaded model, CDP+ is able to learn and produce graded consistency effects. Contrary to the triangle and the connectionist dual process models, CDP+ accounts for serial effects and has more accurate nonword reading performance. CDP+ also beats all previous models by an order of magnitude when predicting individual item-level variance on large databases. Thus, the authors show that building on existing theories by combining the best features of previous models-a nested modeling strategy that is commonly used in other areas of science but often neglected in psychology-results in better and more powerful computational models.

 This article describes the Dual Route Cascaded (DRC) model, a computational model of visual word recognition and reading aloud. The DRC is a computational realization of the dual-route theory of reading, and is the only computational model of reading that can perform the 2 tasks most commonly used to study reading: lexical decision and reading aloud. For both tasks, the authors show that a wide variety of variables that influence human latencies influence the DRC model's latencies in exactly the same way. The DRC model simulates a number of such effects that other computational models of reading do not, but there appear to be no effects that any other current computational model of reading can simulate but that the DRC model cannot. The authors conclude that the DRC model is the most successful of the existing computational models of reading.

-----------------------
 At least 3 different types of computational model have been shown to account for various facets of both normal and impaired single word reading: (a) the connectionist triangle model, (b) the dual-route cascaded model, and (c) the connectionist dual process model. Major strengths and weaknesses of these models are identified. In the spirit of nested incremental modeling, a new connectionist dual process model (the CDP+ model) is presented. This model builds on the strengths of 2 of the previous models while eliminating their weaknesses. Contrary to the dual-route cascaded model, CDP+ is able to learn and produce graded consistency effects. Contrary to the triangle and the connectionist dual process models, CDP+ accounts for serial effects and has more accurate nonword reading performance. CDP+ also beats all previous models by an order of magnitude when predicting individual item-level variance on large databases. Thus, the authors show that building on existing theories by combining the best features of previous models-a nested modeling strategy that is commonly used in other areas of science but often neglected in psychology-results in better and more powerful computational models.

 This article describes the Dual Route Cascaded (DRC) model, a computational model of visual word recognition and reading aloud. The DRC is a computational realization of the dual-route theory of reading, and is the only computational model of reading that can perform the 2 tasks most commonly used to study reading: lexical decision and reading aloud. For both tasks, the authors show that a wide variety of variables that influence human latencies influence the DRC model's latencies in exactly the same way. The DRC model simulates a number of such effects that other computational models of reading do not, but there appear to be no effects that any other current computational model of reading can simulate but that the DRC model cannot. The authors conclude that the DRC model is the most successful of the existing computational models of reading.

-----------------------
 This study examined whether infants process faces analytically by processing facial features as independent parts or holistically by processing them as a configuration. According to the "Switch design" of Cohen and Cashon (2001), 110 four-, 110 six- and 105 eight-month-old infants were first habituated to two faces (a child face and an adult face) using looking time as the dependent variable. After habituation, the infants were tested with a familiar habituation face, a switch face where either the eyes or mouth of one of the habituation faces had been inserted into the other habituation face, or a new face. The results showed that 4-, 6-, and 8-month-old infants processed the faces analytically when the eyes were switched. However, when the mouth was switched, 8-month-old infants processed the faces holistically whereas only a few of the 4- and 6-month-old infants processed the faces analytically or they were unable to perceive the change of mouth. Thus, the face processing mode in the first year of life is dependent on age and the feature varied.

 Two experiments examined whether 8-month-old infants process faces (photos in Experiment 1, schematic faces in Experiment 2) analytically by processing facial features independently of the facial context or configurally by processing the features in conjunction with the facial context. Infants were habituated to two faces and looking time was measured. After habituation they were tested with a habituation face. a switch face, or a novel face. In the switch faces, single features of the habituation faces were switched. The results showed that the infants processed facial features of photographs of faces configurally whereas they processed features of schematic faces (eyes, nose, facial contour) analytically. Thus, although infants have access to both processing modes, for real looking faces they use the configural mode. (C) 2003 Elsevier Ltd. All rights reserved.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Six-month-old infants were presented with sounding objects under 3 conditions of illumination: in full vision, in the dark with target location specified by a glowing and sounding object, and in the dark with location specified by sound alone. Reaching behavior was videotaped with an infrared camera, and hand movement was measured by infrared-emitting diodes on the hand that were tracked by a motion analysis system. No differences were found in reaching behavior for objects in the light and glowing objects in the dark. Reaches for sounding objects in the dark had higher speeds, shorter durations, and more errors compared to the other 2 conditions. These findings indicate that vision of the hand did not appear to affect infants' reaching in this situation, whereas vision of the target did.

 Infants were presented with a moving object under 2 lighting conditions to investigate the role of vision in early reaching. The motion of the target object. also allowed for an analysis of the infants' ability to use a predictive style of reaching. Infants were tested twice, at 5 and 7.5 months of age, with a moving object in the light and the same object painted with luminescent paint in the dark. Infants successfully contacted the glowing object on about half of their attempts at both ages, although 7.5 month-olds reached more often. Infants also took into account the motion of the target object by aiming their reaches ahead of the object and by reaching with their contralateral hand. These results suggest that proprioceptive feedback and sight of the target allow for successful reaching with limited visual information, even in relatively complex reaching tasks. The infants' success also demonstrates their ability to adapt their movements and reaching strategy to the speed and trajectory of the target object in order to reach predictively.

-----------------------
 Several previous studies found an association of clinically diagnosed attention deficit hyperactivity disorder with long alleles of a variation in the DRD4 dopamine receptor gene exon III coding sequence. We evaluated the DRD4 polymorphism in a non-clinically selected sample of children for whom maternal reports of attention problems were available at 4 and 7 years of age. There was a significant elevation in attention problem scores in children carrying DRD4 long alleles that accounted for 3-4% of total variation at each age and for 5-7% of the temporally stable component of the phenotype, Our results show that the DRD4 gene influences normal as well as pathological attention processes, and the results highlight the utility of longitudinal measurements in psychiatric genetics. Psychiatr Genet 11:25-29 (C) 2001 Lippincott Williams &amp; Wilkins.

 The present study utilized a multilevel approach to examine developmental trajectories in risk-taking propensity. We examined the moderating role of specific executive function components, attention shifting and inhibitory control, on the link between exuberant temperament in infancy and propensity for risk taking in childhood. Risk taking was assessed using a task previously associated with sensation seeking and antisocial behaviors. Two hundred ninety-one infants were brought into the lab and behaviors reflecting exuberance were observed at 4, 9, 24, and 36 months of age. Executive function was. assessed at 48 months of age. Risk-taking propensity was measured when children were 60 months of age. The results indicated that exuberance and attention shifting, but not inhibitory control, significantly interacted to predict propensity for risk taking. Exuberance was positively associated with risk-taking propensity among children who were relatively low in attention shifting but unrelated for children high in attention shifting. These findings illustrated the multifinality of developmental outcomes for temperamentally exuberant young children and pointed to the distinct regulatory influences of different executive functions for children of differing temperaments. Attention shifting likely affords a child the ability to consider both positive and negative consequences and moderates the relation between early exuberance and risk-taking propensity.

-----------------------
 Recent studies have noted a relation between the pattern of resting frontal EEG activity and individual differences in affective style in typically developing infants, children, and adults. The authors conducted a pilot study to investigate the pattern of frontal EEG activity during a resting condition (eyes-open, eyes-closed) in a group of children who had one parent clinically diagnosed with social phobia (SP; n = 6) and in a group of typically developing children of similar age with healthy parents (n = 7). Patients with a primary DSM-IV diagnosis of SP with at least one biological child were recruited from the Anxiety Disorders Clinic at McMaster University Medical Centre. We found that children of parents clinically diagnosed with SP tended to exhibit higher overall resting frontal EEG activity compared with the children of healthy parents. This pattern of overall high EEG activity that is specific to the frontal region is similar to that observed in socially anxious profiles. Preliminary findings are discussed in terms of how overall resting frontal brain activation may be an early psychophysiological marker for placing children of parents with social phobia at risk for socioemotional problems before such problems emerge.

 Gene-environment interactions involving exogenous environmental factors are known to shape behavior and personality development. Although gene-environment interactions involving endogenous environmental factors are hypothesized to play an equally important role, this conceptual approach has not been empirically applied in the study of early-developing temperament in humans. Here we report evidence for a gene-endoenvironment (i.e., resting frontal brain electroencephalogram, EEG, asymmetry) interaction in predicting child temperament. The dopamine D4 receptor (DRD4) gene (long allele vs. short allele) moderated the relation between resting frontal EEG asymmetry (left vs. right) at 9 months and temperament at 48 months. Children who exhibited left frontal EEG asymmetry at 9 months and who possessed the DRD4 long allele were significantly more soothable at 48 months than other children. Among children with right frontal EEG asymmetry at 9 months, those with the DRD4 long allele had significantly more difficulties focusing and sustaining attention at 48 months than those with the DRD4 short allele. Resting frontal EEG asymmetry did not influence temperament in the absence of the DRD4 long allele. We discuss how the interaction of genetic and endoenvironmental factors may confer risk and protection for different behavioral styles in children.

-----------------------
 We investigated working memory updating performance in younger and older adults before, during. and after 100-day practice. Performance to presentation time (PT) relation was fitted to a negatively accelerated logistic function. Relative to younger adults, older adults showed lower asymptotes at pretest and posttest, and shallower slopes at pretest. Older adults practicing the task with fast PT gained less than older adults practicing the task with slow PT, probably reflecting the persistent use of a selective strategy throughout the 100-day practice period in the fast FT group. These results have implications for designing and evaluating age-comparative working memory training programs.

 The authors examined life-span differences in the maintenance of skilled episodic memory performance by assessing 100 individuals (10-11, 12-13, 21-26, and 66-79 years old) 11 months after termination of an intensive multisession mnemonic training program (Y. Brehmer, S.-C. Li, V. Muller, T. von Oertzen, &amp; U. Lindenberger, 2007). Skill maintenance was tested in 2 follow-up sessions, the first without and the second with mnemonic reinstruction. Younger and older adults' average performance levels were stable across time. In contrast, both younger and older children's memory performance improved beyond originally attained levels. Older adults' performance improved from the first to the second follow-up session, presumably profiting from instruction-induced skill reactivation. Results suggest that (a) skill maintenance is largely intact in healthy older adults, (b) older adults need environmental support to fully reactivate their former skill levels (cf. F. I. M. Craik, 1983), and (c) children adapt a skill learned 11 months ago to their increasing cognitive capabilities.

-----------------------
 We investigated working memory updating performance in younger and older adults before, during. and after 100-day practice. Performance to presentation time (PT) relation was fitted to a negatively accelerated logistic function. Relative to younger adults, older adults showed lower asymptotes at pretest and posttest, and shallower slopes at pretest. Older adults practicing the task with fast PT gained less than older adults practicing the task with slow PT, probably reflecting the persistent use of a selective strategy throughout the 100-day practice period in the fast FT group. These results have implications for designing and evaluating age-comparative working memory training programs.

 The authors examined life-span differences in the maintenance of skilled episodic memory performance by assessing 100 individuals (10-11, 12-13, 21-26, and 66-79 years old) 11 months after termination of an intensive multisession mnemonic training program (Y. Brehmer, S.-C. Li, V. Muller, T. von Oertzen, &amp; U. Lindenberger, 2007). Skill maintenance was tested in 2 follow-up sessions, the first without and the second with mnemonic reinstruction. Younger and older adults' average performance levels were stable across time. In contrast, both younger and older children's memory performance improved beyond originally attained levels. Older adults' performance improved from the first to the second follow-up session, presumably profiting from instruction-induced skill reactivation. Results suggest that (a) skill maintenance is largely intact in healthy older adults, (b) older adults need environmental support to fully reactivate their former skill levels (cf. F. I. M. Craik, 1983), and (c) children adapt a skill learned 11 months ago to their increasing cognitive capabilities.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 At the one-word stage children use gesture to supplement their speech ('eat'+point at cookie), and the onset of such supplementary gesture-speech combinations predicts the onset of two-word speech ('eat cookie'). Gesture thus signals a child's readiness to produce two-word constructions. The question we ask here is what happens when the child begins to flesh out these early skeletal two-word constructions with additional arguments. One possibility is that gesture continues to be a forerunner of linguistic change as children flesh out their skeletal constructions by adding arguments. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. Our analysis of 40 children - from 14 to 34 months - showed that children relied on gesture to produce the first instance of a variety of constructions. However, once each construction was established in their repertoire, the children did not use gesture to flesh out the construction. Gesture thus acts as a harbinger of linguistic steps only when those steps involve new constructions, not when the steps merely flesh out existing constructions.

 Children who produce one word at a time often use gesture to supplement their speech, turning a single word into an utterance that conveys a sentence-like meaning ('eat'+point at cookie). Interestingly, the age at which children first produce supplementary gesture-speech combinations of this sort reliably predicts the age at which they first produce two-word utterances. Gesture thus serves as a signal that a child will soon be ready to begin producing multi-word sentences. The question is what happens next. Gesture could continue to expand a child's communicative repertoire over development, combining with words to convey increasingly complex ideas. Alternatively, after serving as an opening wedge into language, gesture could cease its role as a forerunner of linguistic change. We addressed this question in a sample of 40 typically developing children, each observed at 14, 18, and 22 months. The number of supplementary gesture-speech combinations the children produced increased significantly from 14 to 22 months. More importantly, the types of supplementary combinations the children produced changed over time and presaged changes in their speech. Children produced three distinct constructions across the two modalities several months before these same constructions appeared entirely within speech. Gesture thus continues to be at the cutting edge of early language development, providing stepping-stones to increasingly complex linguistic constructions. (c) 2005 Elsevier B.V. All rights reserved.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 Recent work on object individuation and object identity in infancy indicates that at least three sources of information may be used for object individuation and object identity: spatiotemporal information, object property information, and object kind information. Several experiments have shown that a major developmental change occurs between 10 and 12 months of age (Xu &amp; Carey, 1996; Xu, Carey &amp; Welch, in press; Van de Walle, Prevor &amp; Carey, under review; Xu, Carey &amp; Quint, in preparation): Infants at 10 months and younger readily use spatiotemporal information in object individuation and object identity tasks, but not until about 12 months of age are infants able to use object property or object kind information to do so. This paper proposes a two-part conjecture about the mechanism underlying this change. The first part borrows ideas from object-based attention and the distinction between "what" and "where" information in visual processing. The hypothesis is that (1) young infants encode object motion and location information separately from object property information; and (2) toward the end of the first year, infants integrate these two sources of information. The second part of the conjecture posits an important role for language. Infants may take distinct labels as referring to distinct kinds of objects from the onset of word learning, and infants use this information in solving the problem of object individuation and object identity. Evidence from human adults, infants, and non-human primates is reviewed to provide support for the conjecture. (C) 1999 Elsevier Science B.V. All rights reserved. PsycINFO classification. 7820.

 Several investigators find that infants fail to use property information to individuate objects until 12 months of age (e.g., Xu &amp; Carey, 1996), while others find that infants successfully employ property information in the service of object individuation at 9.5 months (e.g., Wilcox &amp; Baillargeon, 1998a). This study investigated methodological differences between 2 sets of studies that may help to resolve this apparent conflict. It was hypothesized that infants younger than 12 months rely heavily on spatiotemporal information and it overrides any conflicting property information. In 2 experiments, we used a simplified manual search procedure much like Wilcox and Baillargeon's (1998a) simplified looking time procedure by reducing the number of alternations of the objects. Results suggest that when the spatiotemporal evidence specifying a single object that changes properties is weaker, 10-month-old infants succeed in using property information for object individuation.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 The present study examined English-speaking children's tendency to make argument structure overgeneralization errors (e.g., I disappeared it). Children were exposed to several English verbs of fixed transitivity (exclusively intransitive or exclusively transitive) and then asked questions that encouraged them to overgeneralize usage of the verbs. Seventy-two children (24 in each of three age groups: 3, 4/5, and 8 years of age) experienced four actions performed by puppets. Each action had two verbs of similar meaning associated with it in the context of the experimental action: one more familiar to young children and one less familiar. Children at all ages were more likely to overgeneralize usage of verbs that were less familiar to them, supporting the hypothesis that children's usage of verbs in particular construction types becomes entrenched over time. As children solidly learn the transitivity status of particular verbs, they become more reluctant to use those verbs in other argument structure constructions.

 This research investigates 3- and 5-year-olds' relative fairness in distributing small collections of even or odd numbers of more or less desirable candies, either with an adult experimenter or between two dolls. The authors compare more than 200 children from around the world, growing up in seven highly contrasted cultural and economic contexts, from rich and poor urban areas, to small-scale traditional and rural communities. Across cultures, young children tend to optimize their own gain, not showing many signs of self-sacrifice or generosity. Already by 3 years of age, self-optimizing in distributive justice is based on perspective taking and rudiments of mind reading. By 5 years, overall, children tend to show more fairness in sharing. What varies across cultures is the magnitude of young children's self-interest. More fairness (less self-interest) in distributive justice is evident by children growing up in small-scale urban and traditional societies thought to promote more collective values.

-----------------------
 Thirty-six 2-, 4- and 6-month-old infants were videotaped while interacting with a female adult stranger engaging in either organized or disorganized 1-min peekaboo games. Two-month-old infants gazed and smiled equally at the stranger, regardless of the relative organization of the peekaboo game. In contrast, 4- and 6-month-old infants smiled significantly more and gazed significantly less;in the organized peekaboo condition than in the disorganized peekaboo condition. These results suggest that from a diffuse sensitivity to the presence of a social partner, infants by 4 months develop a new sensitivity to the narrative envelope of protoconversation, in particular the timing and the structure of social exchanges scaffolded by adults. These observations are interpreted as evidence of developing social expectations in the first 6 months of life. This early development is viewed as announcing and preparing the communicative competence that blossoms by the end of the 1st year.

 Two-month-old infants (N = 29) participated in face-to-face interactions with their mothers and with strangers. The contingent responsiveness for smiles and vocalizations, while attending to the partner, was assessed for each partner in both interactions. For smiles and for vocalizations, infants were less responsive to the stranger relative to the mother when the stranger's contingent responsiveness was either more contingent or less contingent than that of the mother. Results are supportive of the hypothesis that young infants develop sensitivities to levels of social contingency present in their maternal interactions, which influence their responsiveness to others.

-----------------------
 A growing body of research indicates that children do not understand mental representation until around age 4. However, children engage in pretend play by age 2, and pretending seems to require understanding mental representation. This apparent contradiction has been reconciled by the claim that in pretense there is precocious understanding of mental representation. 4 studies tested this claim by presenting children with protagonists who were not mentally representing something (i.e., an animal), either because they did not know about the animal or simply because they were not thinking about being the animal. However, the protagonists were acting in ways that could be consistent with pretending to be that animal. Children were then asked whether the protagonists were pretending to be that animal, and children tended to answer in the affirmative. The results suggest that 4-year-olds do not understand that pretending requires mental representation. Children appear to misconstrue pretense as its common external manifestations, such as actions, until at least the sixth year.

 How children understand the mental state of pretense has recently become an active area of inquiry, with some research suggesting that young children do not understand that pretending is based on mentally representing some alternate state of affairs. Because intention is thought to be understood earlier than mental representation generally, these experiments tested whether children understand pretense intentions at an earlier age than they understand pretense mental representations. Children were told about a character's intentions and conflicting actions, and were asked about the character's pretense. Across 5 experiments, children did not demonstrate appreciation that intention is crucial to pretense. Various methodological factors that might have compromised the results were examined, but to no effect.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Researchers studying early social cognition have been particularly interested in pretend play and have obtained evidence indicating that young children do not understand that pretending involves mental representation. The present research investigates whether children think of pretending as a mental state at all, by looking at whether they cluster it with other mental states or with physical processes when making certain judgments. The results from 5 experiments suggest that most children under 6 years of age see pretending as primarily physical. Further, when asked about pretending as a e-part process entailing planning and execution, even 8-year-olds claim that execution of pretense does not involve the mind, although the planning aspect of pretense does.

 Mothers begin to pretend with their children during the second year, when children still have much to learn about the real world. Although it would be easy to confuse what is pretend with what is real, children at this young age often demonstrate comprehension during pretense situations. It is plausible that social referencing, in which the child uses the mother's emotional expression as a guide to behavior, might facilitate this emerging knowledge by signaling to the child not to take the pretend situation seriously. Data from 32 pairs of mothers and their 18-month-olds who had engaged in pretend and real snack behaviors were subjected to a sequential analysis to investigate a social referencing interpretation. Consistent with our hypothesis, behaviors suggestive of a baby's understanding pretense were more likely to follow a specific combination of behaviors consistent with social referencing than other combinations of behaviors. These results provide support for the possibility that children use information obtained through social referencing to assist understanding during pretense interactions.

-----------------------
 Much of young children's symbolic play is heavily scaffolded by adult symbolic action models, which children may imitate, and by adult verbal scripts. The current studies attempted to evaluate 18-35-month-old children's symbolic skills in the absence of such scaffolding. In a study of symbol comprehension, children were rested for their ability to comprehend an adult's use of either a replica object or an associated gesture to communicate which object in an array she want ed. In a study of symbol production, children were given some objects that afforded symbolic manipulations, but without adult symbolic action models or verbal scripts. The results of the two studies converged to suggest chat children below 2 years of age have symbolic skills with gestures, but: nor with objects. It was also found that while children at 26 months were able to use an object as a symbol for another object, they had difficulties when the symbol had another conventional use (e.g. a drinking cup used as a hat). The findings are discussed in terms of DeLoache's dual representation model, and a modification of chat: model is proposed.

 In this study we sought to determine the degree to which 2- to 3-year-old children use objects symbolically in the relative absence of adult symbolic actions or linguistic descriptions, and how the nature of objects influences symbolic play. Results revealed a dramatic increase in children's creative symbolic productions between 2 and 3 years of age, with the tendency to produce symbolic actions influenced to an equal degree by adult symbolic action models and verbal directions. Children of all ages were heavily influenced by the nature of the object to be used as a symbol, with the youngest children using only replica objects as symbols. In a second study, we examined childrens looks to an adult as they engaged in different kinds of activities with objects. The main finding was that children looked to the adult immediately after performing a symbolic action more often than if they performed an instrumental action. We argue for the essentially social nature of symbolic play, both in terms of how children learn to use objects as symbols and in terms of the reasons they do so.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 The current studies used a syntactic priming paradigm with 3- and 4-year-old children. In Experiment 1, children were asked to describe a series of drawings depicting transitive and dative relations to establish baseline production levels. In Experiment 2, an experimenter described a similar series of drawings using one of two syntactic forms (i.e., active/passive for transitive; double-object/prepositional for dative). Children were then asked to describe pictures identical to those shown in the corresponding baseline procedure, In both transitive and dative conditions, 4-year-old children were more likely to use a particular syntactic form if it had been used by the experimenter. Three-year-old children did not show priming effects, but their production of transitive sentences was higher following transitive primes than in Experiment 1. In Experiment 3, an additional group of 3-year-olds participated in a procedure in which they repeated the experimenter's sentences before describing the pictures. This procedure yielded significant priming effects for transitive and dative forms. These results indicate that very young children possess abstract syntactic representations, but that their access to these representations is sensitive to task demands.

 We used a syntactic priming paradigm to show priming effects for active and passive forms in monolingual Spanish-speaking four- and five-vear-olds. In a baseline experiment, we examined children's use of the fue-passive form and found it was virtually non-existent in their speech, although they produced important elements of the form. Children used a more frequent Spanish passive form, the subjectless/se-passive. In a priming experiment, we presented children with drawings described using either active or fue-passive sentences. Children then described novel drawings. Priming was induced for active and passive forms; however, children did not produce the fue-passive provided for them. Instead, children used the subjectless/se-passive and what we term the function-passive, which like the fue-passive, emphasize the patient of the action. We argue that children's use of different passive forms suggests they are sensitive to experimenter's input as it relates to scene interpretation and to syntax.

-----------------------
 Using diffusion tensor imaging and tractography, we found that a disruption in structural connectivity in ventral occipitotemporal cortex may be the neurobiological basis for the lifelong impairment in face recognition that is experienced by individuals who suffer from congenital prosopagnosia. Our findings suggest that white-matter fibers in ventral occipitotemporal cortex support the integrated function of a distributed cortical network that subserves normal face processing.

 The summed activity of multiple nodes of a distributed cortical network supports face recognition in humans, including "core" ventral occipitotemporal cortex (VOTC) regions [1-3], and "extended" regions outside VOTC [4, 5]. Many individuals with congenital prosopagnosia-an impairment in face processing [6-9]-exhibit normal blood oxygenation level-dependent (BOLD) activation in the core VOTC regions [10, 11]. These individuals evince a reduction in the structural integrity of the white matter tracts connecting VOTC to anterior temporal and frontal cortices [12], part of the "extended" face network. The impairment in congenital prosopagnosia may arise, therefore, not from a dysfunction of the core VOTC areas but from a failure to propagate signals between the intact VOTC and the extended nodes of the network. Using the fMR adaptation paradigm with famous and unknown faces, we show that individuals with congenital prosopagnosia evince normal adaptation effects in VOTC, indicating sensitivity to facial identity, but show no differential activation for familiar versus unknown faces outside VOTC, particularly in the precuneus/posterior cingulate cortex and the anterior paracingulate cortex. Normal BOLD activation in VOTC is thus insufficient to subserve intact face recognition, and disrupted information propagation between VOTC and the extended face processing network may explain the functional impairment in congenital prosopagnosia.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 This study examined performance measures and eye movements associated with complex arithmetic strategies in young and older adults. Participants added pairs of three-digit numbers using two different strategies, under choice and no-choice conditions. Older adults made more errors but were not significantly slower than young adults, and response times and errors showed no interaction between age and the number of carries. Older adults chose strategies less adaptively than young adults. Eye movements were consistent with use of required strategies on no-choice trials and reported strategies on choice trials. Eye movement data also suggested that young adults more successfully distinguished between strategies. Implications of these findings for understanding aging effects in complex arithmetic are discussed. (C) 2006 Elsevier B.V. All rights reserved.

 Young and older adults solved complex addition problems such as 49 + 56. Verbal protocols, solution times, and percentage errors documented strategy repertoire and strategy selection in young and older participants and age-related differences in complex arithmetic performance. Both young and older adults used a set of 9 strategies to solve complex addition problems, although many older adults used a smaller strategy repertoire. The data also showed age-related differences in strategy execution and strategy selection. We discuss general implications of the present data to further our understanding of complex arithmetic and the role of strategic variations during aging.

-----------------------
 Mechanisms underlying the binocular combination of visual information were investigated within the context of a visual information acquisition theory proposed by Loftus, Busey, and their colleagues (e.g., as described by T. A. Busey &amp; G. R. Loftus, 1994). A central assumption of the theory is that of a sensory threshold, which engenders an information loss such that information processing subsequent to the threshold is assumed to occur only when the magnitude of a sensory representation triggered by the stimulus exceeds the threshold. The presumed sensory threshold may be situated prior to or subsequent to the point at which the information from the two eyes combines. The location of this threshold was investigated in 3 experiments that provided conclusions about the location of the sensory thresholds and the mechanisms of binocular combination. It is concluded that a linear summation mechanism, an independent sampling information acquisition model, and both pre-and postcombinatorial sources of information loss are required to account for the data.

 We test 3 theories of global and local scene information acquisition, defining global and local in terms of spatial frequencies. By independence theories, high- and low-spatial-frequency information are acquired over the same time course and combine additively. By global-precedence theories, global information acquisition precedes local information acquisition, but they combine additively. By interactive theories, global information also affects local-information acquisition rate. We report 2 digit-recall experiments. In the 1st, we confirmed independence theories. In the 2nd, we disconfirmed both independence theories and interactive theories. leaving global-precedence theories as the remaining alternative. We show that a specific global-precedence theory quantitatively accounted for Experiments 1-2 data as well as for past data. We discuss how their spatial-frequency definition of spatial scale comports with definitions used by others, and we consider the suggestion by P. G. Schyns and colleagues (e.g., D. J. Morrison &amp; Schyns, 2001) that the visual system may act flexibly rather than rigidly in its use of spatial scales.

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 Case-based Reasoning (CBR) began as a theory of human cognition, but has attracted relatively little direct experimental or theoretical investigation in psychology. However, psychologists have developed a range of instance-based theories of cognition and have extensively studied how similarity to past cases can guide categorization of new cases. This paper considers the relation between CBR and psychological research, focussing on similarity in human and artificial case-based reasoning in law. We argue that CBR, psychology and legal theory have complementary contributions to understanding similarity, and describe what each offers. This allows us to establish criteria for assessing existing CBR systems in law and to establish what we consider to be the crucial goals for further research on similarity, both from a psychological and a CBR perspective.

 Normative theories provide essential tools for understanding behaviour, not just for reasoning, judgement, and decision-making, but many other areas of cognition as well; and their utility extends to the development of process theories. Furthermore, the way these tools are used has nothing to do with the is-ought fallacy. There therefore seems no basis for the claim that research would be better off without them.

-----------------------
 Behavioral and brain imaging research indicates that human infants, humans adults, and many nonhuman animals represent large nonsymbolic numbers approximately, discriminating between sets with a ratio limit on accuracy. Some behavioral evidence, especially with human infants, suggests that these representations differ from representations of small numbers of objects. To investigate neural signatures of this distinction, event-related potentials were recorded as adult humans passively viewed the sequential presentation of dot arrays in an adaptation paradigm. In two studies, subjects viewed successive arrays of a single number of dots interspersed with test arrays presenting the same or a different number; numerical range (small numerical quantities 1-3 vs. large numerical quantities 8-24) and ratio difference varied across blocks as continuous variables were controlled. An early-evoked component (N1), observed over widespread posterior scalp locations, was modulated by absolute number with small, but not large, number arrays. In contrast, a later component (P2p), observed over the same scalp locations, was modulated by the ratio difference between arrays for large, but not small, numbers. Despite many years of experience with symbolic systems that apply equally to all numbers, adults spontaneously process small and large numbers differently. They appear to treat small-number arrays as individual objects to be tracked through space and time, and large-number arrays as cardinal values to be compared and manipulated.

 Behavioral research suggests that two cognitive systems are at the foundations of numerical thinking: one for representing 1-3 objects in parallel and one for representing and comparing large, approximate numerical magnitudes. We tested for dissociable neural signatures of these systems in preverbal infants by recording event-related potentials (ERPs) as 6-7.5-month-old infants (n = 32) viewed dot arrays containing either small (1-3) or large (8-32) sets of objects in a number alternation paradigm. If small and large numbers are represented by the same neural system, then the brain response to the arrays should scale with ratio for both number ranges, a behavioral and brain signature of the approximate numerical magnitude system obtained in animals and in human adults. Contrary to this prediction, a mid-latency positivity (P500) over parietal scalp sites was modulated by the ratio between successive large, but not small, numbers. Conversely, an earlier peaking positivity (P400) over occipital-temporal sites was modulated by the absolute cardinal value of small, but not large, numbers. These results provide evidence for two early developing systems of non-verbal numerical cognition: one that responds to small quantities as individual objects and a second that responds to large quantities as approximate numerical values. These brain signatures are functionally similar to those observed in previous studies of non-symbolic number with adults, suggesting that this dissociation may persist over vast differences in experience and formal training in mathematics.

-----------------------
 A search is presented for production of a heavy up-type quark (t') together with its antiparticle, assuming a significant branching ratio for subsequent decay into a W boson and a b quark. The search is based on 4.7 fb(-1) of pp collisions root s = 7 TeV recorded in 2011 with the ATLAS detector at the CERN Large Hadron Collider. Data are analyzed in the lepton + jets final state, characterized by a high-transverse-momentum isolated electron or muon, large missing transverse momentum and at least three jets. The analysis strategy relies on the substantial boost of the W bosons in the t'(t') over bar signal when m(t') greater than or similar to 400 GeV. No significant excess of events above the Standard Model expectation is observed and the result of the search is interpreted in the context of fourth-generation and vector-like quark models. Under the assumption of a branching ratio BR(t' -&gt; W b) = I, a fourth-generation t' quark with mass lower than 656 GeV is excluded at 95% confidence level. In addition, in light of the recent discovery of a new boson of mass similar to 126 GeV at the LHC, upper limits are derived in the two-dimensional plane of BR(t' -&gt; Wb) versus BR(t' -&gt; Ht), where H is the Standard Model Higgs boson, for vector-like quarks of various masses. (C) 2012 CERN. Published by Elsevier B.V. All rights reserved.

 The results of a search for an excited bottom-quark b* in pp collisions at root s = 7 TeV, using 4.7 fb(-1) of data collected by the ATLAS detector at the LHC are presented. In the model studied, a single b*-quark is produced through a chromomagnetic interaction and subsequently decays to a W boson and a top quark. The search is performed in the dilepton and lepton + jets final states, which are combined to set limits on b*-quark couplings for a range of b*-quark masses. For a benchmark with unit size chromomagnetic and Standard Model-like electroweak b* couplings, b* quarks with masses less than 870 GeV are excluded at the 95% credibility level. (C) 2013 CERN. Published by Elsevier B.V. All rights reserved.

-----------------------
 Most human cognition occurs outside conscious awareness or conscious control. Some of these implicit processes influence social perception, judgment and action. The past 15 years of research in implicit social cognition can be characterized as the Age of Measurement because of a proliferation of measurement methods and research evidence demonstrating their practical value for predicting human behavior. Implicit measures assess constructs that are distinct, but related, to self-report assessments, and predict variation in behavior that is not accounted for by those explicit measures. The present state of knowledge provides a foundation for the next age of implicit social cognition: clarification of the mechanisms underlying implicit measurement and how the measured constructs influence behavior.

 The commentary to this special issue discusses how current research on implicit and automatic processes in personality is related to previous attempts to conceptualise and measure automatic or implicit aspects of personality that are difficult to assess using standard self-report measures. The six original contributions of this issue are discussed in the light of earlier works and with respect to interesting research questions emerging from them. We point to a range of new implicit measures of personality, discuss current trends in implicit personality research and conclude with suggestions howfuture research could use implicitpersonality measures to improve our understanding of self-regulatory andproblem behaviour Copyright (c) 2007 John Wiley &amp; Sons, Ltd.

-----------------------
 Most human cognition occurs outside conscious awareness or conscious control. Some of these implicit processes influence social perception, judgment and action. The past 15 years of research in implicit social cognition can be characterized as the Age of Measurement because of a proliferation of measurement methods and research evidence demonstrating their practical value for predicting human behavior. Implicit measures assess constructs that are distinct, but related, to self-report assessments, and predict variation in behavior that is not accounted for by those explicit measures. The present state of knowledge provides a foundation for the next age of implicit social cognition: clarification of the mechanisms underlying implicit measurement and how the measured constructs influence behavior.

 The commentary to this special issue discusses how current research on implicit and automatic processes in personality is related to previous attempts to conceptualise and measure automatic or implicit aspects of personality that are difficult to assess using standard self-report measures. The six original contributions of this issue are discussed in the light of earlier works and with respect to interesting research questions emerging from them. We point to a range of new implicit measures of personality, discuss current trends in implicit personality research and conclude with suggestions howfuture research could use implicitpersonality measures to improve our understanding of self-regulatory andproblem behaviour Copyright (c) 2007 John Wiley &amp; Sons, Ltd.

-----------------------
 Most human cognition occurs outside conscious awareness or conscious control. Some of these implicit processes influence social perception, judgment and action. The past 15 years of research in implicit social cognition can be characterized as the Age of Measurement because of a proliferation of measurement methods and research evidence demonstrating their practical value for predicting human behavior. Implicit measures assess constructs that are distinct, but related, to self-report assessments, and predict variation in behavior that is not accounted for by those explicit measures. The present state of knowledge provides a foundation for the next age of implicit social cognition: clarification of the mechanisms underlying implicit measurement and how the measured constructs influence behavior.

 The commentary to this special issue discusses how current research on implicit and automatic processes in personality is related to previous attempts to conceptualise and measure automatic or implicit aspects of personality that are difficult to assess using standard self-report measures. The six original contributions of this issue are discussed in the light of earlier works and with respect to interesting research questions emerging from them. We point to a range of new implicit measures of personality, discuss current trends in implicit personality research and conclude with suggestions howfuture research could use implicitpersonality measures to improve our understanding of self-regulatory andproblem behaviour Copyright (c) 2007 John Wiley &amp; Sons, Ltd.

-----------------------
 Most theories in social and political psychology stress self-interest, intergroup conflict, ethnocentrism, homophily, ingroup bias, outgroup antipathy, dominance, and resistance. System justification theory is influenced by these perspectives-including social identity and social dominance theories-but it departs from them in several respects. Advocates of system justification theory argue that (a) there is a general ideological motive to justify the existing social order, (b) this motive is at least partially responsible for the internalization of inferiority among members of disadvantaged groups, (c) it is observed most readily at an implicit, nonconscious level of awareness and (d) paradoxically, it is sometimes strongest among those who are most harmed by the status quo. This article reviews and integrates 10 years of research on 20 hypotheses derived from a system justification perspective, focusing on the phenomenon of implicit outgroup favoritism among members of disadvantaged groups (including African Americans, the elderly, and gays/lesbians) and its relation to political ideology (especially liberalism-conservatism).

 Mental process and mental experience are not the same thing. The former is the operation of the mind; the latter is the subjective life that emerges from that operation. In social evaluation, implicit and explicit attitudes express this distinction. Although it is clear that they are not the same, how they differ is not. Across content domains, implicit and explicit attitude measures show substantial variability in the strength of correspondence, ranging from near zero to strongly positive. Variation in controllability, intentionality, awareness, or efficiency is thought to differentiate implicit and explicit attitudes. Dual-process theories and empirical evidence for moderating influences of implicit-explicit attitude relations provide a framework for comprehending relations between the operation and the experience of the mind.

-----------------------
 Studies indicate that children believe that positive behaviors are more likely than negative ones to remain stable across time and situations. The present study assessed whether children hold such optimism equally regarding their own vs. others' behavioral patterns. Thirty five-year-olds answered questions about the extent to which they viewed themselves as having various positively, neutrally, and negatively valued behavioral patterns. An experimenter then asked children about the extent to which behavioral patterns that children thought they had would remain stable in themselves and in others, and the extent to which behavioral patterns that they did not think they had would remain stable in others. We found that children gave higher stability ratings for positive behaviors in themselves than in others, and the opposite regarding negative behaviors. This self-protective optimism is discussed vis--vis the relationship between children's beliefs about traits and their behaviors and motivations.

 The Trends in International Mathematics and Science Study found that even though children from all East Asian countries outperformed American children, American students reported higher self-evaluation of their math and science abilities than did students from East Asian countries such as China, Korea, and Japan (Mullis, Martin, Gonzalez, &amp; Chrostowski, 2004). Such cross-cultural differences in self-appraisal fit the stereotype of the modest East Asian and contribute to the received view that East Asians have less positive self-concepts than Americans. This view was summarized recently by Heine, Lehman, Markus, and Kitayama ( 1999) as follows: "The need for positive self-regard, as it is currently conceptualized, is not a universal, but rather is rooted in significant aspects of North American culture'' ( p. 766; but cf. Sedikides, Gaertner,&amp; Vevea, 2005).

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 The masked affective priming task was used as an unobtrusive measure of intergroup prejudices in a sample of German adolescents (aged 13-15). Pictures of Turks and Germans were used as masked primes that preceded positive and negative target adjectives conveying either other-relevant valence (e.g., honest, evil) or possessor-relevant valence (e.g., talented, dull). Affective priming indices (denoting relative negativity of Turkish primes) were positively correlated with the open expression of prejudices towards Turks and foreigners in general in questionnaires as well as with discriminative interaction behavior in a virtual ball-tossing game. As expected, these correlations were found only for priming indices based on other-relevant targets, thereby emphasizing the differentiation of automatic prejudice into (imputed) hostility and depreciation.

 In this paper, we yield evidence for the dependence of affective priming on the congruency of the previous trial. Affective priming refers to the finding that valence categorizations of targets are facilitated when the preceding prime is of the same valence. In two experiments, affective priming was diminished after incongruent trials (i.e., prime and target were of different valence), whereas, significant affective priming was observed after congruent trials (i.e., prime and target were of same valence). We compare this pattern to the known sequential dependencies in Stroop- and Eriksen-type tasks. Furthermore, our results can help to improve the statistical power of studies in which the affective priming task is used as a measure for automatic evaluations of attitude-objects. (c) 2008 Elsevier B.V. All rights reserved.

-----------------------
 Adaptive network and exemplar-similarity models were compared on their ability to predict category learning and transfer data. An exemplar-based network (Kruschke, 1990a, 1990b, 1992) that combines key aspects of both modeling approaches was also tested. The exemplar-based network incorporates an exemplar-based category representation in which exemplars become associated to categories through the same error-driven, interactive learning rules that are assumed in standard adaptive networks. Experiment 1, which partially replicated and extended the probabilistic classification learning paradigm of Gluck and Bower (1988a), demonstrated the importance of an error-driven learning rule. Experiment 2, which extended the classification learning paradigm of Medin and Schaffer (1978) that discriminated between exemplar and prototype models, demonstrated the importance of an exemplar-based category representation. Only the exemplar-based network accounted for all the major qualitative phenomena; it also achieved good quantitative predictions of the learning and transfer data in both experiments.

 This research's purpose was to contrast the representations resulting from learning of the same categories by either classifying instances or inferring instance features. Prior inference learning research, particularly T. Yamauchi and A. B. Markman (1998), has suggested that feature inference learning fosters prototype representation, whereas classification learning encourages exemplar representation. Experiment I supported this hypothesis. Averaged and individual participant data from transfer after inference training were better fit by a prototype than by an exemplar model. However, Experiment 2. with contrasting inference learning conditions, indicated that the prototype model was mimicking a set of label-based bidirectional rules, as determined by the inference learning task demands in Experiment 1. Only the set of rules model accounted for all the inference learning conditions in these experiments.

-----------------------
 Adaptive network and exemplar-similarity models were compared on their ability to predict category learning and transfer data. An exemplar-based network (Kruschke, 1990a, 1990b, 1992) that combines key aspects of both modeling approaches was also tested. The exemplar-based network incorporates an exemplar-based category representation in which exemplars become associated to categories through the same error-driven, interactive learning rules that are assumed in standard adaptive networks. Experiment 1, which partially replicated and extended the probabilistic classification learning paradigm of Gluck and Bower (1988a), demonstrated the importance of an error-driven learning rule. Experiment 2, which extended the classification learning paradigm of Medin and Schaffer (1978) that discriminated between exemplar and prototype models, demonstrated the importance of an exemplar-based category representation. Only the exemplar-based network accounted for all the major qualitative phenomena; it also achieved good quantitative predictions of the learning and transfer data in both experiments.

 This research's purpose was to contrast the representations resulting from learning of the same categories by either classifying instances or inferring instance features. Prior inference learning research, particularly T. Yamauchi and A. B. Markman (1998), has suggested that feature inference learning fosters prototype representation, whereas classification learning encourages exemplar representation. Experiment I supported this hypothesis. Averaged and individual participant data from transfer after inference training were better fit by a prototype than by an exemplar model. However, Experiment 2. with contrasting inference learning conditions, indicated that the prototype model was mimicking a set of label-based bidirectional rules, as determined by the inference learning task demands in Experiment 1. Only the set of rules model accounted for all the inference learning conditions in these experiments.

-----------------------
 Observers completed perceptual categorization tasks in which base rates and payoffs were manipulated separately or simultaneously across a range of category discriminabilities. Decision criterion estimates from the simultaneous base-rate/payoff conditions were closer to optimal than those predicted from the independence assumption, in line with predictions from the flat-maxima hypothesis. A hybrid model that instantiated the flat-maxima and competition between reward and accuracy maximization hypotheses was applied to the data as well as used in a reanalysis of C. J. Bohil and W. J. Maddox's (2001) study. The hybrid model was superior to a model that incorporated the independence assumption, suggesting that violations of the independence assumption are to be expected and are well captured by the flat-maxima hypothesis, without requiring any additional assumptions.

 Observers completed perceptual categorization tasks that included separate base-rate/payoff manipulations, corresponding simultaneous base-rate/payoff manipulations, and conflicting simultaneous base-rate/payoff manipulations. Performance (1) was closer to optimal for 2:1 than for 3:1 base-rate/payoff ratios and when base rates as opposed to payoffs were manipulated, and (2) was more in line with the predictions from the flat-maxima hypothesis than from the independence assumption of the optimal classifier in corresponding and conflicting simultaneous base-rate/payoff conditions. A hybrid model that instantiated simultaneously the flat-maxima and the competition between reward and accuracy maximization (COBRA) hypotheses was applied to the data. The hybrid model was superior to a model that incorporated the independence assumption, suggesting that violations of the independence assumption are to be expected and are well captured by the flat-maxima hypothesis without requiring any additional assumptions. The parameters indicated that observers' reward-maximizing decision criterion rapidly approaches the optimal value and that more weight is placed on accuracy maximization in separate and corresponding simultaneous base-rate/payoff conditions than in conflicting simultaneous base-rate/payoff conditions.

-----------------------
 Three experiments assessed people's ability to strategically regulate memory accuracy in free report. Older adults were substantially less accurate than young adults in free report cued recall. Both older and younger adults made gains in memory accuracy from forced report to free report, but older adults did so at the expense of greater losses in quantity correct. This pattern of gains in accuracy at the cost of losses in quantity was mediated by the level of memory monitoring, and older adults showed less correspondence between their confidence judgments and the accuracy of their responses. When young adults encoded items with full vs. divided attention, the resulting differences in retention set off a cascade of effects including poorer memory monitoring and, ultimately, lower accuracy in free report. We suggest that older adults' problems with memory monitoring and memory accuracy stem from impairments in their ability to recollect details of events. (C) 2002 Elsevier Science (USA). All rights reserved

 Researchers studying human memory have increasingly focused on memory accuracy in aging populations. In this article we briefly review the literature on memory accuracy in healthy older adults. The prevailing evidence indicates that, compared to younger adults, older adults exhibit both diminished memory accuracy and greater susceptibility to misinformation. In addition, older adults demonstrate high levels of confidence in their false memories. We suggest an explanatory framework for the high level of false memories observed in older adults, a framework based on the theory that consciously controlled uses of memory decline with age, making older adults more susceptible to false memories that rely on automatic processes. We also point to future research that may remedy such deficits in accuracy.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Three studies demonstrated that postsuppressional rebound (PSR) may be both reduced and enhanced by manipulating people's attributions about why they experience difficulty during suppression. Telling participants that suppression failures indicate a high motivation to use the suppressed construct produced more PSR than telling them that suppression failures indicate a low motivation to use the construct (Study 1). Telling participants that an external stimulus would make suppression easy produced more PSR than telling them that it would make suppression difficult (Study 2). Telling participants that suppressing a stereotype is difficult and unindicative of prejudice eliminated PSR (Study 3). These results support the notion that PSR occurs because people infer from the difficulty experienced during suppression and from suppression failures that they are motivated to use the suppressed construct.

 Generally, the accessibility of goal-related constructs is inhibited upon goal fulfillment. In line with this notion, the current studies explored whether violent computer games may reduce relative accessibility of aggression if the game involves the fulfillment of an aggressive goal. Specifically, in Study 1, participants who watched a trailer for a violent computer game that fulfilled the goal of venting anger showed less relative accessibility of aggression compared to participants who watched the trailer without goal fulfillment. In Study 2, actually playing a violent computer game to vent anger also decreased the relative accessibility of aggression compared to a control condition in which the game was played without such a goal. Lastly, in Study 3, the relative accessibility of aggression was reduced after playing a violent computer game for participants who reported a high general tendency to vent their anger.

-----------------------
 Indigenisation movements are a most welcome development for two interrelated reasons. The first is to make psychology relevant to non-Euroamerican societies. The second reason is to show up the ethnocentricity in contemporary Euroamerican psychology, including cross-cultural psychology. The topics studied, the concepts and instruments used, and the inferences drawn in most behavioural research are centred on Western views and interests. However, it can be questioned whether the current literature on indigenisation serves the societies for which it is intended much better than "traditional" cross-cultural psychology. The indigenisation literature seems to capitalise too much on cross-cultural differences in behaviour and to negate important invariances in psychological functioning across cultures. Such a one-sided emphasis is argued to be factually incorrect and theoretically misleading.

 Four traditions in research on personality and culture are distinguished: (i) the culture-and-personality school and recent relativistic perspectives, (ii) the trait approach, (iii) interactionistic orientations, and (iv) situationist approaches. Next, the first two of these traditions are evaluated to ascertain how much variance is explained by culture. Thereafter, it is argued that the (questionable) focus on explanations with a high level of inclusiveness or generality is a major reason for the near absence of situationist interpretation of crosscultural differences. Finally, three possible strategies are discussed to bridge the gap between relativism (emphasizing differences) and universalism (assuming basic similarities). A suggestion is made as to how both approaches can be valuable when unexplainable, as well as explainable variances, in cross-cultural personality research are taken seriously.

-----------------------
 Because the probability of obtaining an experimental. finding given that the null hypothesis is true [p(F\H,)] is not the same as the probability that the null hypothesis is true given a finding [p(H-0\F)], calculating the former probability does not justify conclusions about the latter one. As the standard null-hypothesis significance-testing procedure does just that, it is logically invalid (J. Cohen, 1994). Theoretically, Bayes's theorem yields p(H-0\F), but in practice, researchers rarely know the correct values for 2 of the variables in the theorem. Nevertheless, by considering a wide range of possible values for the unknown variables, it is possible to calculate a range of theoretical values for p(H-0\F) and to draw conclusions about both hypothesis testing and theory evaluation.

 According to Bayesians, the null hypothesis significance-testing procedure is not deductively valid because it involves the retention or rejection of the null hypothesis under conditions where the posterior probability of that hypothesis is not known. Other criticisms are that this procedure is pointless and encourages imprecise hypotheses. However , according to non-Bayesians, there is no way of assigning a prior probability to the null hypothesis, and so Bayesian statistics do not work either. Consequently, no procedure has been accepted by both groups as providing a compelling reason to accept or reject hypotheses. The author aims to provide such a method. In the process, the author distinguishes between probability and epistemic estimation and argues that, although both are important in a science that is not completely deterministic, epistemic estimation is most relevant for hypothesis testing. Based on this analysis, the author proposes that hypotheses be evaluated via epistemic ratios and explores the implications of this proposal. One implication is that it is possible to encourage precise theorizing by imposing a penalty for imprecise hypotheses.

-----------------------
 Because the probability of obtaining an experimental. finding given that the null hypothesis is true [p(F\H,)] is not the same as the probability that the null hypothesis is true given a finding [p(H-0\F)], calculating the former probability does not justify conclusions about the latter one. As the standard null-hypothesis significance-testing procedure does just that, it is logically invalid (J. Cohen, 1994). Theoretically, Bayes's theorem yields p(H-0\F), but in practice, researchers rarely know the correct values for 2 of the variables in the theorem. Nevertheless, by considering a wide range of possible values for the unknown variables, it is possible to calculate a range of theoretical values for p(H-0\F) and to draw conclusions about both hypothesis testing and theory evaluation.

 According to Bayesians, the null hypothesis significance-testing procedure is not deductively valid because it involves the retention or rejection of the null hypothesis under conditions where the posterior probability of that hypothesis is not known. Other criticisms are that this procedure is pointless and encourages imprecise hypotheses. However , according to non-Bayesians, there is no way of assigning a prior probability to the null hypothesis, and so Bayesian statistics do not work either. Consequently, no procedure has been accepted by both groups as providing a compelling reason to accept or reject hypotheses. The author aims to provide such a method. In the process, the author distinguishes between probability and epistemic estimation and argues that, although both are important in a science that is not completely deterministic, epistemic estimation is most relevant for hypothesis testing. Based on this analysis, the author proposes that hypotheses be evaluated via epistemic ratios and explores the implications of this proposal. One implication is that it is possible to encourage precise theorizing by imposing a penalty for imprecise hypotheses.

-----------------------
 The covariation component of everyday causal inference has been depicted, in both cognitive and social psychology as well as in philosophy, as heterogeneous and prone to biases. The models and biases discussed in these domains are analyzed with respect to focal sets: contextually determined sets of events over which covariation is computed. Moreover, these models are compared to our probabilistic contrast model, which specifies causes as first and higher order contrasts computed over events in a focal set. Contrary to the previous depiction of covariation computation, the present assessment indicates that a single normative mechanism-the computation of probabilistic contrasts-underlies this essential component of natural causal induction both in everyday and in scientific situations.

 Because causal relations are neither observable nor deducible, they must be induced from observable events. The 2 dominant approaches to the psychology of causal induction-the covariation approach and the causal power approach-are each crippled by fundamental problems. This article proposes an integration of these approaches that overcomes these problems. The proposal is that reasoners innately treat the relation between covariation (a function defined in terms of observable events) and causal power(an unobservable entity) as that between scientists' law or model and their theory explaining the model. This solution is formalized in the power PC theory, a causal power theory of the probabilistic contrast model(P. W. Cheng &amp; L. R. Novick, 1990). The article reviews diverse old and new empirical tests discriminating this theory from previous models, none of which is justified by a theory. The results uniquely support the power PC theory.

-----------------------
 The idea that adverse life circumstances and negative life events contribute to disorder and disease has long been held. Advances in conceptualizing and defining these conditions under the common label of life stress have led to progress in measuring both the environmental and individual response characteristics that may promote disorder and disease. In general, a substantial and growing research literature supports the basic premise that life stress plays an important role in the development of many psychological and physical problems. Recent research, too, strongly suggests that interest in life stress in relation to health and disease will accelerate over the coming years. Yet debates and controversies remain concerning how to best conceptualize and measure life stress, which presents distinctive challenges for advancing the field. The present review examines the major issues pertaining to these debates, controversies, and challenges, for they will be crucial to resolve if progress is to be made in understanding ways in which life stress may or may not contribute to psychological and physical disorders.

 People have long believed that adversity and stress contribute to emotional problems in general and to depression in particular. A considerable body of research has supported this intuition, documenting a consistent association between major stressful life events and the onset of clinical depression. However, most individuals under stress do not become depressed, sometimes depression develops without prior stress, and distinguishing psychological distress from major depression can be diagnostically challenging. In varying forms and degrees, life stress may play multiple roles in relation to major depression. In this article, we outline the opportunities and obstacles associated with conceptualizing depression from a life-stress perspective and discuss the implications for future research.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 The authors address commentaries by J. Cassidy (2003), E. M. Cummings (2003), L. A. Sroufe (2003), and E. Waters and T. P. Beauchaine (2003) on their taxometric analysis of Strange Situation behavior (R. C. Fraley &amp; S. J. Spieker, 2003) by discussing four questions: Has the categorical model of attachment facilitated theoretical and empirical innovations in the field? How does a continuum of security fit into the two-dimensional model? What is the role of types and dimensions in understanding the function and organization of behavior? and Is dimensionality a null hypothesis in taxometric research?

 In contemporary psychology there is debate over whether individual differences in psychological constructs are stable over extended periods of time. The authors argue that it is impossible to resolve such debates unless researchers focus on patterns of stability and the developmental mechanisms that may give rise to them. To facilitate this shift in emphasis, they describe a formal model that integrates 3 developmental processes: stochastic-contextual processes, person-environment transactions, and developmental constancies. The theoretical and mathematical analyses indicate that this model makes novel predictions about the way in which test-retest correlations are structured across a wide range of ages and test-retest intervals. The authors illustrate the utility of the model by comparing its predictions against meta-analytic data on Neuroticism. The discussion emphasizes the value of focusing on patterns of continuity, not only as phenomena to be explained but as data capable of clarifying the developmental processes underlying stability and change for a variety of psychological constructs.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 In the study reported in this paper, we investigated the categorization of well-known and novel food items in the categories fruits and vegetables. Predictions based on Nosofsky's (1984, 1986) generalized context model (GCM), on a multiplicative-similarity prototype model, and on an instantiation model as applied in Storms, De Boeck, and Ruts (2001) were compared. Despite suggestions in the literature that prototype models predict categorization from large categories better than exemplar models do, our results showed that the exemplar-based GCM yielded clearly better predictions than did a (multiplicative-similarity) prototype model.

 An IRT model with a parameter-driven process for change is proposed. Quantitative differences between persons are taken into account by a continuous latent variable, as in common IRT models. In addition, qualitative interindividual differences and autodependencies are accounted for by assuming within-subject variability with respect to the parameters of the IRT model. In particular, the parameters of the IRT model are governed by an unobserved or '' hidden ''' homogeneous Markov process. The model includes the mixture linear logistic test model (Mislevy &amp; Verhelst, 1990), the mixture Rasch model (Rost, 1990), and the Saltus model (Wilson, 1989) as specific instances. The model is applied to a longitudinal experiment on discontinuity in conservation acquisition (van der Maas, 1993).

-----------------------
 Despite the potency of confession evidence in criminal law, recent DNA exonerations indicate that false confessions are a contributing factor in numerous wrongful convictions. After distinguishing between voluntary, compliant, and internalized false confessions, this article reviews research implicating a sequence of three processes responsible for false confessions and the adverse consequences of these confessions. First, police often target innocent people for interrogation because of erroneous judgments of truth and deception made during preinterrogation interviews. Second, innocent people are sometimes induced to confess as a function of certain police interrogation tactics, dispositional suspect vulnerabilities, and naive mental state that accompanies innocence. Third, people cannot readily distinguish between true and false confessions and often fail to discount those confessions they Perceive to be coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

 Despite the commonsense belief that people do not confess to crimes they did not commit, 20 to 25% of all DNA exonerations involve innocent prisoners who confessed. After distinguishing between voluntary, compliant, and internalized false confessions, this article suggests that a sequence of three processes is responsible for false confessions and their adverse consequences. First, police sometimes target innocent people for interrogation because of erroneous judgments of truth and deception. Second, innocent people sometimes confess as a function of certain interrogation tactics, dispositional suspect vulnerabilities, and the phenomenology of innocence. Third, jurors fail to discount even those confessions they see as coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

-----------------------
 Despite the potency of confession evidence in criminal law, recent DNA exonerations indicate that false confessions are a contributing factor in numerous wrongful convictions. After distinguishing between voluntary, compliant, and internalized false confessions, this article reviews research implicating a sequence of three processes responsible for false confessions and the adverse consequences of these confessions. First, police often target innocent people for interrogation because of erroneous judgments of truth and deception made during preinterrogation interviews. Second, innocent people are sometimes induced to confess as a function of certain police interrogation tactics, dispositional suspect vulnerabilities, and naive mental state that accompanies innocence. Third, people cannot readily distinguish between true and false confessions and often fail to discount those confessions they Perceive to be coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

 Despite the commonsense belief that people do not confess to crimes they did not commit, 20 to 25% of all DNA exonerations involve innocent prisoners who confessed. After distinguishing between voluntary, compliant, and internalized false confessions, this article suggests that a sequence of three processes is responsible for false confessions and their adverse consequences. First, police sometimes target innocent people for interrogation because of erroneous judgments of truth and deception. Second, innocent people sometimes confess as a function of certain interrogation tactics, dispositional suspect vulnerabilities, and the phenomenology of innocence. Third, jurors fail to discount even those confessions they see as coerced. At present, researchers are seeking ways to improve the accuracy of confession evidence and its evaluation in the courtroom.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 Three multimethod studies (total N = 348) probed the hypothesis that women's attraction to men would be influenced by male prosocial orientation. In Study 1, prosocial men were rated as more physically and sexually attractive, socially desirable, and desirable as dates than were nonprosocial men. Dominant men were no more attractive than low-dominance men, and male dominance did not interact with male prosocial orientation in eliciting attraction from women. In Study 2, prosocial orientation was manipulated to avoid ''personalism,'' but still affected attraction. Across all measures attraction was an interactive function of dominance and prosocial tendencies. Dominance alone did not increase any form of attraction measured. In Study 3, male prosocial tendencies and dominance interacted to affect women's attraction to men. Results are discussed in terms of the place of altruism and dominance in evolutionary approaches to human interpersonal attraction.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Hierarchical relations among theoretically generated lower order scales of adult temperament were explored in two studies. In Study One, 258 undergraduates completed the Adult Temperament Questionnaire (ATQ). A five-factor model emerged from exploratory factor analysis, with factors labeled Orienting Sensitivity, Effortful Control, Extraversion, Affiliativeness, and Negative Affect. This model showed considerable convergence with the Big Five. Study Two, with a community sample, of 700 participants, yielded a six-factor model, distinguishing aggressive negative affect from non-aggressive negative affect. Relations of the six temperament factors to Cloninger's TCI, the Five Factor Model, and the Multi-Language Seven were investigated, providing support for the discriminating power of the six-factor temperament model in understanding individual differences in adult temperament and personality. (C) 2006 Elsevier Inc. All rights reserved.

 The higher order structure of temperament was examined in two studies using the Adult Temperament Questionnaire. Because previous research showed robust levels of convergence between Rothbart's constructs of temperament and the Big Five factors, we hypothesized a higher order two-factor model of temperament based on Digman's higher order two-factor model of personality traits derived from factor analysis of the Big Five factors. Study I included 258 undergraduates. Digman's model did not fit the data well, so we conducted an exploratory two-factor solution. One factor included extraversion/positive emotionality, orienting sensitivity, and affiliativeness, and the other, negative affect versus effortful control content. This two-factor model of temperament model diverged from the Digman model only on the agreeableness-affiliativeness loadings. Study 2 involved a community sample of 700 participants. Confirmatory factor analysis supported the alternative model found in Study 1. Findings are discussed in relation to research on attention and emotion. (C) 2009 Elsevier Ltd. All rights reserved.

-----------------------
 The United States is becoming increasingly diverse, yet interracial contact continues to be awkward, if not stressful, for many. Indeed, recent research suggests that individuals often exit interracial interactions feeling drained both cognitively and emotionally. This article reviews research examining how interracial encounters give rise to these outcomes, zeroing in on the mediating role of self-regulation and the moderating influence of prejudice concerns. Given that interracial contact may be the most promising avenue to prejudice reduction, it is important to examine factors that undermine positive interracial contact experiences, as well as those that facilitate them.

 This paper explores the role of racial bias toward Blacks in interracial relations, and in racial disparities in health care in the United States. Our analyses of these issues focuses primarily on studies of prejudice published in the past 10 years and on health disparity research published since the report of the US Institute of Medicine (IOM) Panel on Racial and Ethnic Disparities in Health Care in 2003. Recent social psychological research reveals that racial biases occur implicitly, without intention or awareness, as well as explicitly, and these implicit biases have implications for understanding how interracial interactions frequently produce mistrust. We further illustrate how this perspective can illuminate and integrate findings from research on disparities and biases in health care, addressing the orientations of both providers and patients. We conclude by considering future directions for research and intervention. (c) 2008 Published by Elsevier Ltd.

-----------------------
 The United States is becoming increasingly diverse, yet interracial contact continues to be awkward, if not stressful, for many. Indeed, recent research suggests that individuals often exit interracial interactions feeling drained both cognitively and emotionally. This article reviews research examining how interracial encounters give rise to these outcomes, zeroing in on the mediating role of self-regulation and the moderating influence of prejudice concerns. Given that interracial contact may be the most promising avenue to prejudice reduction, it is important to examine factors that undermine positive interracial contact experiences, as well as those that facilitate them.

 This paper explores the role of racial bias toward Blacks in interracial relations, and in racial disparities in health care in the United States. Our analyses of these issues focuses primarily on studies of prejudice published in the past 10 years and on health disparity research published since the report of the US Institute of Medicine (IOM) Panel on Racial and Ethnic Disparities in Health Care in 2003. Recent social psychological research reveals that racial biases occur implicitly, without intention or awareness, as well as explicitly, and these implicit biases have implications for understanding how interracial interactions frequently produce mistrust. We further illustrate how this perspective can illuminate and integrate findings from research on disparities and biases in health care, addressing the orientations of both providers and patients. We conclude by considering future directions for research and intervention. (c) 2008 Published by Elsevier Ltd.

-----------------------
 The United States is becoming increasingly diverse, yet interracial contact continues to be awkward, if not stressful, for many. Indeed, recent research suggests that individuals often exit interracial interactions feeling drained both cognitively and emotionally. This article reviews research examining how interracial encounters give rise to these outcomes, zeroing in on the mediating role of self-regulation and the moderating influence of prejudice concerns. Given that interracial contact may be the most promising avenue to prejudice reduction, it is important to examine factors that undermine positive interracial contact experiences, as well as those that facilitate them.

 This paper explores the role of racial bias toward Blacks in interracial relations, and in racial disparities in health care in the United States. Our analyses of these issues focuses primarily on studies of prejudice published in the past 10 years and on health disparity research published since the report of the US Institute of Medicine (IOM) Panel on Racial and Ethnic Disparities in Health Care in 2003. Recent social psychological research reveals that racial biases occur implicitly, without intention or awareness, as well as explicitly, and these implicit biases have implications for understanding how interracial interactions frequently produce mistrust. We further illustrate how this perspective can illuminate and integrate findings from research on disparities and biases in health care, addressing the orientations of both providers and patients. We conclude by considering future directions for research and intervention. (c) 2008 Published by Elsevier Ltd.

-----------------------
 The United States is becoming increasingly diverse, yet interracial contact continues to be awkward, if not stressful, for many. Indeed, recent research suggests that individuals often exit interracial interactions feeling drained both cognitively and emotionally. This article reviews research examining how interracial encounters give rise to these outcomes, zeroing in on the mediating role of self-regulation and the moderating influence of prejudice concerns. Given that interracial contact may be the most promising avenue to prejudice reduction, it is important to examine factors that undermine positive interracial contact experiences, as well as those that facilitate them.

 This paper explores the role of racial bias toward Blacks in interracial relations, and in racial disparities in health care in the United States. Our analyses of these issues focuses primarily on studies of prejudice published in the past 10 years and on health disparity research published since the report of the US Institute of Medicine (IOM) Panel on Racial and Ethnic Disparities in Health Care in 2003. Recent social psychological research reveals that racial biases occur implicitly, without intention or awareness, as well as explicitly, and these implicit biases have implications for understanding how interracial interactions frequently produce mistrust. We further illustrate how this perspective can illuminate and integrate findings from research on disparities and biases in health care, addressing the orientations of both providers and patients. We conclude by considering future directions for research and intervention. (c) 2008 Published by Elsevier Ltd.

-----------------------
 Sixty European American male and female participants' implicit gender-related attitudes were assessed prior to engaging in a cross-gender dyadic interaction, according to one of three situational roles (superior, subordinate, or equal-status partner). Results revealed that the social roles affected male participants' gender attitudes. Specifically, male participants who anticipated an interaction with a female superior revealed negatively biased evaluative attitudes about women. By contrast, males who expected to interact with a female equal-status partner or subordinate revealed attitudes that were biased in favor of women. This finding highlights the importance of situational factors in the generation of implicit attitudes regarding social groups. Specifically, the present data point to the influence of situational status on males' attitudes regarding women. Implications of this work for integration and diversity initiatives are discussed.

 The current research examines the extent to which individuals endorse "sexblind" versus "sexaware" ideologies. Analogous to colorblind and multicultural ideologies, sexblindness involves ignoring sex categorization when perceiving others, and sexawareness involves recognizing and celebrating sex differences. Results revealed that participants endorsed sexblindness more (and, thus, sexawareness less) in work than in social contexts. Further, sexblindness was correlated with an internal motivation to respond without sexism, suggesting people perceive sexblindness as a way to reduce sexism. Consistent with this view, the more participants endorsed sexblindness in social settings, the less benevolent sexism they harbored. The implications of sexaware and sexblind ideologies and the difference between these ideologies and colorblind and multicultural ideologies are discussed.

-----------------------
 Sixty European American male and female participants' implicit gender-related attitudes were assessed prior to engaging in a cross-gender dyadic interaction, according to one of three situational roles (superior, subordinate, or equal-status partner). Results revealed that the social roles affected male participants' gender attitudes. Specifically, male participants who anticipated an interaction with a female superior revealed negatively biased evaluative attitudes about women. By contrast, males who expected to interact with a female equal-status partner or subordinate revealed attitudes that were biased in favor of women. This finding highlights the importance of situational factors in the generation of implicit attitudes regarding social groups. Specifically, the present data point to the influence of situational status on males' attitudes regarding women. Implications of this work for integration and diversity initiatives are discussed.

 The current research examines the extent to which individuals endorse "sexblind" versus "sexaware" ideologies. Analogous to colorblind and multicultural ideologies, sexblindness involves ignoring sex categorization when perceiving others, and sexawareness involves recognizing and celebrating sex differences. Results revealed that participants endorsed sexblindness more (and, thus, sexawareness less) in work than in social contexts. Further, sexblindness was correlated with an internal motivation to respond without sexism, suggesting people perceive sexblindness as a way to reduce sexism. Consistent with this view, the more participants endorsed sexblindness in social settings, the less benevolent sexism they harbored. The implications of sexaware and sexblind ideologies and the difference between these ideologies and colorblind and multicultural ideologies are discussed.

-----------------------
 Sixty European American male and female participants' implicit gender-related attitudes were assessed prior to engaging in a cross-gender dyadic interaction, according to one of three situational roles (superior, subordinate, or equal-status partner). Results revealed that the social roles affected male participants' gender attitudes. Specifically, male participants who anticipated an interaction with a female superior revealed negatively biased evaluative attitudes about women. By contrast, males who expected to interact with a female equal-status partner or subordinate revealed attitudes that were biased in favor of women. This finding highlights the importance of situational factors in the generation of implicit attitudes regarding social groups. Specifically, the present data point to the influence of situational status on males' attitudes regarding women. Implications of this work for integration and diversity initiatives are discussed.

 The current research examines the extent to which individuals endorse "sexblind" versus "sexaware" ideologies. Analogous to colorblind and multicultural ideologies, sexblindness involves ignoring sex categorization when perceiving others, and sexawareness involves recognizing and celebrating sex differences. Results revealed that participants endorsed sexblindness more (and, thus, sexawareness less) in work than in social contexts. Further, sexblindness was correlated with an internal motivation to respond without sexism, suggesting people perceive sexblindness as a way to reduce sexism. Consistent with this view, the more participants endorsed sexblindness in social settings, the less benevolent sexism they harbored. The implications of sexaware and sexblind ideologies and the difference between these ideologies and colorblind and multicultural ideologies are discussed.

-----------------------
 Sixty European American male and female participants' implicit gender-related attitudes were assessed prior to engaging in a cross-gender dyadic interaction, according to one of three situational roles (superior, subordinate, or equal-status partner). Results revealed that the social roles affected male participants' gender attitudes. Specifically, male participants who anticipated an interaction with a female superior revealed negatively biased evaluative attitudes about women. By contrast, males who expected to interact with a female equal-status partner or subordinate revealed attitudes that were biased in favor of women. This finding highlights the importance of situational factors in the generation of implicit attitudes regarding social groups. Specifically, the present data point to the influence of situational status on males' attitudes regarding women. Implications of this work for integration and diversity initiatives are discussed.

 The current research examines the extent to which individuals endorse "sexblind" versus "sexaware" ideologies. Analogous to colorblind and multicultural ideologies, sexblindness involves ignoring sex categorization when perceiving others, and sexawareness involves recognizing and celebrating sex differences. Results revealed that participants endorsed sexblindness more (and, thus, sexawareness less) in work than in social contexts. Further, sexblindness was correlated with an internal motivation to respond without sexism, suggesting people perceive sexblindness as a way to reduce sexism. Consistent with this view, the more participants endorsed sexblindness in social settings, the less benevolent sexism they harbored. The implications of sexaware and sexblind ideologies and the difference between these ideologies and colorblind and multicultural ideologies are discussed.

-----------------------
 The 42-item Iowa Personality Questionnaire (IPQ) was designed to briefly assess the same dimensions of personality measured by the Multidimensional Personality Questionnaire (MPQ; Tellegen, 1982). Results from three studies indicate that the IPQ can be used as a reasonable alternative to the MPQ when time constraints place limits on the length of questionnaires. For example, correlations between self-reports on the IPQ and the MPQ were .67 for Constraint, .75 for Negative Emotionality, and .77 for Positive Emotionality and the IPQ and MPQ demonstrate similar criterion-related validity (Study 1). Importantly, Study 3 illustrates how the IPQ can be used to address the substantive issue of the rank-order consistency of personality traits across time. Specifically, longitudinal results indicated that scores on the MPQ assessed in 1994 forecast scores on the IPQ in 2001. These findings provide further evidence for the continuity of individual differences across time. (c) 2004 Elsevier Inc. All rights reserved.

 Cross-sectional studies show that divorced people report lower levels of life satisfaction than do married people. However, such studies cannot determine whether satisfaction actually changes following divorce. In the current study, data from an 18-year panel study of more than 30,000 Germans were used to examine reaction and adaptation to divorce. Results show that satisfaction drops as one approaches divorce and then gradually rebounds over time. However, the return to baseline is not complete. In addition, prospective analyses show that people who will divorce are less happy than those who stay married, even before either group gets married. Thus, the association between divorce and life satisfaction is due to both preexisting differences and lasting changes following the event.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 The 42-item Iowa Personality Questionnaire (IPQ) was designed to briefly assess the same dimensions of personality measured by the Multidimensional Personality Questionnaire (MPQ; Tellegen, 1982). Results from three studies indicate that the IPQ can be used as a reasonable alternative to the MPQ when time constraints place limits on the length of questionnaires. For example, correlations between self-reports on the IPQ and the MPQ were .67 for Constraint, .75 for Negative Emotionality, and .77 for Positive Emotionality and the IPQ and MPQ demonstrate similar criterion-related validity (Study 1). Importantly, Study 3 illustrates how the IPQ can be used to address the substantive issue of the rank-order consistency of personality traits across time. Specifically, longitudinal results indicated that scores on the MPQ assessed in 1994 forecast scores on the IPQ in 2001. These findings provide further evidence for the continuity of individual differences across time. (c) 2004 Elsevier Inc. All rights reserved.

 Cross-sectional studies show that divorced people report lower levels of life satisfaction than do married people. However, such studies cannot determine whether satisfaction actually changes following divorce. In the current study, data from an 18-year panel study of more than 30,000 Germans were used to examine reaction and adaptation to divorce. Results show that satisfaction drops as one approaches divorce and then gradually rebounds over time. However, the return to baseline is not complete. In addition, prospective analyses show that people who will divorce are less happy than those who stay married, even before either group gets married. Thus, the association between divorce and life satisfaction is due to both preexisting differences and lasting changes following the event.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 The present study examined the influence of stable personality traits on romantic relationships using longitudinal data on a large, representative sample of young adults. Relationship experiences (quality, conflict, and abuse) showed relatively small mean-level changes over time and significant levels of rank-order stability, even across different relationship partners. Antecedent personality traits (assessed at age 18) predicted relationship experiences at age 26 and change in relationship experiences from age 21 to 26. Conversely, relationship experiences also predicted change in personality. Importantly, these findings generally held across relationship partners, suggesting that some people tend to be generally happy (or unhappy) across relationships, and this is due, in part, to stable individual differences in personality. Discussion focuses on the broader implications of the findings, in particular the need for relationship researchers to consider the importance of personality for why relationships thrive or fail and the need for personality researchers to consider the impact of relationship experiences on intraindividual personality development.

 This study provides a comprehensive picture of age differences in self-esteem from age 9 to 90 years using cross-sectional data collected from 326,641 individuals over the Internet. Self-esteem levels were high in childhood, dropped during adolescence, rose gradually throughout adulthood, and declined sharply in old age. This trajectory generally held across gender, socioeconomic status, ethnicity, and nationality (U.S. citizens vs. non-U.S. citizens). Overall, these findings support previous research, help clarify inconsistencies in the literature, and document new trends that require further investigation.

-----------------------
 This report provides evidence for the reliability, validity, and developmental course of the psychopathic personality traits (factors) of Fearless Dominance (FD) and Impulsive Antisociality (IA) as assessed by items from the Multidimensional Personality Questionnaire (MPQ; Patrick, Curtin, Tellegen, 2002). In Study 1, MPQ-based measures of FD and IA were strongly correlated with their corresponding composite scores from the Psychopathic Personality Inventory-Revised (Lilienfeld Widows, 2005). In Study 2, FD and IA had relatively distinct associations with measures of normal and maladaptive personality traits. In Study 3, FD and IA had substantial retest coefficients during the transition to adulthood, and both traits showed average declines with an especially substantial drop in IA. In Study 4, FD and IA were correlated with measures of internalizing and externalizing problems in ways consistent with previous research and theory. Collectively, these results provide important information about the assessment of FD and IA.

 This paper describes the creation and initial validation of new self-report measures of the psychopathic traits of Fearless Dominance and Impulsive Antisociality. In Study 1 we created new measures of these traits from the item content of three existing personality inventories: the IPIP-NEO [Johnson, J. A. (2000). Developing a short form of the IPIP-NEO: A report to HGW Consulting. Unpublished manuscript. Department of Psychology, University of Pennsylvania, DuBois, PA.], NEO PI-R [Costa, P. T., Jr., &amp; McCrae, R. R. (1992). Revised NEO Personality Inventory (NEO PI-R) and NEO Five-Factor Inventory (NEO-FFI) professional manual. Odessa, FL: Psychological Assessment Resources.], and HEXACO PI-R [Lee, K., &amp; Ashton, M. C. (2004). Psychometric properties of the HEXACO personality inventory. Multivariate Behavioral Research, 39, 329-358.]. Study 2 used confirmatory factor analysis to evaluate convergence across these new measures. Study 3 examined how strongly the HEXACO PI-R and IPIP-NEO measures converged with the Psychopathic Personality Inventory-Revised [Lilienfeld, S. O. &amp; Widows, M. R. (2005). Psychopathic Personality Inventory-Revised: Professional manual. Lutz, FI: Psychological Assessment Resources.]. Finally, Study 4 assessed self-other convergence and associations with observable aggressive responses for the IPIP-NEO scales. These studies provide important information about the assessment of these psychopathic traits. (C) 2009 Elsevier Inc. All rights reserved.

-----------------------
 The present study examined the influence of stable personality traits on romantic relationships using longitudinal data on a large, representative sample of young adults. Relationship experiences (quality, conflict, and abuse) showed relatively small mean-level changes over time and significant levels of rank-order stability, even across different relationship partners. Antecedent personality traits (assessed at age 18) predicted relationship experiences at age 26 and change in relationship experiences from age 21 to 26. Conversely, relationship experiences also predicted change in personality. Importantly, these findings generally held across relationship partners, suggesting that some people tend to be generally happy (or unhappy) across relationships, and this is due, in part, to stable individual differences in personality. Discussion focuses on the broader implications of the findings, in particular the need for relationship researchers to consider the importance of personality for why relationships thrive or fail and the need for personality researchers to consider the impact of relationship experiences on intraindividual personality development.

 This study provides a comprehensive picture of age differences in self-esteem from age 9 to 90 years using cross-sectional data collected from 326,641 individuals over the Internet. Self-esteem levels were high in childhood, dropped during adolescence, rose gradually throughout adulthood, and declined sharply in old age. This trajectory generally held across gender, socioeconomic status, ethnicity, and nationality (U.S. citizens vs. non-U.S. citizens). Overall, these findings support previous research, help clarify inconsistencies in the literature, and document new trends that require further investigation.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 Participants listened to randomly selected excerpts of popular music and rated how nostalgic each song made them feel. Nostalgia was stronger to the extent that a song was autobiographically salient, arousing, familiar, and elicited a greater number of positive, negative, and mixed emotions. These effects were moderated by individual differences (nostalgia proneness, mood state, dimensions of the Affective Neurosciences Personality Scale, and factors of the Big Five Inventory). Nostalgia proneness predicted stronger nostalgic experiences, even after controlling for other individual difference measures. Nostalgia proneness was predicted by the Sadness dimension of the Affective Neurosciences Personality Scale and Neuroticism of the Big Five Inventory. Nostalgia was associated with both joy and sadness, whereas nonnostalgic and nonautobiographical experiences were associated with irritation.

 This research examines the mechanisms underlying group-based differences in implicit attitudes and malleability of implicit attitudes resulting from exposure to exemplars. We tested whether these effects are due to differences in activated associations or to the regulation of those associations. In Study 1, Black participants exhibited less pro-White bias and activation of pro-White and anti-Black associations compared with White participants. In Study 2, exposure to positive Black and negative White exemplars produced lower pro-White bias and less activation of biased associations. These findings indicate that group-based differences in implicit attitudes and exemplar exposure effects reflect variability in and malleability of automatic associations. Implications for understanding the role of contact on intergroup attitudes are discussed.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 Continuity and change in Person-Environment Fit (PE Fit) and its relation to personality development was studied in a 4-year longitudinal study of college students (N=305). PE Fit demonstrated moderate rank-order stability and small increases in mean-levels over time. Antecedents to PE Fit included gender (being male), high academic ability, low agreeableness, and low neuroticism. Outcomes associated with PE Fit included greater personality consistency and changes in personality in the direction of higher self-esteem and lower agreeableness and neuroticism. The implications of the findings for personality development are discussed.

 The continuity and change of the needs and evaluations of the college environment and person-environment fit (PE fit) with the college environment were studied in a 4-year longitudinal study of students (N = 191). Perceptions of the environment changed more dramatically than corresponding self-perceived needs. PE fit demonstrated moderate levels of consistency over the 4-year span, but no significant increases in mean levels were found over time. Antecedents to PE fit in the college environment included both intelligence and openness to experience. Outcomes associated with PE fit included changes in personality traits linked to openness to experience and higher academic achievement. The implications of the findings for personality development and the relationship of PE fit to successful outcomes are discussed.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 The authors examined age differences in shame, guilt, and 2 forms of pride (authentic and hubristic) from age 13 years to age 89 years, using cross-sectional data from 2,611 individuals. Shame decreased from adolescence into middle adulthood, reaching a nadir around age 50 years, and then increased in old age. Guilt increased from adolescence into old age, reaching a plateau at about age 70 years. Authentic pride increased from adolescence into old age, whereas hubristic pride decreased from adolescence into middle adulthood, reaching a minimum around age 65 years, and then increased in old age. On average, women reported experiencing more shame and guilt; Blacks reported experiencing less shame and Asians more hubristic pride than other ethnicities. Across the life span, shame and hubristic pride tended to be negatively related to psychological well-being, and shame-free guilt and authentic pride showed positive relations with well-being. Overall, the findings support the maturity principle of personality development and suggest that as people age they become more prone to experiencing psychologically adaptive self-conscious emotions, such as guilt and authentic pride, and less prone to experiencing psychologically maladaptive ones, such as shame and hubristic pride.

 The authors examined the development of self-esteem from young adulthood to old age. Data came from the Americans' Changing Lives study, which includes 4 assessments across a 16-year period of a nationally representative sample of 3,617 individuals aged 25 years to 104 years. Latent growth curve analyses indicated that self-esteem follows a quadratic trajectory across the adult life span, increasing during young and middle adulthood, reaching a peak at about age 60 years, and then declining in old age. No cohort differences in the self-esteem trajectory were found. Women had lower self-esteem than did men in young adulthood, but their trajectories converged in old age. Whites and Blacks had similar trajectories in young and middle adulthood, but the self-esteem of Blacks declined more sharply in old age than did the self-esteem of Whites. More educated individuals had higher self-esteem than did less educated individuals, but their trajectories were similar. Moreover, the results suggested that changes in socioeconomic status and physical health account for the decline in self-esteem that occurs in old age.

-----------------------
 Terror management research has often shown that after reminders of mortality people show greater investment in and support for groups to which they belong. The question for the present research was whether or not this would extend to Euro American investment in their identification as White. Although it seemed unlikely that White participants would directly exhibit increased identification as Whites, we hypothesized that mortality salience would increase sympathy for other Whites who expressed racial pride or favoritism toward Whites. In support of the hypothesis, a White person expressing pride in his race was viewed by White participants as particularly racist relative to a Black person who does so in Study 1, but was deemed less racist after White participants were reminded of their own mortality in Study 2. Similarly, in Study 3, White participants rated an explicitly racist White employer as less racist when they were reminded beforehand of their own mortality. The results were discussed in terms of implications for affiliation with racist ideologies and terror management defenses.

 The present research investigated the hypotheses that elderly people can be reminders of our mortality and that concerns about our own mortality can therefore instigate ageism. In Study 1, college-age participants who saw photos of two elderly people subsequently showed more death accessibility than participants who saw photos of only younger people. In Study 2 making mortality salient for participants increased distancing from the average elderly person and decreased perceptions that the average elderly person Possesses favorable attitudes. Mortality salience did not affect ratings of teenagers. In Study 3, these mortality salience effects were moderated by prior reported similarity to elderly people. Distancing from, and derogation of, elderly people after mortality salience occurred only in participants who, weeks before the study, rated their personalities as relatively similar to the average elderly person's. Discussion addresses distinguishing ageism from other forms of prejudice, as well as possibilities for reducing ageism.

-----------------------
 Terror management research has often shown that after reminders of mortality people show greater investment in and support for groups to which they belong. The question for the present research was whether or not this would extend to Euro American investment in their identification as White. Although it seemed unlikely that White participants would directly exhibit increased identification as Whites, we hypothesized that mortality salience would increase sympathy for other Whites who expressed racial pride or favoritism toward Whites. In support of the hypothesis, a White person expressing pride in his race was viewed by White participants as particularly racist relative to a Black person who does so in Study 1, but was deemed less racist after White participants were reminded of their own mortality in Study 2. Similarly, in Study 3, White participants rated an explicitly racist White employer as less racist when they were reminded beforehand of their own mortality. The results were discussed in terms of implications for affiliation with racist ideologies and terror management defenses.

 The present research investigated the hypotheses that elderly people can be reminders of our mortality and that concerns about our own mortality can therefore instigate ageism. In Study 1, college-age participants who saw photos of two elderly people subsequently showed more death accessibility than participants who saw photos of only younger people. In Study 2 making mortality salient for participants increased distancing from the average elderly person and decreased perceptions that the average elderly person Possesses favorable attitudes. Mortality salience did not affect ratings of teenagers. In Study 3, these mortality salience effects were moderated by prior reported similarity to elderly people. Distancing from, and derogation of, elderly people after mortality salience occurred only in participants who, weeks before the study, rated their personalities as relatively similar to the average elderly person's. Discussion addresses distinguishing ageism from other forms of prejudice, as well as possibilities for reducing ageism.

-----------------------
 Terror management research has often shown that after reminders of mortality people show greater investment in and support for groups to which they belong. The question for the present research was whether or not this would extend to Euro American investment in their identification as White. Although it seemed unlikely that White participants would directly exhibit increased identification as Whites, we hypothesized that mortality salience would increase sympathy for other Whites who expressed racial pride or favoritism toward Whites. In support of the hypothesis, a White person expressing pride in his race was viewed by White participants as particularly racist relative to a Black person who does so in Study 1, but was deemed less racist after White participants were reminded of their own mortality in Study 2. Similarly, in Study 3, White participants rated an explicitly racist White employer as less racist when they were reminded beforehand of their own mortality. The results were discussed in terms of implications for affiliation with racist ideologies and terror management defenses.

 The present research investigated the hypotheses that elderly people can be reminders of our mortality and that concerns about our own mortality can therefore instigate ageism. In Study 1, college-age participants who saw photos of two elderly people subsequently showed more death accessibility than participants who saw photos of only younger people. In Study 2 making mortality salient for participants increased distancing from the average elderly person and decreased perceptions that the average elderly person Possesses favorable attitudes. Mortality salience did not affect ratings of teenagers. In Study 3, these mortality salience effects were moderated by prior reported similarity to elderly people. Distancing from, and derogation of, elderly people after mortality salience occurred only in participants who, weeks before the study, rated their personalities as relatively similar to the average elderly person's. Discussion addresses distinguishing ageism from other forms of prejudice, as well as possibilities for reducing ageism.

-----------------------
 Terror management research has often shown that after reminders of mortality people show greater investment in and support for groups to which they belong. The question for the present research was whether or not this would extend to Euro American investment in their identification as White. Although it seemed unlikely that White participants would directly exhibit increased identification as Whites, we hypothesized that mortality salience would increase sympathy for other Whites who expressed racial pride or favoritism toward Whites. In support of the hypothesis, a White person expressing pride in his race was viewed by White participants as particularly racist relative to a Black person who does so in Study 1, but was deemed less racist after White participants were reminded of their own mortality in Study 2. Similarly, in Study 3, White participants rated an explicitly racist White employer as less racist when they were reminded beforehand of their own mortality. The results were discussed in terms of implications for affiliation with racist ideologies and terror management defenses.

 The present research investigated the hypotheses that elderly people can be reminders of our mortality and that concerns about our own mortality can therefore instigate ageism. In Study 1, college-age participants who saw photos of two elderly people subsequently showed more death accessibility than participants who saw photos of only younger people. In Study 2 making mortality salient for participants increased distancing from the average elderly person and decreased perceptions that the average elderly person Possesses favorable attitudes. Mortality salience did not affect ratings of teenagers. In Study 3, these mortality salience effects were moderated by prior reported similarity to elderly people. Distancing from, and derogation of, elderly people after mortality salience occurred only in participants who, weeks before the study, rated their personalities as relatively similar to the average elderly person's. Discussion addresses distinguishing ageism from other forms of prejudice, as well as possibilities for reducing ageism.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 This project explores the impact of Barack Obama's presidential campaign and the resulting high levels of exposure to a positive, counter-stereotypic Black exemplar, on prejudice and stereotyping among non-Black participants. We found dramatically decreased levels of implicit anti-Black prejudice and stereotyping as compared with bias observed previously at the same institutions and in the literature. Providing some insight why the bias was reduced, Study I demonstrated that participants had positive Black exemplars come to mind or anticipated that other people have positive exemplars come to mind when they thought of Black people and this was associated with low levels of racial prejudice. Our second study revealed that participants who had qualities strongly associated with Obama as a political figure (e.g., president) activated when they were primed with "Black" had lower levels of implicit prejudice. These findings indicate that the extensive exposure to Obama resulted in a drop in implicit bias. (C) 2009 Elsevier Inc. All rights reserved.

 The present research experimentally evaluated whether exposure to Barack Obama, a positive counter-stereotypic exemplar, can result in a decrease in implicit anti-Black prejudice among non-Black participants. In order to undo any existing influence of exposure to Obama, we first exposed some participants to negative Black exemplars. Participants were assigned to one of three conditions where they were exposed subtly to negative Black exemplars, to negative Black exemplars and then Obama, or to neutral X's (i.e., control). Participants who were only primed with negative Black exemplars showed more implicit negativity toward Black people compared to the control group. Participants exposed to the same negative Black exemplars and then Obama showed a decrease in implicit racial bias levels compared to those in the negative exemplar only condition, providing experimental evidence that exposure to Obama can decrease implicit racial bias levels. These findings indicate that even subtle exposure to a positive, counter-stereotypic exemplar can reduce implicit prejudice. (c) 2010 Elsevier Inc. All rights reserved.

-----------------------
 Self-evaluations after interracial and dyadic interactions were examined. African American and White females interacted with either a same- or different-race partner in one of 3 role conditions: the high-status role of an interviewer, the low-status role of an applicant, or a peer of equal status. Following the interaction, responses to the Collective Self-Esteem scale (Luhtanen &amp; Crocker, 1992) assessed social self-evaluation, while the Rosenberg Self-Esteem scale (Rosenberg, 1965) and the State Self-Esteem scale (Heatherton &amp; Polivy, 1991) assessed personal self-esteem, Combinations of racial composition and situational role had striking influences on self-evaluations. For instance, when situational roles signaled a reversal from societal status, participants reported lower collective self-esteem than when situational and societal status were consistent. Thus, roles can have compelling consequences for self-evaluation after intergroup interactions.

 Despite growing racioethnic diversity in U.S. organizations, few organizational studies have focused on Black-White interracial interactions. Two experiments examined the influence of interaction roles, and the social scripts they trigger, on White participants' anxiety during dyadic interactions with Black partners. Results from both studies reveal that White participants exhibited greater discomfort in Black-White interactions than in same-race interactions unless their interaction role offered an accessible script to guide behavior. Thus, the present findings suggest organizations may be able to attenuate anxiety among White employees by (a) providing opportunities for initial Black-White interactions in settings with clearly defined social scripts for behavior and (b) helping them to develop behavioral scripts for naturally occurring Black-White workplace interactions.

-----------------------
 Although skeptics continue to doubt that most people are "ideological," evidence suggests that meaningful left-right differences do exist and that they may be rooted in basic personality dispositions, that is, relatively stable individual differences in psychological needs, motives, and orientations toward the world. Seventy-five years of theory and research on personality and political orientation has produced a long list of dispositions, traits, and behaviors. Applying a theory of ideology as motivated social cognition and a "Big Five" framework, we find that two traits, Openness to New Experiences and Conscientiousness, parsimoniously capture many of the ways in which individual differences underlying political orientation have been conceptualized. In three studies we investigate the relationship between personality and political orientation using multiple domains and measurement techniques, including: self-reported personality assessment; nonverbal behavior in the context of social interaction; and personal possessions and the characteristics of living and working spaces. We obtained consistent and converging evidence that personality differences between liberals and conservatives are robust, replicable, and behaviorally significant, especially with respect to social (vs. economic) dimensions of ideology. In general, liberals are more open-minded, creative, curious, and novelty seeking, whereas conservatives are more orderly, conventional, and better organized.

 We trace the rise, fall, and resurgence of political ideology as a topic of research in social, personality, and political psychology. For over 200 years, political belief systems have been classified usefully according to a single left-right (or liberal-conservative) dimension that, we believe, possesses two core aspects: (a) advocating versus resisting social change and (b) rejecting versus accepting inequality. There have been many skeptics of the notion that most people are ideologically inclined, but recent psychological evidence suggests that left-right differences are pronounced in many life domains. Implicit as well as explicit preferences for tradition, conformity, order, stability, traditional values, and hierarchy-versus those for progress, rebelliousness, chaos, flexibility, feminism, and equality-are associated with conservatism and liberalism, respectively. Conservatives score consistently higher than liberals on measures of system justification. Furthermore, there are personality and lifestyle differences between liberals and conservatives as well as situational variables that induce either liberal or conservative shifts in political opinions. Our thesis is that ideological belief systems may be structured according to a left-right dimension for largely psychological reasons linked to variability in the needs to reduce uncertainty and threat.

-----------------------
 Although skeptics continue to doubt that most people are "ideological," evidence suggests that meaningful left-right differences do exist and that they may be rooted in basic personality dispositions, that is, relatively stable individual differences in psychological needs, motives, and orientations toward the world. Seventy-five years of theory and research on personality and political orientation has produced a long list of dispositions, traits, and behaviors. Applying a theory of ideology as motivated social cognition and a "Big Five" framework, we find that two traits, Openness to New Experiences and Conscientiousness, parsimoniously capture many of the ways in which individual differences underlying political orientation have been conceptualized. In three studies we investigate the relationship between personality and political orientation using multiple domains and measurement techniques, including: self-reported personality assessment; nonverbal behavior in the context of social interaction; and personal possessions and the characteristics of living and working spaces. We obtained consistent and converging evidence that personality differences between liberals and conservatives are robust, replicable, and behaviorally significant, especially with respect to social (vs. economic) dimensions of ideology. In general, liberals are more open-minded, creative, curious, and novelty seeking, whereas conservatives are more orderly, conventional, and better organized.

 We trace the rise, fall, and resurgence of political ideology as a topic of research in social, personality, and political psychology. For over 200 years, political belief systems have been classified usefully according to a single left-right (or liberal-conservative) dimension that, we believe, possesses two core aspects: (a) advocating versus resisting social change and (b) rejecting versus accepting inequality. There have been many skeptics of the notion that most people are ideologically inclined, but recent psychological evidence suggests that left-right differences are pronounced in many life domains. Implicit as well as explicit preferences for tradition, conformity, order, stability, traditional values, and hierarchy-versus those for progress, rebelliousness, chaos, flexibility, feminism, and equality-are associated with conservatism and liberalism, respectively. Conservatives score consistently higher than liberals on measures of system justification. Furthermore, there are personality and lifestyle differences between liberals and conservatives as well as situational variables that induce either liberal or conservative shifts in political opinions. Our thesis is that ideological belief systems may be structured according to a left-right dimension for largely psychological reasons linked to variability in the needs to reduce uncertainty and threat.

-----------------------
 Although skeptics continue to doubt that most people are "ideological," evidence suggests that meaningful left-right differences do exist and that they may be rooted in basic personality dispositions, that is, relatively stable individual differences in psychological needs, motives, and orientations toward the world. Seventy-five years of theory and research on personality and political orientation has produced a long list of dispositions, traits, and behaviors. Applying a theory of ideology as motivated social cognition and a "Big Five" framework, we find that two traits, Openness to New Experiences and Conscientiousness, parsimoniously capture many of the ways in which individual differences underlying political orientation have been conceptualized. In three studies we investigate the relationship between personality and political orientation using multiple domains and measurement techniques, including: self-reported personality assessment; nonverbal behavior in the context of social interaction; and personal possessions and the characteristics of living and working spaces. We obtained consistent and converging evidence that personality differences between liberals and conservatives are robust, replicable, and behaviorally significant, especially with respect to social (vs. economic) dimensions of ideology. In general, liberals are more open-minded, creative, curious, and novelty seeking, whereas conservatives are more orderly, conventional, and better organized.

 We trace the rise, fall, and resurgence of political ideology as a topic of research in social, personality, and political psychology. For over 200 years, political belief systems have been classified usefully according to a single left-right (or liberal-conservative) dimension that, we believe, possesses two core aspects: (a) advocating versus resisting social change and (b) rejecting versus accepting inequality. There have been many skeptics of the notion that most people are ideologically inclined, but recent psychological evidence suggests that left-right differences are pronounced in many life domains. Implicit as well as explicit preferences for tradition, conformity, order, stability, traditional values, and hierarchy-versus those for progress, rebelliousness, chaos, flexibility, feminism, and equality-are associated with conservatism and liberalism, respectively. Conservatives score consistently higher than liberals on measures of system justification. Furthermore, there are personality and lifestyle differences between liberals and conservatives as well as situational variables that induce either liberal or conservative shifts in political opinions. Our thesis is that ideological belief systems may be structured according to a left-right dimension for largely psychological reasons linked to variability in the needs to reduce uncertainty and threat.

-----------------------
 Two experiments examined the impact of attention on sensorimotor skills. In Experiment 1, experienced golfers putted under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step putting performance. Dual-task condition putting was more accurate. In Experiment 2, right-footed novice and experienced soccer players dribbled through a slalom course under dual-task or skill-focused conditions. When using their dominant right foot, experts again performed better in the dual-task condition. However, when using their less proficient left foot, experts performed better in the skill-focused condition. Novices performed better under skill-focus regardless of foot. Whereas novices and the less-proficient performances of experts benefit from online attentional monitoring of step-by-step performance, high-level skill execution is harmed.

 In two experiments, we examined the attentional mechanisms governing sensorimotor skill execution across levels of expertise, In Experiment 1, novice and expert golfers took a series of putts under dual-task conditions designed to distract attention from putting and under skill-focused conditions that prompted attention to step-by-step performance. Novices performed better under skill-focused than under dual-task conditions. Experts showed the opposite pattern. In Experiment 2, novice and expert golfers putted under instructions that emphasized either putting accuracy or speed-the latter intended to reduce the time available to monitor and explicitly adjust execution parameters. Novices putted better under accuracy instructions. Experts were more accurate under speed instructions. In agreement with theories of skill acquisition and automaticity, novice performance is enhanced by conditions that allow for on-line attentional monitoring (e.g., skill-focused or accuracy instructions) in comparison with conditions that prevent explicit attentional control of skill execution (e.g., dud-task or speed constraints). In contrast, the proceduralized skill of experts benefits from environments that limit, rather than encourage, attention to execution.

-----------------------
 The authors describe how contemporary learning theory and research provide the basis for perspectives on the etiology and maintenance of anxiety disorders that capture the complexity associated with individual differences in the development and course of these disorders. These insights from modern research on learning overcome the shortcomings of earlier overly simplistic behavioral approaches, which sometimes have been justifiably criticized. The authors show how considerations of early learning histories and temperamental vulnerabilities affect the short- and long-term outcomes of experiences with stressful events. They also demonstrate how contextual variables during and following stressful learning events affect the course of anxiety disorder symptoms once they develop. This range of variables can lead to a rich and nuanced understanding of the etiology and course of anxiety disorders.

 Current etiological models of anxiety disorders emphasize both internal diatheses, or risk factors, and external stressors as important in the development and maintenance of clinical anxiety. Although considerable evidence suggests personality, genetic, and environmental variables are important to these diathesis-stress interactions, this general approach could be greatly enriched by incorporating recent developments in experimental research on fear and anxiety learning. In this article, we attempt to integrate the experimental literature on fear/anxiety learning and the psychopathology literature on clinical anxiety, identify areas of inconsistency, and recommend directions for future research. First, we provide an overview of contemporary models of anxiety disorders involving fear/anxiety learning. Next, we review the literature on individual differences in associative learning among anxious and non-anxious individuals. We also examine additional possible sources of individual differences in the learning of both fear and anxiety, and indicate where possible parallels may be drawn. Finally, we discuss recent developments in basic experimental research on fear conditioning and anxiety, with particular attention to research on contextual learning, and indicate the relevance of these findings to anxiety disorders. (C) 2007 Elsevier B.V. All rights reserved.

-----------------------
 Working memory capacity was differentiated along functional and content-related facets. Twenty-four tasks were constructed to operationalize the cells of the proposed taxonomy. We tested 133 university students with the new tasks, together with six working memory marker tasks. With structural equation models, three working memory functions could be distinguished: Simultaneous storage and processing, supervision, and coordination of elements into structures. Each function was further subdivided into distinct components of variance. On the content dimension, evidence for a dissociation between verbal-numerical working memory and spatial working memory was comparatively weak. (C) 2003 Elsevier Science Inc. All rights reserved.

 Investigates the relationship between three factors of working memory (storage and processing, relational integration, and supervision) and four factors of intelligence (reasoning, speed. memory, and creativity) using Structural equation models. Relational integration predicted reasoning ability at least as well as the storage-and-processing construct. Supervision, Measured as specific switch costs. was not related to intelligence, but general switch costs were moderately correlated to the reasoning factor. The results question the view of working memory as a device for slot-age and processing. and the executive-attention account of working memory. They are better explained by theories describing working memory as a system for building relational representations through temporary bindings between component representations. (C) 2008 Elsevier Inc. All rights reserved.

-----------------------
 Lejeune (1998) (Switching or gating? The attentional challenge in cognitive models of psychological time. Behav. Process. 44, 127-45) analyzed and compared two models of prospective timing: the classical switching model and the attentional-gate model. Lejeune argued that a modified switch notion, which can be opened and closed in a frequency which reflects the amount of attentional resources allocated for timing can provide a satisfactory explanation for the impact of attention on prospective timing, and therefore the notion of an 'attentional switch' is favored over adding an 'attentional gate.' In the present analysis, the two competing models are compared in terms of correspondence with the nature of attentional processes, as well as in terms of logical analysis and explanatory power. Based on this comparison, it is argued that gating is a better model of prospective timing than switching. (C) 2000 Published by Elsevier Science B.V. All rights reserved.

 Lejeune (1998) (Switching or gating? The attentional challenge in cognitive models of psychological time. Behav. Process. 34, 127-45) analyzed and compared two models of prospective timing: the classical switching model and the attentional-gate model. Lejeune argued that a modified switch notion, which can he opened and closed in a frequency which reflects the amount of attentional resources allocated for timing can provide a satisfactory explanation for the impact of attention on prospective timing, and therefore the notion of an 'attentional switch' is favored over adding an 'attentional gate.' In the present analysis, the two competing models are compared in terms of correspondence with the nature of attentional processes, as well as in terms of logical analysis and explanatory power. Based on this comparison, it is argued that gating is a better model of prospective timing than switching. (C) 2000 Elsevier Science B.V. All rights reserved.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 This study sought to address the lack of experimental research examining the influence of contextual factors on African American students' learning. A total of 162 low-income African American and White fourth graders were randomly assigned to ethnically homogeneous, communally structured groups of three to work on a motion acceleration task using either computer simulation or physical tools, or to a control group that did not participate in the learning activities. A 3 (condition) x 2 (ethnicity) MANOVA was computed with initial learning and transfer as dependent variables. Results indicate African American and White students performed equally well on the test of initial learning, with both groups scoring significantly higher than the control group. However, African Americans' transfer outcomes were better than those of their White counterparts. Regarding tools, work with physical apparatus yielded better transfer outcomes than work with computer simulation. Implications for creating optimal learning contexts for African American students are discussed.

 This study tests the hypothesis that cultural differences in group orientation predict an interaction between the student variableethnicityand a learning context variablereward structureon math performance after group learning. One hundred and thirty-two African-American and European-American female and male fourth and fifth grade students studied math estimation in one of three group learning contexts. The learning contexts operationalized were: intergroup competitive, interpersonally competitive, and communal-no reward. ANCOVA confirmed a predicted interaction of ethnicity with learning context on post study session performance. Although there was no difference overall, African-American and European-American students performed best in the aggregate in different contexts. Independent ratings of students' group-positive behaviors mirrored the two-way interaction between learning context and ethnicity. The findings suggest that important student variables interact with the variable elements of group learning and should be studied in greater detail. They also support Boykin's (1994) contention that the cultural context of learning is a critical mediator of children's achievement.

-----------------------
 We show that visio-spatial representations and reasoning can be used as a similarity metric for case-based protein structure prediction. Our system retrieves pairs of alpha-helices based on contact map similarity, then transfers and adapts the structure information to an unknown helix pair. We show that similar protein contact maps predict a similar three-dimensional protein structure. The success of this method provides support for the notion that changing representations can enable similarity metrics in case-based reasoning.

 We show that visuospatial representations and reasoning techniques can be used as a similarity metric for analogical protein structure prediction. Our system retrieves pairs of a-helices based on contact map similarity, then transfers and adapts the structure information to an unknown helix pair, showing that similar protein contact maps predict similar 3D protein structure. The success of this method provides support for the notion that changing representations can enable similarity metrics in analogy.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 This paper is about fitting multivariate normal mixture distributions subject to structural equation modeling. The general model comprises common factor and structural regression models. The introduction of covariance and mean structure models reduces the number of parameters to be estimated in fitting the mixture and enables one to investigate a variety of substantive hypotheses concerning the differences between the components in the mixture. Within the general model, individual parameters can be subjected to equality, nonlinear and simple bounds constraints. Confidence intervals are based on the inverse of the Hessian and on the likelihood profile. Several illustrations are given and results of a simulation study concerning the confidence intervals are reported.

 Considering a dyad as a dynamic system whose current state depends on its past state has allowed researchers to investigate whether and how partners influence each other. Some researchers have also focused on how differences between dyads in their interaction patterns are related to other differences between them. A promising approach in this area is the model that was proposed by Gottman and Murray, which is based on nonlinear coupled difference equations. In this paper, it is shown that their model is a special case of the threshold autoregressive (TAR) model. As a consequence, we can make use of existing knowledge about TAR models with respect to parameter estimation, model alternatives and model selection. We propose a new estimation procedure and perform a simulation study to compare it to the estimation procedure developed by Gottman and Murray. In addition, we include an empirical example based on interaction data of three dyads.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Several experiments, focusing on decisions made by young, voting-age citizens of the United States about how to respond to incidents of international conflict, are summarized. Participants recommended measured reactions to an initial attack. Repeated attacks led to escalated reaction, however, eventually matching or exceeding the conflict level of the attack itself. If a peace treaty between contending nations was in place, women were more forgiving of an attack, and men were more aggressive. There was little overall difference in reactions to terrorist versus military attacks. Participants responded with a higher level of conflict to terrorist attacks on military than on cultural-educational targets.

 Two experiments examined participants' responses to simulated news reports of terrorist attacks. Participants were told that a nondemocratic nation had sponsored strikes on military and cultural or educational sites in the United States. Participants in both experiments reacted more conflictually to terrorist attacks on military sites than to those on cultural or educational sites. Their conflictual responses on a thermometer scale escalated after repeated attacks. When tested in 2002 and 2004, 1 and 3 years after the real World Trade Center attacks, participants' reactions were more conflictual than those of participants examined before September 11, 2001. Furthermore, current participants' fear and anger increased, and forgiveness decreased, over repeated simulated attacks. Participants lower in masculinity showed more fear and less anger than did those higher in masculinity. This study shows that terrorist attacks produce more than simple terror.

-----------------------
 Social projection is a judgemental heuristic that allows people to make quick and reasonably accurate predictions about others. The first part of this paper presents a review of the status of projection as a highly (though not fully) automatic process, its separateness from superficially similar processes of self-stereotyping, and its implications for intergroup perception. The second part places social projection within the context of the theory of evidential decision making, which highlights the benefits and the liabilities of projection in social dilemma situations. The main benefit is that projection can enhance cooperation within a group by leading individuals to believe that their own behavioural choices will be reciprocated. However, when interpersonal social dilemmas are nested within intergroup dilemmas, differential projection (i.e., strong ingroup projection paired with weak outgroup projection) yields collectively undesirable outcomes.

 Evidence for cooperation in social dilemmas is empirically robust, socially desirable, and theoretically controversial. We review theoretical positions offering normative or descriptive accounts for cooperation and note the scarcity of critical tests among them. We then introduce a modified prisoner's dilemma to perform a critical test of the social projection hypothesis. According to this hypothesis, people cooperate inasmuch as they believe others respond to the situation as they themselves do. The data from three illustrative studies uniquely support the projection hypothesis. We make the analytical case for the social projection hypothesis in the context of the theory of evidential decision making. We review and rebut critical arguments that have been leveled against this theory. We note that a meta-theoretical benefit of evidential decision making is that the rationality of cooperators in social dilemmas is restored without appeals to murky notions of "collective rationality."

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 Our examination of a number of widely used methodology texts in psychology revealed that they present a narrow conception of science that fails to recognize major changes in the understanding of science that have occurred since 1960. Such changes include an emphasis on explanation, theoretical promise and scientific importance, and the difficulties associated with hypothesis testing. Because these texts provide the primary source of science education for psychology students, they poorly prepare students for dealing with contemporary science. In this article, we detail the inadequacies of current methodological texts with respect to their treatment of science and describe a more currently acceptable conception of science. We provide instructors with recommendations for improving the science education of psychology students.

 The purpose of this article is to describe a relatively new movement in the history and philosophy of science, naturalism, a form of pragmatism emphasizing that methodological principles are empirical statements. Thus, methodological principles must be evaluated and justified on the same basis as other empirical statements. On this view, methodological statements may be less secure than the specific scientific theories to which they give rise. The authors examined the feasibility of a naturalistic approach to methodology using logical and historical analysis and by contrasting theories that predict new facts versus theories that explain already known facts. They provide examples of how differences over methodological issues in psychology and in science generally may be resolved using a naturalistic, or empirical, approach.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 A growing body of literature shows that imaging contrary-to-truth experiences can change memory. Recent experiments are reviewed to show that when people think about or image a false event, entire false memories can be implanted. Imagination inflation can occur even when there is no overt social pressure, and when hypothetical events are imaged only briefly. Overall, studies of imagination inflation show that imagining a counter-factual event can make subjects more confident that is actually occurred. We discuss possible mechanisms for imagination inflation and find that, with evidence supporting the involvement of both source confusion and familiarity in creating inflation, the primary mechanism is till to be determined. We briefly review evidence on individual differences in susceptibility to inflation. Finally, the widespread use of imagination-based techniques in self-help and clinical contexts suggests that there may be practical implications when imagination is used as a therapeutic tool.

 Photographs help people illustrate the stories of their lives and the significant stories of their society. However, photographs can do more than illustrate events; in this article, we show that photographs can distort memory for them. We describe the course of our "false-memory implantation" research, and review recent work showing that photographs can sometimes increase-while other times decrease-false memories. First, we discuss research showing that a doctored photo, showing subjects taking a completely fictitious hot-air-balloon ride, can cultivate false memories for that experience. We hypothesize that the photograph helps subjects to imagine details about the event that they later confuse with reality. Second, we show that although photographs are indeed powerful sources of influence on memory, they are not necessarily as powerful as narrative. In fact, in certain circumstances, photographs might constrain imagination. Third, we discuss research showing that true photographs can also cultivate false memories. Finally, we present recent work showing that photographs can create false memories for current events.

-----------------------
 Objectives. Errors in eyewitness accounts can occur when a witness comes into contact with post-event 'misinformation'. A common way to encounter misinformation is through face-to-face interaction, in particular, via conversation with other individuals who also witnessed the crime. The current research compares this kind of misinformation with the non-social post-event narrative method typically employed in laboratory studies.

 The forgetting and remembering phenomena that Erdelyi outlines here have little to do with the concept of repression. None of the research that he describes shows that it is possible for people to repress (and then recover) memories for entire, significant, and potentiatly emotion-laden events. In the absence of scientific evidence, we continue to challenge the validity of the concept of repression.

-----------------------
 Under what conditions do perceivers prefer to assign a bound variable interpretation to a pronominal that is ambiguous between a bound variable and a coreferential interpretation? Several experiments were designed to test the hypothesis that language perceivers prefer a bound variable over a coreferential interpretation of a pronoun because the former only requires consultation of a logical form (LF) representation, while the latter requires access to a discourse representation. The hypothesis was disconfirmed in two respects. First, although bound variable interpretations show a processing advantage over coreferential interpretations in VP ellipsis constructions, the preference for bound variable interpretations is not general-it does nor extend to other quantificational contexts. Second, the preference for bound variable interpretations in VP ellipsis constructions is not limited to examples in which the antecedent and the ellipsis site occur in the same sentence. If the bound variable advantage were due to the ready availability of the LF for the current sentence, the advantage should disappear across sentence boundaries. An alternative hypothesis is then considered which could explain the source of the bound variable advantage in VP ellipsis contexts.

 Four naturalness judgment experiments were conducted to test different hypotheses about prosodic phrasing. The hypothesis that syntactic constituents should not be broken into distinct prosodic phrases [as in Truckenbrodt's Wrap constraint (Truckenbrodt, H., 1995. Phonological Phrases: Their Relation to Syntax, Focus, and Prominence. Unpublished PhD Dissertation, MIT)] was less predictive of the results of Experiments I and 2 than the hypothesis that constituents may be freely divided into prosodic phrases, as long as the resulting phrases are semantically coherent [Selkirk, E., 1984. Phonology and Syntax: The Relation Between Sound and Structure. MIT Press, Cambridge, MA]. The results of two further experiments confirmed Watson and Gibson's (Watson, D. G., Gibson, E., 2001. Linguistic structure and intonational phrasing. Paper presented at the Fourteenth Annual CUNY Conference on Human Sentence Processing, Philadelphia, 15-17 March 2001) claim that prosodic breaks are natural before long upcoming constituents, but did not support their hypothesis that the distance between a new item and its integration site is what motivates the presence of a prosodic phrase boundary. The results are interpreted as further evidence that the use of high level breaks in language comprehension is not governed by an invariant local mapping from syntax or processing considerations to prosody/intonation, but is related to the overall pattern of intonational choices made. (C) 2003 Elsevier B.V. All rights reserved.

-----------------------
 Two experimental tasks in psychology, the two-stage gambling game and the Prisoner's Dilemma game, show that people violate the sure thing principle of decision theory. These paradoxical findings have resisted explanation by classical decision theory for over a decade. A quantum probability model, based on a Hilbert space representation and Schrodinger's equation, provides a simple and elegant explanation for this behaviour. The quantum model is compared with an equivalent Markov model and it is shown that the latter is unable to account for violations of the sure thing principle. Accordingly, it is argued that quantum probability provides a better framework for modelling human decision-making.

 Naive observers typically perceive some groupings for a set of stimuli as more intuitive than others. The problem of predicting category intuitiveness has been historically considered the remit of models of unsupervised categorization. In contrast, this article develops a measure of category intuitiveness from one of the most widely supported models of supervised categorization, the generalized context model (GCM). Considering different category assignments for a set of instances, the authors asked how well the GCM can predict the classification of each instance on the basis of all the other instances. The category assignment that results in the smallest prediction error is interpreted as the most intuitive for the GCM-the authors refer to this way of applying the GCM as "unsupervised GCM." The authors systematically compared predictions of category intuitiveness from the unsupervised GCM and two models of unsupervised categorization: the simplicity model and the rational model. The unsupervised GCM compared favorably with the simplicity model and the rational model. This success of the unsupervised GCM illustrates that the distinction between supervised and unsupervised categorization may need to be reconsidered. However, no model emerged as clearly superior, indicating that there is more work to be done in understanding and modeling category intuitiveness.

-----------------------
 Analogy is a powerful cognitive mechanism that people use to make inferences and learn new abstractions. The history of work on analogy in modern cognitive science is sketched, focusing on contributions from cognitive psychology, artificial intelligence, and philosophy of science. This review sets the stage for the 3 articles that follow in this Science Watch section.

 This paper considers the past and future of Psychology within Cognitive Science. In the history section, I focus on three questions: (a) how has the position of Psychology evolved within Cognitive Science, relative to the other disciplines that make up Cognitive Science; (b) how have particular Cognitive Science areas within Psychology waxed or waned; and (c) what have we gained and lost. After discussing what's happened since the late 1970s, when the Society and the journal began, I speculate about where the field is going.

-----------------------
 This paper replies to Politzer's (2007) criticisms of the mental model theory of conditionals. It argues that the theory provides a correct account of negation of conditionals, that it does not provide a truth-functional account of their meaning, though it predicts that certain interpretations of conditionals yield acceptable versions of the 'paradoxes' of material implication, and that it postulates three main strategies for estimating the probabilities of conditionals.

 O'Brien, Braine, and Yang argue that the mental model theory of propositional reasoning is easy to refute, and they report 3 experiments that they believe falsify the theory. In contrast, Bonatti argues that the model theory is too flexible to be falsified. We show that O'Brien et al.'s experiments do not refute the model theory and that Bonatti's claims are ill founded. Formal rule theories of propositional reasoning have 3 major weaknesses in comparison with the model theory: (a) They have no decision procedure; (b) they lack predictive power, providing no account of several robust phenomena (e.g., erroneous conclusions tend to be consistent with the premises); and (c) as a class of theories, they are difficult to refute experimentally.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 This study investigated the usefulness of explicit spatial coordinates from eye movements for the precision of covert shifts of attention within dense arrays of items. Observers shifted their attention covertly from one item to the next in response to a series of beeps and reported the color of the disc on which the series ended, providing an estimate of the accuracy of the "attentional walk". We compared performance in this task when only covert shifts of attention were done to performance when observers first executed an explicit eye movement to the starting point of the attentional walk before beginning the covert attentional walk. The hypothesis was that the eye movement would activate explicit coordinates of the starting point of the attentional walk within brain systems that are involved in controlling both shifts of attention and eye movements. This in turn would provide an anchor for the attentional walk, thereby improving performance. The evidence did not support this hypothesis. Performance was no better with an explicit eye movement prior to the attentional walk than without one. This suggests that covert orienting-shifting attention-and overt orienting-shifting the eyes-access the same coordinate system and therefore activating new coordinates interferes with the old ones, no matter what the system of orienting is.

 The attentional walk task was used to investigate the temporal properties of covert shifts of attention. Observers shifted attention within arrays of identical items in response to a series of auditory commands and reported the color of the final disk. The density of the arrays and the timing of the shift commands varied. Performance decreased as density increased, and the minimal amount of time needed to shift attention depended on the density of the display, varying from 350 to 750 msec. In addition, the observers were able to maintain attentional focus for at least 3,500 msec without a decline in performance, regardless of density. Thus, although the ability to maintain attention at a given position was found to be independent of the precision with which that location was defined, more precise attentional shifts required more time to execute.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 This review provides a cumulative perspective on current human factors research by first briefly acknowledging previous Annual Review articles. We show that several recent conceptual advances are an outgrowth of the information-processing approach adopted by the field and present several areas of current research that are built directly on prior work. Topic areas that provide fundamental tools for human factors analyses are summarized, and several current application areas are reviewed. We end by considering alternatives to the information-processing approach that have been proposed and placing those alternatives in context. We argue that the information-processing language provides the foundation that has enabled much of the growth in human factors. This growth reflects a cumulative development of concepts and methods that continues today.

 In the global economy, design of digital media often involves teams of individuals from a variety of cultures who must function together. Similarly, products must be designed and marketed taking specific cultural characteristics into account. Much is known about decision processes, culture and cognition, design of products and interfaces for human interaction with machines, and organizational processes, but this knowledge is dispersed across several disciplines and research areas. This article reviews current work in these areas and proposes a research agenda for fostering increased understanding of the ways in which cultural differences influence decision making and action in design and use of digital media.

-----------------------
 The delayed-JOL effect is the finding in which judgments of learning (JOLs) are more accurate at predicting eventual recall when they are made a short time after study rather than immediately after study. The present research replicated this effect and found that the kind of cue that is used for JOLs is critical. In particular, following the study of stimulus-response paired associates, there is an extremely robust delayed-JOL effect when the cue for JOLs is the stimulus alone (every one of 45 subjects showed the effect); however, there is little, if any, delayed-JOL effect when the cue for JOLs is the stimulus-response pair. This finding has important implications for education: To have the greatest accuracy at predicting eventual recall, a person should make JOLs not immediately after study but, instead, shortly after study (i.e., delayed JOLs) with the cue for JOLs being the stimulus alone. The theoretical mechanisms for the delayed-JOL effect are currently unknown, but some speculations are offered.

 Spellman and Bjork (this issue) have proposed an explanation of our earlier findings in which people's accuracy at predicting their subsequent recall proved greater when their judgments of learning (JOLs) were delayed until shortly after study than when made immediately after study (the delayed-JOL effect). The empirically testable portion of Spellman and Bjork's explanation does not account for three relevant kinds of data (some already published and some newly presented here), including one replicated finding that disconfirms their core assumption. Nevertheless, Spellman and Bjork's commentary has helped us sharpen our ideas both about what will have to be explained and about what form of explanation will be needed. At present, there is not yet an adequate scientific explanation for the delayed-JOL effect.

-----------------------
 The modifier effect is the reduction in perceived likelihood of a generic property sentence, when the head noun is modified. We investigated the prediction that the modifier effect would be stronger for mutable than for central properties, without finding evidence for this predicted interaction over the course of five experiments. However Experiment 6, which provided a brief context for the modified concepts to lend them greater credibility, did reveal the predicted interaction. It is argued that the modifier effect arises primarily from a general lack of confidence in generic statements about the typical properties of unfamiliar concepts. Neither prototype nor classical models of concept combination receive support from the phenomenon. (C) 2010 Elsevier Inc. All rights reserved.

 Within-category induction is the projection of a generic property from a class (Apples are sweet) to a subtype of that class (Chinese apples are sweet). The modifier effect refers to the discovery reported by Connolly et al., that the subtype statement tends to be judged less likely to be true than the original unmodified sentence. The effect was replicated and shown to be moderated by the typicality of the modifier (Experiment 1). Likelihood judgements were also found to correlate between modified and unmodified versions of sentences. Experiment 2 elicited justifications, which suggested three types of reason for the effect-pragmatics, knowledge-based reasoning, and uncertainty about attribute inheritance. It is argued that the results provide clear evidence for the default inheritance of prototypical attributes in modified concepts, although a full account of the effect remains to be given.

-----------------------
 Two lexical decision experiments investigated priming for a critical item (CI, sleep) and its related yoked associate (YA, blanket) when one had been studied in a related Deese/Roediger-McDermott (DRM) list (Experiments 1 &amp; 2) or a list of totally unrelated words (Experiment 2) and the other had been nonstudied. Semantic priming from the related DRM list occurred for nonstudied CIs (but not YAs) regardless of whether the Cl received within-test priming from its studied related YA during the lexical decision task, though the effect in the absence of within-test priming averaged across experiments was only significant by a one-tailed test. Also averaged across experiments, repetition priming occurred for both studied Os and YAs when they had been studied in related DRM lists whether or not there was also within-test priming from a nonstudied related yoked pairmate, though individual effects within the two experiments were sometimes not significant. Repetition priming boosted semantic priming from related DRM lists less for CIs than for YAs, similar to the finding that memory discriminability is poorer for CIs than for YAs in episodic recognition. This smaller repetition priming boost for CIs than for YAs occurred to the same degree when the CIs or YAs were studied in an unrelated list. When nonstudied CIs and YAs were totally unrelated to all previously studied items and separated by 3-7 items in the lexical decision task, a YA produced a small 16 msec priming effect for its CI, averaged across both experiments. The implications of these results for the activation account of the DRM false-memory effect and for single-prime versus multiple-prime long-term semantic priming effects are discussed.

 Four experiments examined whether studying a single Deese/Roediger-McDermott (DRM) list produces semantic priming for nonstudied critical items (Cls) and semantic + repetition priming for studied associates. After 30 s of mental arithmetic that followed the study of a DRM list, priming was assessed in a lexical decision task when the nonwords were either pronounceable (Experiment 1) or pseudohomophones (Experiments 2 - 4). Priming was measured relative to a baseline containing exactly the same Cls and associates that had not been primed by their related DRM lists. Significant CI semantic priming effects occurred in all four experiments, whether or not there was within-test priming from a related associate preceding the CI by 3 - 7 items. To our knowledge, these are the first experiments using standard DRM study procedures to provide a convincing demonstration of a genuine CI semantic priming effect in a delayed indirect memory test that should be free of intentional retrieval strategies. Discussion focuses on measuring long-term semantic activation effects without the influence of source monitoring in a lexical decision task. (c) 2005 Elsevier Inc. All rights reserved.

-----------------------
 The relations between automatic processing and (the absence of) consciousness are discussed in this paper. It is argued that automatic processing should not be identified with the absence of consciousness. The organism has access to representations resulting from automatic processing, but these representations, in contrast to the representations resulting from nonautomatic processing, are not propositional. Therefore monitoring of the process, the defining feature of nonautomatic processing, is not possible. (C) 1997 Academic Press.

 We apply Dienes &amp; Perner's (D&amp;P's) framework to the automatic/nonautomatic processing contrast. Our analysis leads to the conclusion that automatic and nonautomatic processing result in representations that have explicit results. We propose equating consciousness with explicitness of aspects rather than with full explicitness as defined by D&amp;P.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 Previous research showed that object decision priming was found for possible, but not impossible, three-dimensional objects (e.g., Schacter, Cooper, &amp; Delaney, 1990; Schacter, Cooper, Delaney, Peterson, &amp; Tharan, 1991). We tested those objects and found that the impossible objects were subjectively more complex than the possible objects. We then constructed two sets of possible and im possible objects - one set that was equated for complexity, and one set that differed - for use in the object decision test. The results showed that when impossible objects were high in complexity and possible objects were low in complexity, priming was found only for possible objects; when possible and impossible objects were equated at a moderate level of complexity, priming was observed for both object types. These findings indicate that perceived object complexity, more than object possibility-impossibility, determined priming in the object decision test. The demonstration of object decision priming for possible and impossible objects calls for a reformulation of the structural description system explanation.

 Previous research demonstrated object decision priming for possible, but net impossible three-dimensional objects (e.g., Schacter et al. 1990; 1991). Subsequent research by Carrasco and Seamen (1996) found that when possible and impossible objects were equated for complexity, priming was observed for both object types. The present research extended the complexity results. Possible objects demonstrated object decision priming with greater classification accuracy for studied than nonstudied objects, following exposure durations of 900 ms to 30 s. The pattern for impossible objects was a function of their complexity. Highly complex impossible objects showed greater classification accuracy for nonstudied than studied objects, whereas moderately complex impossible objects showed no difference in classification accuracy, except following the longest duration where studied objects were classified more accurately than nonstudied objects. The conditions under which priming was observed for possible and impossible objects was discussed in terms of stimulus complexity and the ease of generating structural representations of the stimuli and the presence of a general response bias.

-----------------------
 Objective: To investigate the long-term effect of tibolone on mammographic density.
 Objective To investigate the effect of 10 years of treatment with tibolone on aortic stiffness and endothelial function.
-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. The video game included groups of three potential targets. Participants chose which of the three targets in each group was producing distal explosions. The actual source of the explosion effect varied in the delay between the firing of its weapon and the effect (from 0 to 2 s), whether these programed average delays were constant or varied from shot to shot, and whether the delays were unfilled or filled with an auditory event. In Experiment 1, participants' choice accuracy was highest with shorter delays, but there was no effect of filling the delay and some beneficial effect of varying the delay. These results were re-examined in Experiment 2 but with participants experiencing the same average delay for seven subsequent decisions before the next average delay was introduced. In this experiment, men showed a strong and consistent benefit of filling a delay whereas women did not. Participants' behavior is considered within the context of a model that assumes that choice behavior is driven by experienced contiguity for the target and foils. (C) 2009 Elsevier Inc. All rights reserved.

 A first-person shooter video game was adapted for the study of causal decision making within dynamic environments. Participants chose which of three potential targets in each of 21 groups was producing distal explosions. The source of the explosion effect varied in the delay between the firing of its weapon and its effect (0.0, 0.5, 1.0, and 2.0 s), the probability that the weapon caused the effect (50%, 75%, and 100%), and the occurrence of auditory events that filled the delay. In Experiment 1, participants' choice accuracy was highest with short delays but was not affected by probability; participants often compensated for lower probability by increasing their latencies, and thus the number of outcomes sampled. In Experiment 2, a broad range of delays (0-2 s) and probabilities (20%-100%) were randomly sampled for each cause; the results largely replicated those of the prior experiment. The experiments demonstrate people's ability to successfully modulate their environmental sampling in the face of uncertainty due to lower cause-effect probabilities, but not in the presence of longer cause-effect delays.

-----------------------
 Participants were given several 2-option choices and then asked to review how they felt about their decisions, to review the details of their decisions, or to do an unrelated task. When later asked to attribute features to the previous options, in each condition older adults (64-83 years) attributed significantly more positive and fewer negative features to their chosen options than to foregone options. Younger adults' (18-22 years) attributions were as choice-supportive as those of older adults in the affective review condition but were less so in the other conditions. The age difference was present even when older and younger adults were equated for source identification and recognition accuracy. This study suggests that as people age, their tendency to distort memory in favor of the options they chose increases. In addition, it suggests that affectively reviewing choices increases younger adults' tendency toward choice-supportive memory.

 Previous studies have found that, when remembering, older adults often rely more on schematic knowledge than do younger adults. We replicated this finding when participants were induced to review the facts of the event; furthermore, neuropsychological correlates suggested that this age-related increase in schema reliance is associated with declines in reflective processes. In addition, when they were induced to focus on their feelings and reactions when reviewing an event, both older and younger adults' later memory of the event was strongly affected by their schematic knowledge.

-----------------------
 Two experimental tasks in psychology, the two-stage gambling game and the Prisoner's Dilemma game, show that people violate the sure thing principle of decision theory. These paradoxical findings have resisted explanation by classical decision theory for over a decade. A quantum probability model, based on a Hilbert space representation and Schrodinger's equation, provides a simple and elegant explanation for this behaviour. The quantum model is compared with an equivalent Markov model and it is shown that the latter is unable to account for violations of the sure thing principle. Accordingly, it is argued that quantum probability provides a better framework for modelling human decision-making.

 Naive observers typically perceive some groupings for a set of stimuli as more intuitive than others. The problem of predicting category intuitiveness has been historically considered the remit of models of unsupervised categorization. In contrast, this article develops a measure of category intuitiveness from one of the most widely supported models of supervised categorization, the generalized context model (GCM). Considering different category assignments for a set of instances, the authors asked how well the GCM can predict the classification of each instance on the basis of all the other instances. The category assignment that results in the smallest prediction error is interpreted as the most intuitive for the GCM-the authors refer to this way of applying the GCM as "unsupervised GCM." The authors systematically compared predictions of category intuitiveness from the unsupervised GCM and two models of unsupervised categorization: the simplicity model and the rational model. The unsupervised GCM compared favorably with the simplicity model and the rational model. This success of the unsupervised GCM illustrates that the distinction between supervised and unsupervised categorization may need to be reconsidered. However, no model emerged as clearly superior, indicating that there is more work to be done in understanding and modeling category intuitiveness.

-----------------------
 Because the probability of obtaining an experimental. finding given that the null hypothesis is true [p(F\H,)] is not the same as the probability that the null hypothesis is true given a finding [p(H-0\F)], calculating the former probability does not justify conclusions about the latter one. As the standard null-hypothesis significance-testing procedure does just that, it is logically invalid (J. Cohen, 1994). Theoretically, Bayes's theorem yields p(H-0\F), but in practice, researchers rarely know the correct values for 2 of the variables in the theorem. Nevertheless, by considering a wide range of possible values for the unknown variables, it is possible to calculate a range of theoretical values for p(H-0\F) and to draw conclusions about both hypothesis testing and theory evaluation.

 According to Bayesians, the null hypothesis significance-testing procedure is not deductively valid because it involves the retention or rejection of the null hypothesis under conditions where the posterior probability of that hypothesis is not known. Other criticisms are that this procedure is pointless and encourages imprecise hypotheses. However , according to non-Bayesians, there is no way of assigning a prior probability to the null hypothesis, and so Bayesian statistics do not work either. Consequently, no procedure has been accepted by both groups as providing a compelling reason to accept or reject hypotheses. The author aims to provide such a method. In the process, the author distinguishes between probability and epistemic estimation and argues that, although both are important in a science that is not completely deterministic, epistemic estimation is most relevant for hypothesis testing. Based on this analysis, the author proposes that hypotheses be evaluated via epistemic ratios and explores the implications of this proposal. One implication is that it is possible to encourage precise theorizing by imposing a penalty for imprecise hypotheses.

-----------------------
 The authors present a theory of stochastic interactive parallel processing with special emphasis on channel interactions and their relation to system capacity. The approach is based both on linear systems theory augmented with stochastic elements and decisional operators and on a metatheory of parallel channels' dependencies that incorporates standard independent and coactive parallel models as special cases. The metatheory is applied to OR and AND experimental paradigms, and the authors establish new theorems relating response time performance in these designs to earlier and novel issues. One notable outcome is the remarkable processing efficiency associated with linear parallel-channel systems that include mutually positive interactions. The results may offer insight into perceptual and cognitive configural-holistic processing systems.

 The maximum and minimum of a sample from a probability distribution are extremely important random variables in many areas of psychological theory, methodology, and statistics. For instance, the behavior of the mean of the maximum or minimum processing time, as a function of the number of component random processing times (n), has been studied extensively in an effort to identify the underlying processing architecture (e.g., Townsend &amp; Ashby, 1983; Colonius &amp; Vorberg, 1994). Little is known concerning how measures of variability of the maximum or minimum change with n. Here, a new measure of random variability, the quantile spread, is introduced, which possesses sufficient strength to define distributional orderings and derive a number of results concerning variability of the maximum and the minimum statistics. The quantile spread ordering may be useful in many venues. Several interesting open problems are pointed out.

-----------------------
 Four studies (N = 711) probed processes that underlie sex differences in attraction. In Study 1, women reported expertise at judging men's physical attractiveness, and this expertise was acknowledged by men. In Study 2, women evaluated physical attractiveness after seeing ratings supposedly made by same-sex peers. Women were influenced by other women's ratings, especially negative ratings. Study 3 indicated that women were influenced by female peers in rating both male and female stimulus persons. There was no evidence that men were influenced by peers. In Study 4, Ss evaluated physically attractive persons of the other sex after reading a detailed description. Women were more influenced by peer evaluations than were men. Results were discussed in terms of sex differences and social influence processes in interpersonal attraction.

 This research program explored links among prosocial motives, empathy, and helping behavior. Preliminary work found significant relations among components of self-reported empathy and personality (N = 223). In Study 1, the authors examined the generality of prosocial behavior across situations and group memberships of victims (N = 622). In Study 2, empathic focus and the victim's outgroup status were experimentally manipulated (N = 87). Study 3 (N 245) replicated and extended Study 2 by collecting measures of prosocial emotions before helping. In Study 4 (N = 244), empathic focus and cost of helping as predictors of helping behavior were experimentally manipulated. Overall, prosocial motivation is linked to (a) Agreeableness as a dimension of personality, (b) proximal prosocial cognition and motives, and (c) helping behavior across a range of situations and victims. In persons low in prosocial motivation, when costs of helping are high, efforts to induce empathy situationally can undermine prosocial behavior.

-----------------------
 Previous work showed that older adults' choice performance can be wiser than that of younger adults (Tentori, Osherson, Hasher, &amp; May, 2001). We contrasted two possible interpretations: a general expertise/wisdom view that suggests that older adults are generally more skilled at making decisions than younger adults and a domain-specific expertise view that suggests that older adults are more skilled decision makers only in domains in which they have greater knowledge. These hypotheses were contrasted using attraction effect tasks in two different domains: carning extra credit in a course and grocery shopping, domains presumed to be of different levels of knowledge to younger and older adults. Older adults showed consistent choice for both domains; younger adults showed consistent choice only for the extra credit problem. Several explanations of these findings are considered, including Damasio's somatic marker theory and age differences in reliance on heuristic versus analytic styles.

 A decision-maker is "irregular" if she would choose B from {A, B, C} but not from {A, B} (for example, preferring vanilla ice cream in a choice between vanilla and chocolate, but chocolate in a choice among vanilla, chocolate and strawberry). Similarly to previous studies we observed irregular choices by college students faced with hypothetical discount cards for supermarkets. However, older adults showed no such tendency. The same pattern was observed in three separate studies. We interpret the results in terms of a choice strategy by older adults that protects them from excessive spending. (C) 2001 Elsevier Science B.V. All ri-hts reserved.

-----------------------
